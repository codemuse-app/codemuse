{"options":{"type":"directed","multi":true,"allowSelfLoops":true},"attributes":{},"nodes":[{"key":"scip-typescript npm view 0.1.0 src/`reportWebVitals.ts`/","attributes":{"range":[0,0,15,0],"symbol":"scip-typescript npm view 0.1.0 src/`reportWebVitals.ts`/","content":"import { ReportHandler } from 'web-vitals';\n\nconst reportWebVitals = (onPerfEntry?: ReportHandler) => {\n  if (onPerfEntry && onPerfEntry instanceof Function) {\n    import('web-vitals').then(({ getCLS, getFID, getFCP, getLCP, getTTFB }) => {\n      getCLS(onPerfEntry);\n      getFID(onPerfEntry);\n      getFCP(onPerfEntry);\n      getLCP(onPerfEntry);\n      getTTFB(onPerfEntry);\n    });\n  }\n};\n\nexport default reportWebVitals;\n","file":"/demo/realtime-txt2img/view/src/reportWebVitals.ts","language":"typescript","fileHash":"fa208d5a5ab6d20b64122998b074707dea08d12ed619652955f77df958d85786","hash":"fa208d5a5ab6d20b64122998b074707dea08d12ed619652955f77df958d85786","processedContent":"import { ReportHandler } from 'web-vitals';\n\nconst reportWebVitals = (onPerfEntry?: ReportHandler) => {\n  \"\"\"This code is a function that takes a callback function as an argument and measures the performance of the web page using the `web-vitals` library. It then calls the callback function with the measured performance metrics as arguments.\"\"\"\n  pass\n\nexport default reportWebVitals; #This code is a function that takes a callback function as an argument and measures the performance of the web page using the `web-vitals` library. It then calls the callback function with the measured performance metrics as arguments.\n","documentation":"This code is a function that measures the performance of a web page using the `web-vitals` library and calls a callback function with the measured performance metrics as arguments."}},{"key":"scip-typescript npm view 0.1.0 src/`reportWebVitals.ts`/reportWebVitals.","attributes":{"range":[2,24,12,1],"symbol":"scip-typescript npm view 0.1.0 src/`reportWebVitals.ts`/reportWebVitals.","content":"(onPerfEntry?: ReportHandler) => {\n  if (onPerfEntry && onPerfEntry instanceof Function) {\n    import('web-vitals').then(({ getCLS, getFID, getFCP, getLCP, getTTFB }) => {\n      getCLS(onPerfEntry);\n      getFID(onPerfEntry);\n      getFCP(onPerfEntry);\n      getLCP(onPerfEntry);\n      getTTFB(onPerfEntry);\n    });\n  }\n}","file":"/demo/realtime-txt2img/view/src/reportWebVitals.ts","language":"typescript","fileHash":"fa208d5a5ab6d20b64122998b074707dea08d12ed619652955f77df958d85786","hash":"c3008dc6d6bd68704fbe728e42b034bd390b302ade32e2dfc0f613cc7f3f0029","processedContent":"(onPerfEntry?: ReportHandler) => {\n  if (onPerfEntry && onPerfEntry instanceof Function) {\n    import('web-vitals').then(({ getCLS, getFID, getFCP, getLCP, getTTFB }) => {\n      getCLS(onPerfEntry);\n      getFID(onPerfEntry);\n      getFCP(onPerfEntry);\n      getLCP(onPerfEntry);\n      getTTFB(onPerfEntry);\n    });\n  }\n}","documentation":"This code is a function that takes a callback function as an argument and measures the performance of the web page using the `web-vitals` library. It then calls the callback function with the measured performance metrics as arguments."}},{"key":"scip-typescript npm view 0.1.0 src/`react-app-env.d.ts`/","attributes":{"range":[0,0,1,0],"symbol":"scip-typescript npm view 0.1.0 src/`react-app-env.d.ts`/","content":"/// <reference types=\"react-scripts\" />\n","file":"/demo/realtime-txt2img/view/src/react-app-env.d.ts","language":"typescript","fileHash":"57eda4c4c04a1dca45c62857326882ce9cc948c4b52973c0e3c3b7e4c3fa3990","hash":"57eda4c4c04a1dca45c62857326882ce9cc948c4b52973c0e3c3b7e4c3fa3990","processedContent":"/// <reference types=\"react-scripts\" />\n","documentation":"This code is a reference to the React-Scripts library, which provides a set of tools for building and deploying React applications."}},{"key":"scip-typescript npm view 0.1.0 src/`index.tsx`/","attributes":{"range":[0,0,19,0],"symbol":"scip-typescript npm view 0.1.0 src/`index.tsx`/","content":"import React from \"react\";\nimport ReactDOM from \"react-dom/client\";\nimport \"./index.css\";\nimport App from \"./App\";\nimport reportWebVitals from \"./reportWebVitals\";\n\nconst root = ReactDOM.createRoot(\n  document.getElementById(\"root\") as HTMLElement\n);\nroot.render(\n  <React.StrictMode>\n    <App />\n  </React.StrictMode>\n);\n\n// If you want to start measuring performance in your app, pass a function\n// to log results (for example: reportWebVitals(console.log))\n// or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals\nreportWebVitals();\n","file":"/demo/realtime-txt2img/view/src/index.tsx","language":"typescript","fileHash":"39884e660617a5c80f5c881b4e46a187fbad8236d878e65a601cbe68ed1f82ba","hash":"39884e660617a5c80f5c881b4e46a187fbad8236d878e65a601cbe68ed1f82ba","processedContent":"import React from \"react\"; #undefined\nimport ReactDOM from \"react-dom/client\"; #undefined\nimport \"./index.css\";\nimport App from \"./App\";\n\"\"\"\nscip-typescript npm view 0.1.0 src/`App.tsx`/App().: This code is a React component that renders a form with a text input field and a grid of images. When the user types in the input field, the component fetches 16 images from an API based on the input prompt, and displays them in the grid. The component also calculates the edit distance between the previous and current input prompts to prevent unnecessary API calls.\nscip-typescript npm view 0.1.0 src/`App.tsx`/: This code is a React component that renders a form with a text input field and a grid of images. When the user types in the input field, the component fetches 16 images from an API based on the input prompt, and displays them in the grid. The component also calculates the edit distance between the previous and current input prompts to prevent unnecessary API calls.\n\"\"\"\nimport reportWebVitals from \"./reportWebVitals\";\n\"\"\"\nscip-typescript npm view 0.1.0 src/`reportWebVitals.ts`/reportWebVitals.: This code is a function that takes a callback function as an argument and measures the performance of the web page using the `web-vitals` library. It then calls the callback function with the measured performance metrics as arguments.\nscip-typescript npm view 0.1.0 src/`reportWebVitals.ts`/: This code is a function that measures the performance of a web page using the `web-vitals` library and calls a callback function with the measured performance metrics as arguments.\n\"\"\"\n\nconst root = ReactDOM.createRoot(\n  document.getElementById(\"root\") as HTMLElement #undefined\n);\nroot.render( #undefined\n  <React.StrictMode>\n    <App /> #This code is a React component that renders a form with a text input field and a grid of images. When the user types in the input field, the component fetches 16 images from an API based on the input prompt, and displays them in the grid. The component also calculates the edit distance between the previous and current input prompts to prevent unnecessary API calls.\n  </React.StrictMode>\n);\n\n// If you want to start measuring performance in your app, pass a function\n// to log results (for example: reportWebVitals(console.log))\n// or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals\nreportWebVitals(); #This code is a function that takes a callback function as an argument and measures the performance of the web page using the `web-vitals` library. It then calls the callback function with the measured performance metrics as arguments.\n","documentation":"The code is a React application that renders a form with a text input field and a grid of images. When the user types in the input field, the component fetches 16 images from an API based on the input prompt, and displays them in the grid. The component also calculates the edit distance between the previous and current input prompts to prevent unnecessary API calls."}},{"key":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/","attributes":{"symbol":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/","language":"typescript"}},{"key":"scip-typescript npm @types/react-dom 18.2.17 `client.d.ts`/","attributes":{"symbol":"scip-typescript npm @types/react-dom 18.2.17 `client.d.ts`/","language":"typescript"}},{"key":"scip-typescript npm view 0.1.0 src/`App.tsx`/App().","attributes":{"symbol":"scip-typescript npm view 0.1.0 src/`App.tsx`/App().","language":"typescript","range":[3,0,137,1],"content":"function App() {\n  const [inputPrompt, setInputPrompt] = useState(\"\");\n  const [lastPrompt, setLastPrompt] = useState(\"\");\n  const [images, setImages] = useState(Array(16).fill(\"images/white.jpg\"));\n\n  const calculateEditDistance = (a: string, b: string) => {\n    if (a.length === 0) return b.length;\n    if (b.length === 0) return a.length;\n\n    const matrix = [];\n\n    for (let i = 0; i <= b.length; i++) {\n      matrix[i] = [i];\n    }\n    for (let i = 0; i <= a.length; i++) {\n      matrix[0][i] = i;\n    }\n\n    for (let i = 1; i <= b.length; i++) {\n      for (let j = 1; j <= a.length; j++) {\n        if (b.charAt(i - 1) === a.charAt(j - 1)) {\n          matrix[i][j] = matrix[i - 1][j - 1];\n        } else {\n          matrix[i][j] = Math.min(\n            matrix[i - 1][j - 1] + 1,\n            Math.min(matrix[i][j - 1] + 1, matrix[i - 1][j] + 1)\n          );\n        }\n      }\n    }\n\n    return matrix[b.length][a.length];\n  };\n\n  const fetchImage = useCallback(\n    async (index: number) => {\n      try {\n        const response = await fetch(\"api/predict\", {\n          method: \"POST\",\n          headers: { \"Content-Type\": \"application/json\" },\n          body: JSON.stringify({ prompt: inputPrompt }),\n        });\n        const data = await response.json();\n        const imageUrl = `data:image/jpeg;base64,${data.base64_image}`;\n\n        setImages((prevImages) => {\n          const newImages = [...prevImages];\n          newImages[index] = imageUrl;\n          return newImages;\n        });\n      } catch (error) {\n        console.error(\"Error fetching image:\", error);\n      }\n    },\n    [inputPrompt]\n  );\n\n  const handlePromptChange = (event: React.ChangeEvent<HTMLInputElement>) => {\n    setInputPrompt(event.target.value);\n    const newPrompt = event.target.value;\n    const editDistance = calculateEditDistance(lastPrompt, newPrompt);\n\n    if (editDistance >= 4) {\n      setInputPrompt(newPrompt);\n      setLastPrompt(newPrompt);\n      for (let i = 0; i < 16; i++) {\n        fetchImage(i);\n      }\n    }\n  };\n\n  return (\n    <div\n      className=\"App\"\n      style={{\n        backgroundColor: \"#282c34\",\n        height: \"100vh\",\n        display: \"flex\",\n        alignItems: \"center\",\n        justifyContent: \"center\",\n        margin: \"0\",\n        color: \"#ffffff\",\n        padding: \"20px\",\n      }}\n    >\n      <div\n        style={{\n          backgroundColor: \"#282c34\",\n          alignItems: \"center\",\n          justifyContent: \"center\",\n          display: \"flex\",\n          flexDirection: \"column\",\n        }}\n      >\n        <Grid\n          container\n          spacing={1}\n          style={{ maxWidth: \"60rem\", maxHeight: \"70%\" }}\n        >\n          {images.map((image, index) => (\n            <Grid item xs={3} key={index}>\n              <img\n                src={image}\n                alt={`Generated ${index}`}\n                style={{\n                  display: \"block\",\n                  margin: \"0 auto\",\n                  maxWidth: \"100%\",\n                  maxHeight: \"150px\",\n                  borderRadius: \"10px\",\n                }}\n              />\n            </Grid>\n          ))}\n        </Grid>\n        <TextField\n          variant=\"outlined\"\n          value={inputPrompt}\n          onChange={handlePromptChange}\n          style={{\n            marginBottom: \"20px\",\n            marginTop: \"20px\",\n            width: \"100%\",\n            maxWidth: \"50rem\",\n            color: \"#ffffff\",\n            borderColor: \"#ffffff\",\n            borderRadius: \"10px\",\n            backgroundColor: \"#ffffff\",\n          }}\n          placeholder=\"Enter a prompt\"\n        />\n      </div>\n    </div>\n  );\n}","file":"/demo/realtime-txt2img/view/src/App.tsx","fileHash":"4b81b84990150d0718872a64bab3e32da0c1b659b7af7f4241ddd19ad3bffe64","hash":"7288b37de8155a4c2d33d67d6e9c0f8242c8d0d1d5bc560c7594878e01098277","processedContent":"function App() {\n  const [inputPrompt, setInputPrompt] = useState(\"\"); #undefined\n  const [lastPrompt, setLastPrompt] = useState(\"\"); #undefined\n  const [images, setImages] = useState(Array(16).fill(\"images/white.jpg\")); #undefined\n\n  const calculateEditDistance = (a: string, b: string) => {\n    if (a.length === 0) return b.length;\n    if (b.length === 0) return a.length;\n\n    const matrix = [];\n\n    for (let i = 0; i <= b.length; i++) {\n      matrix[i] = [i];\n    }\n    for (let i = 0; i <= a.length; i++) {\n      matrix[0][i] = i;\n    }\n\n    for (let i = 1; i <= b.length; i++) {\n      for (let j = 1; j <= a.length; j++) {\n        if (b.charAt(i - 1) === a.charAt(j - 1)) {\n          matrix[i][j] = matrix[i - 1][j - 1];\n        } else {\n          matrix[i][j] = Math.min(\n            matrix[i - 1][j - 1] + 1,\n            Math.min(matrix[i][j - 1] + 1, matrix[i - 1][j] + 1)\n          );\n        }\n      }\n    }\n\n    return matrix[b.length][a.length];\n  };\n\n  const fetchImage = useCallback( #undefined\n    async (index: number) => {\n      try {\n        const response = await fetch(\"api/predict\", { #undefined\n          method: \"POST\",\n          headers: { \"Content-Type\": \"application/json\" },\n          body: JSON.stringify({ prompt: inputPrompt }),\n        });\n        const data = await response.json();\n        const imageUrl = `data:image/jpeg;base64,${data.base64_image}`;\n\n        setImages((prevImages) => {\n          const newImages = [...prevImages];\n          newImages[index] = imageUrl;\n          return newImages;\n        });\n      } catch (error) {\n        console.error(\"Error fetching image:\", error);\n        \"\"\"\n        scip-typescript npm @types/node 20.9.3 `ts4.8`/`globals.d.ts`/console.: undefined\n        scip-typescript npm @types/node 20.9.3 `ts4.8`/`console.d.ts`/`\"node:console\"`/global/console/: undefined\n        scip-typescript npm @types/node 20.9.3 `ts4.8`/`console.d.ts`/`\"node:console\"`/global/console.: undefined\n        scip-typescript npm @types/node 20.9.3 `ts4.8`/`console.d.ts`/`\"node:console\"`/global/Console#error().: undefined\n        \"\"\"\n      }\n    },\n    [inputPrompt]\n  );\n\n  const handlePromptChange = (event: React.ChangeEvent<HTMLInputElement>) => { #undefined\n    setInputPrompt(event.target.value);\n    const newPrompt = event.target.value;\n    const editDistance = calculateEditDistance(lastPrompt, newPrompt);\n\n    if (editDistance >= 4) {\n      setInputPrompt(newPrompt);\n      setLastPrompt(newPrompt);\n      for (let i = 0; i < 16; i++) {\n        fetchImage(i);\n      }\n    }\n  };\n\n  return (\n    <div #undefined\n      className=\"App\" #undefined\n      style={{ #undefined\n        backgroundColor: \"#282c34\",\n        height: \"100vh\",\n        display: \"flex\",\n        alignItems: \"center\",\n        justifyContent: \"center\",\n        margin: \"0\",\n        color: \"#ffffff\",\n        padding: \"20px\",\n      }}\n    >\n      <div #undefined\n        style={{ #undefined\n          backgroundColor: \"#282c34\",\n          alignItems: \"center\",\n          justifyContent: \"center\",\n          display: \"flex\",\n          flexDirection: \"column\",\n        }}\n      >\n        <Grid\n          container\n          spacing={1}\n          style={{ maxWidth: \"60rem\", maxHeight: \"70%\" }}\n        >\n          {images.map((image, index) => (\n            <Grid item xs={3} key={index}>\n              <img #undefined\n                src={image} #undefined\n                alt={`Generated ${index}`} #undefined\n                style={{ #undefined\n                  display: \"block\",\n                  margin: \"0 auto\",\n                  maxWidth: \"100%\",\n                  maxHeight: \"150px\",\n                  borderRadius: \"10px\",\n                }}\n              />\n            </Grid>\n          ))}\n        </Grid>\n        <TextField\n          variant=\"outlined\"\n          value={inputPrompt}\n          onChange={handlePromptChange}\n          style={{\n            marginBottom: \"20px\",\n            marginTop: \"20px\",\n            width: \"100%\",\n            maxWidth: \"50rem\",\n            color: \"#ffffff\",\n            borderColor: \"#ffffff\",\n            borderRadius: \"10px\",\n            backgroundColor: \"#ffffff\",\n          }}\n          placeholder=\"Enter a prompt\"\n        />\n      </div> #undefined\n    </div>\n  );\n}","documentation":"This code is a React component that renders a form with a text input field and a grid of images. When the user types in the input field, the component fetches 16 images from an API based on the input prompt, and displays them in the grid. The component also calculates the edit distance between the previous and current input prompts to prevent unnecessary API calls."}},{"key":"scip-typescript npm view 0.1.0 src/`App.tsx`/","attributes":{"symbol":"scip-typescript npm view 0.1.0 src/`App.tsx`/","language":"typescript","range":[0,0,140,0],"content":"import React, { useCallback, useState } from \"react\";\nimport { TextField, Grid } from \"@mui/material\";\n\nfunction App() {\n  const [inputPrompt, setInputPrompt] = useState(\"\");\n  const [lastPrompt, setLastPrompt] = useState(\"\");\n  const [images, setImages] = useState(Array(16).fill(\"images/white.jpg\"));\n\n  const calculateEditDistance = (a: string, b: string) => {\n    if (a.length === 0) return b.length;\n    if (b.length === 0) return a.length;\n\n    const matrix = [];\n\n    for (let i = 0; i <= b.length; i++) {\n      matrix[i] = [i];\n    }\n    for (let i = 0; i <= a.length; i++) {\n      matrix[0][i] = i;\n    }\n\n    for (let i = 1; i <= b.length; i++) {\n      for (let j = 1; j <= a.length; j++) {\n        if (b.charAt(i - 1) === a.charAt(j - 1)) {\n          matrix[i][j] = matrix[i - 1][j - 1];\n        } else {\n          matrix[i][j] = Math.min(\n            matrix[i - 1][j - 1] + 1,\n            Math.min(matrix[i][j - 1] + 1, matrix[i - 1][j] + 1)\n          );\n        }\n      }\n    }\n\n    return matrix[b.length][a.length];\n  };\n\n  const fetchImage = useCallback(\n    async (index: number) => {\n      try {\n        const response = await fetch(\"api/predict\", {\n          method: \"POST\",\n          headers: { \"Content-Type\": \"application/json\" },\n          body: JSON.stringify({ prompt: inputPrompt }),\n        });\n        const data = await response.json();\n        const imageUrl = `data:image/jpeg;base64,${data.base64_image}`;\n\n        setImages((prevImages) => {\n          const newImages = [...prevImages];\n          newImages[index] = imageUrl;\n          return newImages;\n        });\n      } catch (error) {\n        console.error(\"Error fetching image:\", error);\n      }\n    },\n    [inputPrompt]\n  );\n\n  const handlePromptChange = (event: React.ChangeEvent<HTMLInputElement>) => {\n    setInputPrompt(event.target.value);\n    const newPrompt = event.target.value;\n    const editDistance = calculateEditDistance(lastPrompt, newPrompt);\n\n    if (editDistance >= 4) {\n      setInputPrompt(newPrompt);\n      setLastPrompt(newPrompt);\n      for (let i = 0; i < 16; i++) {\n        fetchImage(i);\n      }\n    }\n  };\n\n  return (\n    <div\n      className=\"App\"\n      style={{\n        backgroundColor: \"#282c34\",\n        height: \"100vh\",\n        display: \"flex\",\n        alignItems: \"center\",\n        justifyContent: \"center\",\n        margin: \"0\",\n        color: \"#ffffff\",\n        padding: \"20px\",\n      }}\n    >\n      <div\n        style={{\n          backgroundColor: \"#282c34\",\n          alignItems: \"center\",\n          justifyContent: \"center\",\n          display: \"flex\",\n          flexDirection: \"column\",\n        }}\n      >\n        <Grid\n          container\n          spacing={1}\n          style={{ maxWidth: \"60rem\", maxHeight: \"70%\" }}\n        >\n          {images.map((image, index) => (\n            <Grid item xs={3} key={index}>\n              <img\n                src={image}\n                alt={`Generated ${index}`}\n                style={{\n                  display: \"block\",\n                  margin: \"0 auto\",\n                  maxWidth: \"100%\",\n                  maxHeight: \"150px\",\n                  borderRadius: \"10px\",\n                }}\n              />\n            </Grid>\n          ))}\n        </Grid>\n        <TextField\n          variant=\"outlined\"\n          value={inputPrompt}\n          onChange={handlePromptChange}\n          style={{\n            marginBottom: \"20px\",\n            marginTop: \"20px\",\n            width: \"100%\",\n            maxWidth: \"50rem\",\n            color: \"#ffffff\",\n            borderColor: \"#ffffff\",\n            borderRadius: \"10px\",\n            backgroundColor: \"#ffffff\",\n          }}\n          placeholder=\"Enter a prompt\"\n        />\n      </div>\n    </div>\n  );\n}\n\nexport default App;\n","file":"/demo/realtime-txt2img/view/src/App.tsx","fileHash":"4b81b84990150d0718872a64bab3e32da0c1b659b7af7f4241ddd19ad3bffe64","hash":"4b81b84990150d0718872a64bab3e32da0c1b659b7af7f4241ddd19ad3bffe64","processedContent":"import React, { useCallback, useState } from \"react\";\n\"\"\"\nscip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/React/useCallback().: undefined\nscip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/React/useState().: undefined\nscip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/: undefined\n\"\"\"\nimport { TextField, Grid } from \"@mui/material\";\n\nfunction App() {\n  \"\"\"This code is a React component that renders a form with a text input field and a grid of images. When the user types in the input field, the component fetches 16 images from an API based on the input prompt, and displays them in the grid. The component also calculates the edit distance between the previous and current input prompts to prevent unnecessary API calls.\"\"\"\n  pass\n\nexport default App; #This code is a React component that renders a form with a text input field and a grid of images. When the user types in the input field, the component fetches 16 images from an API based on the input prompt, and displays them in the grid. The component also calculates the edit distance between the previous and current input prompts to prevent unnecessary API calls.\n","documentation":"This code is a React component that renders a form with a text input field and a grid of images. When the user types in the input field, the component fetches 16 images from an API based on the input prompt, and displays them in the grid. The component also calculates the edit distance between the previous and current input prompts to prevent unnecessary API calls."}},{"key":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`global.d.ts`/HTMLElement#","attributes":{"symbol":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`global.d.ts`/HTMLElement#","language":"typescript"}},{"key":"scip-typescript npm view 0.1.0 src/`index.tsx`/root.","attributes":{"symbol":"scip-typescript npm view 0.1.0 src/`index.tsx`/root.","language":"typescript"}},{"key":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/React/useCallback().","attributes":{"symbol":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/React/useCallback().","language":"typescript"}},{"key":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/React/useState().","attributes":{"symbol":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/React/useState().","language":"typescript"}},{"key":"scip-typescript npm @types/node 20.9.3 `ts4.8`/`globals.d.ts`/fetch().","attributes":{"symbol":"scip-typescript npm @types/node 20.9.3 `ts4.8`/`globals.d.ts`/fetch().","language":"typescript"}},{"key":"scip-typescript npm @types/node 20.9.3 `ts4.8`/`globals.d.ts`/console.","attributes":{"symbol":"scip-typescript npm @types/node 20.9.3 `ts4.8`/`globals.d.ts`/console.","language":"typescript"}},{"key":"scip-typescript npm @types/node 20.9.3 `ts4.8`/`console.d.ts`/`\"node:console\"`/global/console/","attributes":{"symbol":"scip-typescript npm @types/node 20.9.3 `ts4.8`/`console.d.ts`/`\"node:console\"`/global/console/","language":"typescript"}},{"key":"scip-typescript npm @types/node 20.9.3 `ts4.8`/`console.d.ts`/`\"node:console\"`/global/console.","attributes":{"symbol":"scip-typescript npm @types/node 20.9.3 `ts4.8`/`console.d.ts`/`\"node:console\"`/global/console.","language":"typescript"}},{"key":"scip-typescript npm @types/node 20.9.3 `ts4.8`/`console.d.ts`/`\"node:console\"`/global/Console#error().","attributes":{"symbol":"scip-typescript npm @types/node 20.9.3 `ts4.8`/`console.d.ts`/`\"node:console\"`/global/Console#error().","language":"typescript"}},{"key":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`global.d.ts`/HTMLInputElement#","attributes":{"symbol":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`global.d.ts`/HTMLInputElement#","language":"typescript"}},{"key":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/global/JSX/IntrinsicElements#div.","attributes":{"symbol":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/global/JSX/IntrinsicElements#div.","language":"typescript"}},{"key":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/React/HTMLAttributes#className.","attributes":{"symbol":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/React/HTMLAttributes#className.","language":"typescript"}},{"key":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/React/HTMLAttributes#style.","attributes":{"symbol":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/React/HTMLAttributes#style.","language":"typescript"}},{"key":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/global/JSX/IntrinsicElements#img.","attributes":{"symbol":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/global/JSX/IntrinsicElements#img.","language":"typescript"}},{"key":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/React/ImgHTMLAttributes#src.","attributes":{"symbol":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/React/ImgHTMLAttributes#src.","language":"typescript"}},{"key":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/React/ImgHTMLAttributes#alt.","attributes":{"symbol":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/React/ImgHTMLAttributes#alt.","language":"typescript"}},{"key":"scip-python python temp indexer setup/deps_list().","attributes":{"range":[25,0,26,38],"symbol":"scip-python python temp indexer setup/deps_list().","content":"def deps_list(*pkgs):\n    return [deps[pkg] for pkg in pkgs]","file":"/setup.py","language":"python","fileHash":"0ced25e9ee8ffe10f1d9c9f92650587743ab34a559411aeeb2fffb11d304733b","hash":"bdb6ec7e58f7ca262659baf79510bec72eaa943eb9bca36a2b4d67612439e9fd","processedContent":"def deps_list(*pkgs):\n    return [deps[pkg] for pkg in pkgs]","documentation":"This code defines a function called `deps_list` that takes in a variable number of arguments, which are then used to retrieve the dependencies for each package from a dictionary called `deps`. The function returns a list of dependencies for each package."}},{"key":"scip-python python temp indexer setup/__init__:","attributes":{"symbol":"scip-python python temp indexer setup/__init__:","range":[0,0,65,0],"content":"import os\nimport re\n\nfrom setuptools import find_packages, setup\n\n\n_deps = [\n    \"torch\",\n    \"xformers\",\n    \"diffusers\",\n    \"transformers\",\n    \"accelerate\",\n    \"fire\",\n    \"omegaconf\",\n    \"cuda-python\",\n    \"onnx==1.15.0\",\n    \"onnxruntime==1.16.3\",\n    \"protobuf==3.20.2\",\n    \"colored\",\n    \"pywin32\"\n]\n\ndeps = {b: a for a, b in (re.findall(r\"^(([^!=<>~]+)(?:[!=<>~].*)?$)\", x)[0] for x in _deps)}\n\n\ndef deps_list(*pkgs):\n    return [deps[pkg] for pkg in pkgs]\n\n\nextras = {}\nextras[\"xformers\"] = deps_list(\"xformers\")\nextras[\"torch\"] = deps_list(\"torch\", \"accelerate\")\nextras[\"tensorrt\"] = deps_list(\"protobuf\", \"cuda-python\", \"onnx\", \"onnxruntime\", \"colored\")\n\nextras[\"dev\"] = extras[\"xformers\"] + extras[\"torch\"] + extras[\"tensorrt\"]\n\nif os.name == \"nt\":\n    extras[\"tensorrt\"] = extras[\"tensorrt\"] + deps_list(\"pywin32\")\n\ninstall_requires = [\n    deps[\"fire\"],\n    deps[\"omegaconf\"],\n    deps[\"diffusers\"],\n    deps[\"transformers\"],\n]\n\nsetup(\n    name=\"streamdiffusion\",\n    version=\"0.1.0\",\n    description=\"real-time interactive image generation pipeline\",\n    long_description=open(\"README.md\", \"r\", encoding=\"utf-8\").read(),\n    long_description_content_type=\"text/markdown\",\n    keywords=\"deep learning diffusion pytorch stable diffusion audioldm streamdiffusion real-time\",\n    license=\"Apache 2.0 License\",\n    author=\"Aki, kizamimi, ddPn08, Verb, ramune, teftef6220, Tonimono, Chenfeng Xu, Ararat with the help of all our contributors (https://github.com/cumulo-autumn/StreamDiffusion/graphs/contributors)\",\n    author_email=\"cumulokyoukai@gmail.com\",\n    url=\"https://github.com/cumulo-autumn/StreamDiffusion\",\n    package_dir={\"\": \"src\"},\n    packages=find_packages(\"src\"),\n    package_data={\"streamdiffusion\": [\"py.typed\"]},\n    include_package_data=True,\n    python_requires=\">=3.10.0\",\n    install_requires=list(install_requires),\n    extras_require=extras,\n)\n","file":"/setup.py","language":"python","fileHash":"0ced25e9ee8ffe10f1d9c9f92650587743ab34a559411aeeb2fffb11d304733b","hash":"0ced25e9ee8ffe10f1d9c9f92650587743ab34a559411aeeb2fffb11d304733b","processedContent":"import os #undefined\nimport re #undefined\n\nfrom setuptools import find_packages, setup #undefined\n\n\n_deps = [\n    \"torch\",\n    \"xformers\",\n    \"diffusers\",\n    \"transformers\",\n    \"accelerate\",\n    \"fire\",\n    \"omegaconf\",\n    \"cuda-python\",\n    \"onnx==1.15.0\",\n    \"onnxruntime==1.16.3\",\n    \"protobuf==3.20.2\",\n    \"colored\",\n    \"pywin32\"\n]\n\ndeps = {b: a for a, b in (re.findall(r\"^(([^!=<>~]+)(?:[!=<>~].*)?$)\", x)[0] for x in _deps)}\n\"\"\"\nscip-python python python-stdlib 3.11 re/__init__:: undefined\nscip-python python python-stdlib 3.11 re/findall().: undefined\nscip-python python temp indexer setup/_deps.: undefined\n\"\"\"\n\n\ndef deps_list(*pkgs):\n    \"\"\"This code defines a function called `deps_list` that takes in a variable number of arguments, which are then used to retrieve the dependencies for each package from a dictionary called `deps`. The function returns a list of dependencies for each package.\"\"\"\n    pass\n\n\nextras = {}\nextras[\"xformers\"] = deps_list(\"xformers\")\n\"\"\"\nscip-python python temp indexer setup/deps_list().: This code defines a function called `deps_list` that takes in a variable number of arguments, which are then used to retrieve the dependencies for each package from a dictionary called `deps`. The function returns a list of dependencies for each package.\nscip-python python temp indexer setup/extras.: undefined\n\"\"\"\nextras[\"torch\"] = deps_list(\"torch\", \"accelerate\")\n\"\"\"\nscip-python python temp indexer setup/deps_list().: This code defines a function called `deps_list` that takes in a variable number of arguments, which are then used to retrieve the dependencies for each package from a dictionary called `deps`. The function returns a list of dependencies for each package.\nscip-python python temp indexer setup/extras.: undefined\n\"\"\"\nextras[\"tensorrt\"] = deps_list(\"protobuf\", \"cuda-python\", \"onnx\", \"onnxruntime\", \"colored\")\n\"\"\"\nscip-python python temp indexer setup/deps_list().: This code defines a function called `deps_list` that takes in a variable number of arguments, which are then used to retrieve the dependencies for each package from a dictionary called `deps`. The function returns a list of dependencies for each package.\nscip-python python temp indexer setup/extras.: undefined\n\"\"\"\n\nextras[\"dev\"] = extras[\"xformers\"] + extras[\"torch\"] + extras[\"tensorrt\"]\n\"\"\"\nscip-python python temp indexer setup/extras.: undefined\nscip-python python temp indexer setup/extras.: undefined\nscip-python python temp indexer setup/extras.: undefined\nscip-python python temp indexer setup/extras.: undefined\n\"\"\"\n\nif os.name == \"nt\":\n\"\"\"\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 os/name.name.: undefined\n\"\"\"\n    extras[\"tensorrt\"] = extras[\"tensorrt\"] + deps_list(\"pywin32\")\n    \"\"\"\n    scip-python python temp indexer setup/deps_list().: This code defines a function called `deps_list` that takes in a variable number of arguments, which are then used to retrieve the dependencies for each package from a dictionary called `deps`. The function returns a list of dependencies for each package.\n    scip-python python temp indexer setup/extras.: undefined\n    scip-python python temp indexer setup/extras.: undefined\n    \"\"\"\n\ninstall_requires = [\n    deps[\"fire\"], #undefined\n    deps[\"omegaconf\"], #undefined\n    deps[\"diffusers\"], #undefined\n    deps[\"transformers\"], #undefined\n]\n\nsetup(\n    name=\"streamdiffusion\",\n    version=\"0.1.0\",\n    description=\"real-time interactive image generation pipeline\",\n    long_description=open(\"README.md\", \"r\", encoding=\"utf-8\").read(), #undefined\n    long_description_content_type=\"text/markdown\",\n    keywords=\"deep learning diffusion pytorch stable diffusion audioldm streamdiffusion real-time\",\n    license=\"Apache 2.0 License\",\n    author=\"Aki, kizamimi, ddPn08, Verb, ramune, teftef6220, Tonimono, Chenfeng Xu, Ararat with the help of all our contributors (https://github.com/cumulo-autumn/StreamDiffusion/graphs/contributors)\",\n    author_email=\"cumulokyoukai@gmail.com\",\n    url=\"https://github.com/cumulo-autumn/StreamDiffusion\",\n    package_dir={\"\": \"src\"},\n    packages=find_packages(\"src\"),\n    package_data={\"streamdiffusion\": [\"py.typed\"]},\n    include_package_data=True,\n    python_requires=\">=3.10.0\",\n    install_requires=list(install_requires), #undefined\n    extras_require=extras, #undefined\n)\n","documentation":"This code defines a Python package called \"streamdiffusion\" that uses PyTorch and other libraries to perform real-time interactive image generation. It also includes a setup script to install the necessary dependencies."}},{"key":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"symbol":"scip-python python python-stdlib 3.11 os/__init__:","language":"python"}},{"key":"scip-python python python-stdlib 3.11 re/__init__:","attributes":{"symbol":"scip-python python python-stdlib 3.11 re/__init__:","language":"python"}},{"key":"scip-python python setuptools 68.1.2 setuptools/__init__:","attributes":{"symbol":"scip-python python setuptools 68.1.2 setuptools/__init__:","language":"python"}},{"key":"scip-python python python-stdlib 3.11 re/findall().","attributes":{"symbol":"scip-python python python-stdlib 3.11 re/findall().","language":"python"}},{"key":"scip-python python temp indexer setup/_deps.","attributes":{"symbol":"scip-python python temp indexer setup/_deps.","language":"python"}},{"key":"scip-python python temp indexer setup/deps.","attributes":{"symbol":"scip-python python temp indexer setup/deps.","language":"python"}},{"key":"scip-python python temp indexer setup/extras.","attributes":{"symbol":"scip-python python temp indexer setup/extras.","language":"python"}},{"key":"scip-python python python-stdlib 3.11 os/name.name.","attributes":{"symbol":"scip-python python python-stdlib 3.11 os/name.name.","language":"python"}},{"key":"scip-python python python-stdlib 3.11 io/TextIOBase#read().","attributes":{"symbol":"scip-python python python-stdlib 3.11 io/TextIOBase#read().","language":"python"}},{"key":"scip-python python temp indexer setup/install_requires.","attributes":{"symbol":"scip-python python temp indexer setup/install_requires.","language":"python"}},{"key":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#","attributes":{"range":[9,0,47,45],"symbol":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#","content":"@dataclass\nclass Config:\n    \"\"\"\n    The configuration for the API.\n    \"\"\"\n\n    ####################################################################\n    # Server\n    ####################################################################\n    # In most cases, you should leave this as it is.\n    host: str = \"127.0.0.1\"\n    port: int = 9090\n    workers: int = 1\n\n    ####################################################################\n    # Model configuration\n    ####################################################################\n    mode: Literal[\"txt2img\", \"img2img\"] = \"txt2img\"\n    # SD1.x variant model\n    model_id_or_path: str = \"KBlueLeaf/kohaku-v2.1\"\n    # LCM-LORA model\n    lcm_lora_id: str = \"latent-consistency/lcm-lora-sdv1-5\"\n    # TinyVAE model\n    vae_id: str = \"madebyollin/taesd\"\n    # Device to use\n    device: torch.device = torch.device(\"cuda\")\n    # Data type\n    dtype: torch.dtype = torch.float16\n    # acceleration\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"xformers\"\n\n    ####################################################################\n    # Inference configuration\n    ####################################################################\n    # Number of inference steps\n    t_index_list: List[int] = field(default_factory=lambda: [0, 16, 32, 45])\n    # Number of warmup steps\n    warmup: int = 10\n    use_safety_checker: bool = SAFETY_CHECKER","file":"/demo/realtime-txt2img/server/config.py","language":"python","fileHash":"56de3e625cb8bdaeb9ace06a897e34a555fa10ab99a338792857edb136439bbf","hash":"6305129618104c4c3ad186e614ed469c104607eb02d08d0075158ba792ea35a1","processedContent":"@dataclass #undefined\nclass Config:\n    \"\"\"\n    The configuration for the API.\n    \"\"\"\n\n    ####################################################################\n    # Server\n    ####################################################################\n    # In most cases, you should leave this as it is.\n    host: str = \"127.0.0.1\"\n    port: int = 9090\n    workers: int = 1\n\n    ####################################################################\n    # Model configuration\n    ####################################################################\n    mode: Literal[\"txt2img\", \"img2img\"] = \"txt2img\" #undefined\n    # SD1.x variant model\n    model_id_or_path: str = \"KBlueLeaf/kohaku-v2.1\"\n    # LCM-LORA model\n    lcm_lora_id: str = \"latent-consistency/lcm-lora-sdv1-5\"\n    # TinyVAE model\n    vae_id: str = \"madebyollin/taesd\"\n    # Device to use\n    device: torch.device = torch.device(\"cuda\")\n    # Data type\n    dtype: torch.dtype = torch.float16\n    # acceleration\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"xformers\" #undefined\n\n    ####################################################################\n    # Inference configuration\n    ####################################################################\n    # Number of inference steps\n    t_index_list: List[int] = field(default_factory=lambda: [0, 16, 32, 45])\n    \"\"\"\n    scip-python python python-stdlib 3.11 typing/List.: undefined\n    scip-python python python-stdlib 3.11 dataclasses/field().: undefined\n    \"\"\"\n    # Number of warmup steps\n    warmup: int = 10\n    use_safety_checker: bool = SAFETY_CHECKER","documentation":"This code defines a configuration class for an API, with various parameters that control the behavior of the API. The class includes fields for server settings, model configuration, inference configuration, and other options."}},{"key":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/__init__:","attributes":{"symbol":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/__init__:","range":[0,0,48,0],"content":"from dataclasses import dataclass, field\nfrom typing import List, Literal\n\nimport torch\nimport os\n\nSAFETY_CHECKER = os.environ.get(\"SAFETY_CHECKER\", \"False\") == \"True\"\n\n\n@dataclass\nclass Config:\n    \"\"\"\n    The configuration for the API.\n    \"\"\"\n\n    ####################################################################\n    # Server\n    ####################################################################\n    # In most cases, you should leave this as it is.\n    host: str = \"127.0.0.1\"\n    port: int = 9090\n    workers: int = 1\n\n    ####################################################################\n    # Model configuration\n    ####################################################################\n    mode: Literal[\"txt2img\", \"img2img\"] = \"txt2img\"\n    # SD1.x variant model\n    model_id_or_path: str = \"KBlueLeaf/kohaku-v2.1\"\n    # LCM-LORA model\n    lcm_lora_id: str = \"latent-consistency/lcm-lora-sdv1-5\"\n    # TinyVAE model\n    vae_id: str = \"madebyollin/taesd\"\n    # Device to use\n    device: torch.device = torch.device(\"cuda\")\n    # Data type\n    dtype: torch.dtype = torch.float16\n    # acceleration\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"xformers\"\n\n    ####################################################################\n    # Inference configuration\n    ####################################################################\n    # Number of inference steps\n    t_index_list: List[int] = field(default_factory=lambda: [0, 16, 32, 45])\n    # Number of warmup steps\n    warmup: int = 10\n    use_safety_checker: bool = SAFETY_CHECKER\n","file":"/demo/realtime-txt2img/server/config.py","language":"python","fileHash":"56de3e625cb8bdaeb9ace06a897e34a555fa10ab99a338792857edb136439bbf","hash":"56de3e625cb8bdaeb9ace06a897e34a555fa10ab99a338792857edb136439bbf","processedContent":"from dataclasses import dataclass, field\n\"\"\"\nscip-python python python-stdlib 3.11 dataclasses/__init__:: undefined\nscip-python python python-stdlib 3.11 dataclasses/dataclass().: undefined\nscip-python python python-stdlib 3.11 dataclasses/field().: undefined\n\"\"\"\nfrom typing import List, Literal\n\"\"\"\nscip-python python python-stdlib 3.11 typing/__init__:: undefined\nscip-python python python-stdlib 3.11 typing/List.: undefined\nscip-python python python-stdlib 3.11 typing/Literal.: undefined\n\"\"\"\n\nimport torch\nimport os #undefined\n\nSAFETY_CHECKER = os.environ.get(\"SAFETY_CHECKER\", \"False\") == \"True\"\n\"\"\"\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 os/environ.environ.: undefined\nscip-python python python-stdlib 3.11 typing/Mapping#get().: undefined\n\"\"\"\n\n\n@dataclass\nclass Config:\n    \"\"\"This code defines a configuration class for an API, with various parameters that control the behavior of the API. The class includes fields for server settings, model configuration, inference configuration, and other options.\"\"\"\n    pass\n","documentation":"This code defines a configuration class for an API, with various parameters that control the behavior of the API. The class includes fields for server settings, model configuration, inference configuration, and other options."}},{"key":"scip-python python python-stdlib 3.11 dataclasses/__init__:","attributes":{"symbol":"scip-python python python-stdlib 3.11 dataclasses/__init__:","language":"python"}},{"key":"scip-python python python-stdlib 3.11 dataclasses/dataclass().","attributes":{"symbol":"scip-python python python-stdlib 3.11 dataclasses/dataclass().","language":"python"}},{"key":"scip-python python python-stdlib 3.11 dataclasses/field().","attributes":{"symbol":"scip-python python python-stdlib 3.11 dataclasses/field().","language":"python"}},{"key":"scip-python python python-stdlib 3.11 typing/__init__:","attributes":{"symbol":"scip-python python python-stdlib 3.11 typing/__init__:","language":"python"}},{"key":"scip-python python python-stdlib 3.11 typing/List.","attributes":{"symbol":"scip-python python python-stdlib 3.11 typing/List.","language":"python"}},{"key":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"symbol":"scip-python python python-stdlib 3.11 typing/Literal.","language":"python"}},{"key":"scip-python python python-stdlib 3.11 os/environ.environ.","attributes":{"symbol":"scip-python python python-stdlib 3.11 os/environ.environ.","language":"python"}},{"key":"scip-python python python-stdlib 3.11 typing/Mapping#get().","attributes":{"symbol":"scip-python python python-stdlib 3.11 typing/Mapping#get().","language":"python"}},{"key":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/SAFETY_CHECKER.","attributes":{"symbol":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/SAFETY_CHECKER.","language":"python"}},{"key":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/PredictInputModel#","attributes":{"range":[25,0,30,15],"symbol":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/PredictInputModel#","content":"class PredictInputModel(BaseModel):\n    \"\"\"\n    The input model for the /predict endpoint.\n    \"\"\"\n\n    prompt: str","file":"/demo/realtime-txt2img/server/main.py","language":"python","fileHash":"e9e09566156ac6baf8ab576cf2cddb39208028d70924565c22f693ace8136007","hash":"9db324956fc4b4a95c47b243204a866e6860a6576ed05098c5bba8183dedc730","processedContent":"class PredictInputModel(BaseModel):\n    \"\"\"\n    The input model for the /predict endpoint.\n    \"\"\"\n\n    prompt: str","documentation":"The code defines a class called PredictInputModel, which is used as the input model for the /predict endpoint. It contains a single property called prompt, which is a string that represents the input data for the prediction."}},{"key":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/PredictResponseModel#","attributes":{"range":[33,0,38,21],"symbol":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/PredictResponseModel#","content":"class PredictResponseModel(BaseModel):\n    \"\"\"\n    The response model for the /predict endpoint.\n    \"\"\"\n\n    base64_image: str","file":"/demo/realtime-txt2img/server/main.py","language":"python","fileHash":"e9e09566156ac6baf8ab576cf2cddb39208028d70924565c22f693ace8136007","hash":"0de78728f006bd5a420db5b5912a233c4597a59600b1363fd447d789649090bf","processedContent":"class PredictResponseModel(BaseModel):\n    \"\"\"\n    The response model for the /predict endpoint.\n    \"\"\"\n\n    base64_image: str","documentation":"The code defines a response model for the `/predict` endpoint, which includes a single field `base64_image` that represents a base64-encoded image."}},{"key":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/UpdatePromptResponseModel#","attributes":{"range":[41,0,46,15],"symbol":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/UpdatePromptResponseModel#","content":"class UpdatePromptResponseModel(BaseModel):\n    \"\"\"\n    The response model for the /update_prompt endpoint.\n    \"\"\"\n\n    prompt: str","file":"/demo/realtime-txt2img/server/main.py","language":"python","fileHash":"e9e09566156ac6baf8ab576cf2cddb39208028d70924565c22f693ace8136007","hash":"2fd4ac2ff30283fe42ed1aa13774a9446cb75271f8add471d9669a59b7c00259","processedContent":"class UpdatePromptResponseModel(BaseModel):\n    \"\"\"\n    The response model for the /update_prompt endpoint.\n    \"\"\"\n\n    prompt: str","documentation":"The code defines a response model for the /update_prompt endpoint, which includes a prompt field that stores a string value."}},{"key":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#","attributes":{"range":[49,0,152,81],"symbol":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#","content":"class Api:\n    def __init__(self, config: Config) -> None:\n        \"\"\"\n        Initialize the API.\n\n        Parameters\n        ----------\n        config : Config\n            The configuration.\n        \"\"\"\n        self.config = config\n        self.stream_diffusion = StreamDiffusionWrapper(\n            mode=config.mode,\n            model_id_or_path=config.model_id_or_path,\n            lcm_lora_id=config.lcm_lora_id,\n            vae_id=config.vae_id,\n            device=config.device,\n            dtype=config.dtype,\n            acceleration=config.acceleration,\n            t_index_list=config.t_index_list,\n            warmup=config.warmup,\n            use_safety_checker=config.use_safety_checker,\n            cfg_type=\"none\",\n        )\n        self.app = FastAPI()\n        self.app.add_api_route(\n            \"/api/predict\",\n            self._predict,\n            methods=[\"POST\"],\n            response_model=PredictResponseModel,\n        )\n        self.app.add_middleware(\n            CORSMiddleware,\n            allow_origins=[\"*\"],\n            allow_credentials=True,\n            allow_methods=[\"*\"],\n            allow_headers=[\"*\"],\n        )\n        self.app.mount(\n            \"/\", StaticFiles(directory=\"../view/build\", html=True), name=\"public\"\n        )\n\n        self._predict_lock = asyncio.Lock()\n        self._update_prompt_lock = asyncio.Lock()\n\n    async def _predict(self, inp: PredictInputModel) -> PredictResponseModel:\n        \"\"\"\n        Predict an image and return.\n\n        Parameters\n        ----------\n        inp : PredictInputModel\n            The input.\n\n        Returns\n        -------\n        PredictResponseModel\n            The prediction result.\n        \"\"\"\n        async with self._predict_lock:\n            return PredictResponseModel(\n                base64_image=self._pil_to_base64(\n                    self.stream_diffusion(prompt=inp.prompt)\n                )\n            )\n\n    def _pil_to_base64(self, image: Image.Image, format: str = \"JPEG\") -> bytes:\n        \"\"\"\n        Convert a PIL image to base64.\n\n        Parameters\n        ----------\n        image : Image.Image\n            The PIL image.\n\n        format : str\n            The image format, by default \"JPEG\".\n\n        Returns\n        -------\n        bytes\n            The base64 image.\n        \"\"\"\n        buffered = BytesIO()\n        image.convert(\"RGB\").save(buffered, format=format)\n        return base64.b64encode(buffered.getvalue()).decode(\"ascii\")\n\n    def _base64_to_pil(self, base64_image: str) -> Image.Image:\n        \"\"\"\n        Convert a base64 image to PIL.\n\n        Parameters\n        ----------\n        base64_image : str\n            The base64 image.\n\n        Returns\n        -------\n        Image.Image\n            The PIL image.\n        \"\"\"\n        if \"base64,\" in base64_image:\n            base64_image = base64_image.split(\"base64,\")[1]\n        return Image.open(BytesIO(base64.b64decode(base64_image))).convert(\"RGB\")","file":"/demo/realtime-txt2img/server/main.py","language":"python","fileHash":"e9e09566156ac6baf8ab576cf2cddb39208028d70924565c22f693ace8136007","hash":"edc80ca09ec4724afa615ace72ea119fa900af12b409b857b2f7c944f1f903a9","processedContent":"class Api:\n    def __init__(self, config: Config) -> None:\n        \"\"\"This code defines a FastAPI endpoint for image generation using the StreamDiffusion model. It also includes middleware for CORS and static file serving.\"\"\"\n        pass\n\n    async def _predict(self, inp: PredictInputModel) -> PredictResponseModel:\n        \"\"\"This code defines a function called `_predict` that takes an input model and returns a response model with a base64-encoded image. The function uses a lock to ensure that only one prediction is made at a time, and it converts the PIL image to base64 format using the `stream_diffusion` method.\"\"\"\n        pass\n\n    def _pil_to_base64(self, image: Image.Image, format: str = \"JPEG\") -> bytes:\n        \"\"\"This code takes a PIL image and converts it to base64 format, which can be used for embedding images in HTML or other web-based applications.\"\"\"\n        pass\n\n    def _base64_to_pil(self, base64_image: str) -> Image.Image:\n        \"\"\"\n        Convert a base64 image to PIL.\n\n        Parameters\n        ----------\n        base64_image : str\n            The base64 image.\n\n        Returns\n        -------\n        Image.Image\n            The PIL image.\n        \"\"\"\n        if \"base64,\" in base64_image:\n            base64_image = base64_image.split(\"base64,\")[1]\n        return Image.open(BytesIO(base64.b64decode(base64_image))).convert(\"RGB\")","documentation":"This code defines a FastAPI endpoint for image generation using the StreamDiffusion model. It includes middleware for CORS and static file serving, and defines a function called `_predict` that takes an input model and returns a response model with a base64-encoded image."}},{"key":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#__init__().","attributes":{"range":[50,4,92,49],"symbol":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#__init__().","content":"def __init__(self, config: Config) -> None:\n        \"\"\"\n        Initialize the API.\n\n        Parameters\n        ----------\n        config : Config\n            The configuration.\n        \"\"\"\n        self.config = config\n        self.stream_diffusion = StreamDiffusionWrapper(\n            mode=config.mode,\n            model_id_or_path=config.model_id_or_path,\n            lcm_lora_id=config.lcm_lora_id,\n            vae_id=config.vae_id,\n            device=config.device,\n            dtype=config.dtype,\n            acceleration=config.acceleration,\n            t_index_list=config.t_index_list,\n            warmup=config.warmup,\n            use_safety_checker=config.use_safety_checker,\n            cfg_type=\"none\",\n        )\n        self.app = FastAPI()\n        self.app.add_api_route(\n            \"/api/predict\",\n            self._predict,\n            methods=[\"POST\"],\n            response_model=PredictResponseModel,\n        )\n        self.app.add_middleware(\n            CORSMiddleware,\n            allow_origins=[\"*\"],\n            allow_credentials=True,\n            allow_methods=[\"*\"],\n            allow_headers=[\"*\"],\n        )\n        self.app.mount(\n            \"/\", StaticFiles(directory=\"../view/build\", html=True), name=\"public\"\n        )\n\n        self._predict_lock = asyncio.Lock()\n        self._update_prompt_lock = asyncio.Lock()","file":"/demo/realtime-txt2img/server/main.py","language":"python","fileHash":"e9e09566156ac6baf8ab576cf2cddb39208028d70924565c22f693ace8136007","hash":"a42bc1ce8be602ed7300b1f0f27330fa598f1387421efc2159416c56e80f4758","processedContent":"def __init__(self, config: Config) -> None:\n        \"\"\"\n        Initialize the API.\n\n        Parameters\n        ----------\n        config : Config\n            The configuration.\n        \"\"\"\n        self.config = config\n        self.stream_diffusion = StreamDiffusionWrapper( #This code defines a class called `StreamDiffusionWrapper` that loads a model from a given path or ID, performs various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference. It also defines various methods for performing image generation and manipulation.\n            mode=config.mode, #undefined\n            model_id_or_path=config.model_id_or_path, #undefined\n            lcm_lora_id=config.lcm_lora_id, #undefined\n            vae_id=config.vae_id, #undefined\n            device=config.device, #undefined\n            dtype=config.dtype, #undefined\n            acceleration=config.acceleration, #undefined\n            t_index_list=config.t_index_list, #undefined\n            warmup=config.warmup, #undefined\n            use_safety_checker=config.use_safety_checker, #undefined\n            cfg_type=\"none\",\n        )\n        self.app = FastAPI()\n        self.app.add_api_route( #undefined\n            \"/api/predict\",\n            self._predict, #This code defines a function called `_predict` that takes an input model and returns a response model with a base64-encoded image. The function uses a lock to ensure that only one prediction is made at a time, and it converts the PIL image to base64 format using the `stream_diffusion` method.\n            methods=[\"POST\"],\n            response_model=PredictResponseModel, #The code defines a response model for the `/predict` endpoint, which includes a single field `base64_image` that represents a base64-encoded image.\n        )\n        self.app.add_middleware( #undefined\n            CORSMiddleware,\n            allow_origins=[\"*\"],\n            allow_credentials=True,\n            allow_methods=[\"*\"],\n            allow_headers=[\"*\"],\n        )\n        self.app.mount( #undefined\n            \"/\", StaticFiles(directory=\"../view/build\", html=True), name=\"public\"\n        )\n\n        self._predict_lock = asyncio.Lock()\n        \"\"\"\n        scip-python python python-stdlib 3.11 asyncio/__init__:: undefined\n        scip-python python temp indexer `asyncio.locks`/Lock#: undefined\n        \"\"\"\n        self._update_prompt_lock = asyncio.Lock()","documentation":"This code defines a FastAPI endpoint for image generation using the StreamDiffusion model. It also includes middleware for CORS and static file serving."}},{"key":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_predict().","attributes":{"range":[94,4,113,13],"symbol":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_predict().","content":"async def _predict(self, inp: PredictInputModel) -> PredictResponseModel:\n        \"\"\"\n        Predict an image and return.\n\n        Parameters\n        ----------\n        inp : PredictInputModel\n            The input.\n\n        Returns\n        -------\n        PredictResponseModel\n            The prediction result.\n        \"\"\"\n        async with self._predict_lock:\n            return PredictResponseModel(\n                base64_image=self._pil_to_base64(\n                    self.stream_diffusion(prompt=inp.prompt)\n                )\n            )","file":"/demo/realtime-txt2img/server/main.py","language":"python","fileHash":"e9e09566156ac6baf8ab576cf2cddb39208028d70924565c22f693ace8136007","hash":"94f4fd172a639cffe9f12b09740cfbcff8dcb022e6b9524eab3efb9f96208818","processedContent":"async def _predict(self, inp: PredictInputModel) -> PredictResponseModel:\n        \"\"\"\n        Predict an image and return.\n\n        Parameters\n        ----------\n        inp : PredictInputModel\n            The input.\n\n        Returns\n        -------\n        PredictResponseModel\n            The prediction result.\n        \"\"\"\n        async with self._predict_lock: #undefined\n            return PredictResponseModel( #The code defines a response model for the `/predict` endpoint, which includes a single field `base64_image` that represents a base64-encoded image.\n                base64_image=self._pil_to_base64( #This code takes a PIL image and converts it to base64 format, which can be used for embedding images in HTML or other web-based applications.\n                    self.stream_diffusion(prompt=inp.prompt)\n                    \"\"\"\n                    scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#stream_diffusion.: undefined\n                    scip-python python temp indexer `demo.realtime-txt2img.server.main`/PredictInputModel#prompt.: undefined\n                    \"\"\"\n                )\n            )","documentation":"This code defines a function called `_predict` that takes an input model and returns a response model with a base64-encoded image. The function uses a lock to ensure that only one prediction is made at a time, and it converts the PIL image to base64 format using the `stream_diffusion` method."}},{"key":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_pil_to_base64().","attributes":{"range":[115,4,134,68],"symbol":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_pil_to_base64().","content":"def _pil_to_base64(self, image: Image.Image, format: str = \"JPEG\") -> bytes:\n        \"\"\"\n        Convert a PIL image to base64.\n\n        Parameters\n        ----------\n        image : Image.Image\n            The PIL image.\n\n        format : str\n            The image format, by default \"JPEG\".\n\n        Returns\n        -------\n        bytes\n            The base64 image.\n        \"\"\"\n        buffered = BytesIO()\n        image.convert(\"RGB\").save(buffered, format=format)\n        return base64.b64encode(buffered.getvalue()).decode(\"ascii\")","file":"/demo/realtime-txt2img/server/main.py","language":"python","fileHash":"e9e09566156ac6baf8ab576cf2cddb39208028d70924565c22f693ace8136007","hash":"066ddb4e9e7b953ff07a384e7f370bbce0ce8875cc1160a316d9e5f8668bc453","processedContent":"def _pil_to_base64(self, image: Image.Image, format: str = \"JPEG\") -> bytes:\n        \"\"\"\n        Convert a PIL image to base64.\n\n        Parameters\n        ----------\n        image : Image.Image\n            The PIL image.\n\n        format : str\n            The image format, by default \"JPEG\".\n\n        Returns\n        -------\n        bytes\n            The base64 image.\n        \"\"\"\n        buffered = BytesIO() #undefined\n        image.convert(\"RGB\").save(buffered, format=format)\n        \"\"\"\n        scip-python python Pillow 10.0.0 `pil.image`/Image#convert().: undefined\n        scip-python python Pillow 10.0.0 `pil.image`/Image#save().: undefined\n        \"\"\"\n        return base64.b64encode(buffered.getvalue()).decode(\"ascii\")","documentation":"This code takes a PIL image and converts it to base64 format, which can be used for embedding images in HTML or other web-based applications."}},{"key":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_base64_to_pil().","attributes":{"range":[136,4,152,81],"symbol":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_base64_to_pil().","content":"def _base64_to_pil(self, base64_image: str) -> Image.Image:\n        \"\"\"\n        Convert a base64 image to PIL.\n\n        Parameters\n        ----------\n        base64_image : str\n            The base64 image.\n\n        Returns\n        -------\n        Image.Image\n            The PIL image.\n        \"\"\"\n        if \"base64,\" in base64_image:\n            base64_image = base64_image.split(\"base64,\")[1]\n        return Image.open(BytesIO(base64.b64decode(base64_image))).convert(\"RGB\")","file":"/demo/realtime-txt2img/server/main.py","language":"python","fileHash":"e9e09566156ac6baf8ab576cf2cddb39208028d70924565c22f693ace8136007","hash":"c7b56975175595f493355f8ea0f2801229895e551bcdef8904c8739ac0bf4031","processedContent":"def _base64_to_pil(self, base64_image: str) -> Image.Image:\n        \"\"\"\n        Convert a base64 image to PIL.\n\n        Parameters\n        ----------\n        base64_image : str\n            The base64 image.\n\n        Returns\n        -------\n        Image.Image\n            The PIL image.\n        \"\"\"\n        if \"base64,\" in base64_image:\n            base64_image = base64_image.split(\"base64,\")[1]\n        return Image.open(BytesIO(base64.b64decode(base64_image))).convert(\"RGB\")","documentation":"This code takes a base64 image string and converts it to a PIL image object, which can be used for further processing or display."}},{"key":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","attributes":{"symbol":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","range":[0,0,166,0],"content":"import sys\nimport os\nimport asyncio\nimport base64\nimport logging\nfrom io import BytesIO\nfrom pathlib import Path\n\nimport uvicorn\nfrom config import Config\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.staticfiles import StaticFiles\n\nfrom PIL import Image\nfrom pydantic import BaseModel\n\nsys.path.append(os.path.join(os.path.dirname(__file__), \"..\", \"..\", \"..\"))\n\nfrom utils.wrapper import StreamDiffusionWrapper\n\nlogger = logging.getLogger(\"uvicorn\")\nPROJECT_DIR = Path(__file__).parent.parent\n\n\nclass PredictInputModel(BaseModel):\n    \"\"\"\n    The input model for the /predict endpoint.\n    \"\"\"\n\n    prompt: str\n\n\nclass PredictResponseModel(BaseModel):\n    \"\"\"\n    The response model for the /predict endpoint.\n    \"\"\"\n\n    base64_image: str\n\n\nclass UpdatePromptResponseModel(BaseModel):\n    \"\"\"\n    The response model for the /update_prompt endpoint.\n    \"\"\"\n\n    prompt: str\n\n\nclass Api:\n    def __init__(self, config: Config) -> None:\n        \"\"\"\n        Initialize the API.\n\n        Parameters\n        ----------\n        config : Config\n            The configuration.\n        \"\"\"\n        self.config = config\n        self.stream_diffusion = StreamDiffusionWrapper(\n            mode=config.mode,\n            model_id_or_path=config.model_id_or_path,\n            lcm_lora_id=config.lcm_lora_id,\n            vae_id=config.vae_id,\n            device=config.device,\n            dtype=config.dtype,\n            acceleration=config.acceleration,\n            t_index_list=config.t_index_list,\n            warmup=config.warmup,\n            use_safety_checker=config.use_safety_checker,\n            cfg_type=\"none\",\n        )\n        self.app = FastAPI()\n        self.app.add_api_route(\n            \"/api/predict\",\n            self._predict,\n            methods=[\"POST\"],\n            response_model=PredictResponseModel,\n        )\n        self.app.add_middleware(\n            CORSMiddleware,\n            allow_origins=[\"*\"],\n            allow_credentials=True,\n            allow_methods=[\"*\"],\n            allow_headers=[\"*\"],\n        )\n        self.app.mount(\n            \"/\", StaticFiles(directory=\"../view/build\", html=True), name=\"public\"\n        )\n\n        self._predict_lock = asyncio.Lock()\n        self._update_prompt_lock = asyncio.Lock()\n\n    async def _predict(self, inp: PredictInputModel) -> PredictResponseModel:\n        \"\"\"\n        Predict an image and return.\n\n        Parameters\n        ----------\n        inp : PredictInputModel\n            The input.\n\n        Returns\n        -------\n        PredictResponseModel\n            The prediction result.\n        \"\"\"\n        async with self._predict_lock:\n            return PredictResponseModel(\n                base64_image=self._pil_to_base64(\n                    self.stream_diffusion(prompt=inp.prompt)\n                )\n            )\n\n    def _pil_to_base64(self, image: Image.Image, format: str = \"JPEG\") -> bytes:\n        \"\"\"\n        Convert a PIL image to base64.\n\n        Parameters\n        ----------\n        image : Image.Image\n            The PIL image.\n\n        format : str\n            The image format, by default \"JPEG\".\n\n        Returns\n        -------\n        bytes\n            The base64 image.\n        \"\"\"\n        buffered = BytesIO()\n        image.convert(\"RGB\").save(buffered, format=format)\n        return base64.b64encode(buffered.getvalue()).decode(\"ascii\")\n\n    def _base64_to_pil(self, base64_image: str) -> Image.Image:\n        \"\"\"\n        Convert a base64 image to PIL.\n\n        Parameters\n        ----------\n        base64_image : str\n            The base64 image.\n\n        Returns\n        -------\n        Image.Image\n            The PIL image.\n        \"\"\"\n        if \"base64,\" in base64_image:\n            base64_image = base64_image.split(\"base64,\")[1]\n        return Image.open(BytesIO(base64.b64decode(base64_image))).convert(\"RGB\")\n\n\nif __name__ == \"__main__\":\n    from config import Config\n\n    config = Config()\n\n    uvicorn.run(\n        Api(config).app,\n        host=config.host,\n        port=config.port,\n        workers=config.workers,\n    )\n","file":"/demo/realtime-txt2img/server/main.py","language":"python","fileHash":"e9e09566156ac6baf8ab576cf2cddb39208028d70924565c22f693ace8136007","hash":"e9e09566156ac6baf8ab576cf2cddb39208028d70924565c22f693ace8136007","processedContent":"import sys #undefined\nimport os #undefined\nimport asyncio #undefined\nimport base64 #undefined\nimport logging #undefined\nfrom io import BytesIO\n\"\"\"\nscip-python python python-stdlib 3.11 io/__init__:: undefined\nscip-python python python-stdlib 3.11 io/BytesIO#: undefined\n\"\"\"\nfrom pathlib import Path\n\"\"\"\nscip-python python python-stdlib 3.11 pathlib/__init__:: undefined\nscip-python python python-stdlib 3.11 pathlib/Path#: undefined\n\"\"\"\n\nimport uvicorn\nfrom config import Config\n\"\"\"\nscip-python python temp indexer `demo.realtime-txt2img.server.config`/__init__:: This code defines a configuration class for an API, with various parameters that control the behavior of the API. The class includes fields for server settings, model configuration, inference configuration, and other options.\nscip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#: This code defines a configuration class for an API, with various parameters that control the behavior of the API. The class includes fields for server settings, model configuration, inference configuration, and other options.\n\"\"\"\nfrom fastapi import FastAPI #undefined\nfrom fastapi.middleware.cors import CORSMiddleware #undefined\nfrom fastapi.staticfiles import StaticFiles #undefined\n\nfrom PIL import Image\n\"\"\"\nscip-python python Pillow 10.0.0 PIL/__init__:: undefined\nscip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\n\"\"\"\nfrom pydantic import BaseModel #undefined\n\nsys.path.append(os.path.join(os.path.dirname(__file__), \"..\", \"..\", \"..\"))\n\"\"\"\nscip-python python python-stdlib 3.11 sys/__init__:: undefined\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 sys/path.path.: undefined\nscip-python python python-stdlib 3.11 os/path.: undefined\nscip-python python python-stdlib 3.11 os/path.: undefined\nscip-python python python-stdlib 3.11 ntpath/join().: undefined\nscip-python python python-stdlib 3.11 ntpath/join().: undefined\n\"\"\"\n\nfrom utils.wrapper import StreamDiffusionWrapper\n\"\"\"\nscip-python python temp indexer `utils.wrapper`/__init__:: This code defines a class called `StreamDiffusionWrapper` that loads a model for image generation and manipulation, enabling acceleration, and preparing the model for inference.\nscip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#: This code defines a class called `StreamDiffusionWrapper` that loads a model from a given path or ID, performs various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference. It also defines various methods for performing image generation and manipulation.\n\"\"\"\n\nlogger = logging.getLogger(\"uvicorn\")\n\"\"\"\nscip-python python python-stdlib 3.11 logging/__init__:: undefined\nscip-python python python-stdlib 3.11 logging/getLogger().: undefined\n\"\"\"\nPROJECT_DIR = Path(__file__).parent.parent\n\"\"\"\nscip-python python python-stdlib 3.11 pathlib/Path#: undefined\nscip-python python temp indexer `demo.realtime-txt2img.server.main`/: undefined\nscip-python python python-stdlib 3.11 pathlib/PurePath#parent().: undefined\nscip-python python python-stdlib 3.11 pathlib/PurePath#parent().: undefined\n\"\"\"\n\n\nclass PredictInputModel(BaseModel):\n    \"\"\"The code defines a class called PredictInputModel, which is used as the input model for the /predict endpoint. It contains a single property called prompt, which is a string that represents the input data for the prediction.\"\"\"\n    pass\n\n\nclass PredictResponseModel(BaseModel):\n    \"\"\"The code defines a response model for the `/predict` endpoint, which includes a single field `base64_image` that represents a base64-encoded image.\"\"\"\n    pass\n\n\nclass UpdatePromptResponseModel(BaseModel):\n    \"\"\"The code defines a response model for the /update_prompt endpoint, which includes a prompt field that stores a string value.\"\"\"\n    pass\n\n\nclass Api:\n    \"\"\"This code defines a FastAPI endpoint for image generation using the StreamDiffusion model. It includes middleware for CORS and static file serving, and defines a function called `_predict` that takes an input model and returns a response model with a base64-encoded image.\"\"\"\n    pass\n\n\nif __name__ == \"__main__\": #undefined\n    from config import Config\n    \"\"\"\n    scip-python python temp indexer `demo.realtime-txt2img.server.config`/__init__:: This code defines a configuration class for an API, with various parameters that control the behavior of the API. The class includes fields for server settings, model configuration, inference configuration, and other options.\n    scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#: This code defines a configuration class for an API, with various parameters that control the behavior of the API. The class includes fields for server settings, model configuration, inference configuration, and other options.\n    \"\"\"\n\n    config = Config() #This code defines a configuration class for an API, with various parameters that control the behavior of the API. The class includes fields for server settings, model configuration, inference configuration, and other options.\n\n    uvicorn.run(\n        Api(config).app,\n        \"\"\"\n        scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#: This code defines a FastAPI endpoint for image generation using the StreamDiffusion model. It includes middleware for CORS and static file serving, and defines a function called `_predict` that takes an input model and returns a response model with a base64-encoded image.\n        scip-python python temp indexer `demo.realtime-txt2img.server.main`/config.: undefined\n        scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#app.: undefined\n        \"\"\"\n        host=config.host,\n        \"\"\"\n        scip-python python temp indexer `demo.realtime-txt2img.server.main`/config.: undefined\n        scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#host.: undefined\n        \"\"\"\n        port=config.port,\n        \"\"\"\n        scip-python python temp indexer `demo.realtime-txt2img.server.main`/config.: undefined\n        scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#port.: undefined\n        \"\"\"\n        workers=config.workers,\n        \"\"\"\n        scip-python python temp indexer `demo.realtime-txt2img.server.main`/config.: undefined\n        scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#workers.: undefined\n        \"\"\"\n    )\n","documentation":"This code defines a FastAPI endpoint for image generation using the StreamDiffusion model, with various parameters that control the behavior of the API. It includes middleware for CORS and static file serving, and defines a function called `_predict` that takes an input model and returns a response model with a base64-encoded image."}},{"key":"scip-python python python-stdlib 3.11 sys/__init__:","attributes":{"symbol":"scip-python python python-stdlib 3.11 sys/__init__:","language":"python"}},{"key":"scip-python python python-stdlib 3.11 asyncio/__init__:","attributes":{"symbol":"scip-python python python-stdlib 3.11 asyncio/__init__:","language":"python"}},{"key":"scip-python python python-stdlib 3.11 base64/__init__:","attributes":{"symbol":"scip-python python python-stdlib 3.11 base64/__init__:","language":"python"}},{"key":"scip-python python python-stdlib 3.11 logging/__init__:","attributes":{"symbol":"scip-python python python-stdlib 3.11 logging/__init__:","language":"python"}},{"key":"scip-python python python-stdlib 3.11 io/__init__:","attributes":{"symbol":"scip-python python python-stdlib 3.11 io/__init__:","language":"python"}},{"key":"scip-python python python-stdlib 3.11 io/BytesIO#","attributes":{"symbol":"scip-python python python-stdlib 3.11 io/BytesIO#","language":"python"}},{"key":"scip-python python python-stdlib 3.11 pathlib/__init__:","attributes":{"symbol":"scip-python python python-stdlib 3.11 pathlib/__init__:","language":"python"}},{"key":"scip-python python python-stdlib 3.11 pathlib/Path#","attributes":{"symbol":"scip-python python python-stdlib 3.11 pathlib/Path#","language":"python"}},{"key":"scip-python python temp indexer fastapi/__init__:","attributes":{"symbol":"scip-python python temp indexer fastapi/__init__:","language":"python"}},{"key":"scip-python python temp indexer `fastapi.middleware.cors`/__init__:","attributes":{"symbol":"scip-python python temp indexer `fastapi.middleware.cors`/__init__:","language":"python"}},{"key":"scip-python python temp indexer `fastapi.staticfiles`/__init__:","attributes":{"symbol":"scip-python python temp indexer `fastapi.staticfiles`/__init__:","language":"python"}},{"key":"scip-python python Pillow 10.0.0 PIL/__init__:","attributes":{"symbol":"scip-python python Pillow 10.0.0 PIL/__init__:","language":"python"}},{"key":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"symbol":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","language":"python"}},{"key":"scip-python python temp indexer pydantic/__init__:","attributes":{"symbol":"scip-python python temp indexer pydantic/__init__:","language":"python"}},{"key":"scip-python python python-stdlib 3.11 sys/path.path.","attributes":{"symbol":"scip-python python python-stdlib 3.11 sys/path.path.","language":"python"}},{"key":"scip-python python python-stdlib 3.11 os/path.","attributes":{"symbol":"scip-python python python-stdlib 3.11 os/path.","language":"python"}},{"key":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"symbol":"scip-python python python-stdlib 3.11 ntpath/join().","language":"python"}},{"key":"scip-python python temp indexer `utils.wrapper`/__init__:","attributes":{"symbol":"scip-python python temp indexer `utils.wrapper`/__init__:","language":"python","range":[0,0,657,0],"content":"import gc\nimport os\nfrom pathlib import Path\nimport traceback\nfrom typing import List, Literal, Optional, Union, Dict\n\nimport numpy as np\nimport torch\nfrom diffusers import AutoencoderTiny, StableDiffusionPipeline\nfrom PIL import Image\n\nfrom streamdiffusion import StreamDiffusion\nfrom streamdiffusion.image_utils import postprocess_image\n\n\ntorch.set_grad_enabled(False)\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\n\n\nclass StreamDiffusionWrapper:\n    def __init__(\n        self,\n        model_id_or_path: str,\n        t_index_list: List[int],\n        lora_dict: Optional[Dict[str, float]] = None,\n        mode: Literal[\"img2img\", \"txt2img\"] = \"img2img\",\n        output_type: Literal[\"pil\", \"pt\", \"np\", \"latent\"] = \"pil\",\n        lcm_lora_id: Optional[str] = None,\n        vae_id: Optional[str] = None,\n        device: Literal[\"cpu\", \"cuda\"] = \"cuda\",\n        dtype: torch.dtype = torch.float16,\n        frame_buffer_size: int = 1,\n        width: int = 512,\n        height: int = 512,\n        warmup: int = 10,\n        acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"tensorrt\",\n        do_add_noise: bool = True,\n        device_ids: Optional[List[int]] = None,\n        use_lcm_lora: bool = True,\n        use_tiny_vae: bool = True,\n        enable_similar_image_filter: bool = False,\n        similar_image_filter_threshold: float = 0.98,\n        similar_image_filter_max_skip_frame: int = 10,\n        use_denoising_batch: bool = True,\n        cfg_type: Literal[\"none\", \"full\", \"self\", \"initialize\"] = \"self\",\n        seed: int = 2,\n        use_safety_checker: bool = False,\n    ):\n        \"\"\"\n        Initializes the StreamDiffusionWrapper.\n\n        Parameters\n        ----------\n        model_id_or_path : str\n            The model id or path to load.\n        t_index_list : List[int]\n            The t_index_list to use for inference.\n        lora_dict : Optional[Dict[str, float]], optional\n            The lora_dict to load, by default None.\n            Keys are the LoRA names and values are the LoRA scales.\n            Example: {\"LoRA_1\" : 0.5 , \"LoRA_2\" : 0.7 ,...}\n        mode : Literal[\"img2img\", \"txt2img\"], optional\n            txt2img or img2img, by default \"img2img\".\n        output_type : Literal[\"pil\", \"pt\", \"np\", \"latent\"], optional\n            The output type of image, by default \"pil\".\n        lcm_lora_id : Optional[str], optional\n            The lcm_lora_id to load, by default None.\n            If None, the default LCM-LoRA\n            (\"latent-consistency/lcm-lora-sdv1-5\") will be used.\n        vae_id : Optional[str], optional\n            The vae_id to load, by default None.\n            If None, the default TinyVAE\n            (\"madebyollin/taesd\") will be used.\n        device : Literal[\"cpu\", \"cuda\"], optional\n            The device to use for inference, by default \"cuda\".\n        dtype : torch.dtype, optional\n            The dtype for inference, by default torch.float16.\n        frame_buffer_size : int, optional\n            The frame buffer size for denoising batch, by default 1.\n        width : int, optional\n            The width of the image, by default 512.\n        height : int, optional\n            The height of the image, by default 512.\n        warmup : int, optional\n            The number of warmup steps to perform, by default 10.\n        acceleration : Literal[\"none\", \"xformers\", \"tensorrt\"], optional\n            The acceleration method, by default \"tensorrt\".\n        do_add_noise : bool, optional\n            Whether to add noise for following denoising steps or not,\n            by default True.\n        device_ids : Optional[List[int]], optional\n            The device ids to use for DataParallel, by default None.\n        use_lcm_lora : bool, optional\n            Whether to use LCM-LoRA or not, by default True.\n        use_tiny_vae : bool, optional\n            Whether to use TinyVAE or not, by default True.\n        enable_similar_image_filter : bool, optional\n            Whether to enable similar image filter or not,\n            by default False.\n        similar_image_filter_threshold : float, optional\n            The threshold for similar image filter, by default 0.98.\n        similar_image_filter_max_skip_frame : int, optional\n            The max skip frame for similar image filter, by default 10.\n        use_denoising_batch : bool, optional\n            Whether to use denoising batch or not, by default True.\n        cfg_type : Literal[\"none\", \"full\", \"self\", \"initialize\"],\n        optional\n            The cfg_type for img2img mode, by default \"self\".\n            You cannot use anything other than \"none\" for txt2img mode.\n        seed : int, optional\n            The seed, by default 2.\n        use_safety_checker : bool, optional\n            Whether to use safety checker or not, by default False.\n        \"\"\"\n        self.sd_turbo = \"turbo\" in model_id_or_path\n\n        if mode == \"txt2img\":\n            if cfg_type != \"none\":\n                raise ValueError(\n                    f\"txt2img mode accepts only cfg_type = 'none', but got {cfg_type}\"\n                )\n            if use_denoising_batch and frame_buffer_size > 1:\n                if not self.sd_turbo:\n                    raise ValueError(\n                        \"txt2img mode cannot use denoising batch with frame_buffer_size > 1.\"\n                    )\n\n        if mode == \"img2img\":\n            if not use_denoising_batch:\n                raise NotImplementedError(\n                    \"img2img mode must use denoising batch for now.\"\n                )\n\n        self.device = device\n        self.dtype = dtype\n        self.width = width\n        self.height = height\n        self.mode = mode\n        self.output_type = output_type\n        self.frame_buffer_size = frame_buffer_size\n        self.batch_size = (\n            len(t_index_list) * frame_buffer_size\n            if use_denoising_batch\n            else frame_buffer_size\n        )\n\n        self.use_denoising_batch = use_denoising_batch\n        self.use_safety_checker = use_safety_checker\n\n        self.stream: StreamDiffusion = self._load_model(\n            model_id_or_path=model_id_or_path,\n            lora_dict=lora_dict,\n            lcm_lora_id=lcm_lora_id,\n            vae_id=vae_id,\n            t_index_list=t_index_list,\n            acceleration=acceleration,\n            warmup=warmup,\n            do_add_noise=do_add_noise,\n            use_lcm_lora=use_lcm_lora,\n            use_tiny_vae=use_tiny_vae,\n            cfg_type=cfg_type,\n            seed=seed,\n        )\n\n        if device_ids is not None:\n            self.stream.unet = torch.nn.DataParallel(\n                self.stream.unet, device_ids=device_ids\n            )\n\n        if enable_similar_image_filter:\n            self.stream.enable_similar_image_filter(similar_image_filter_threshold, similar_image_filter_max_skip_frame)\n\n    def prepare(\n        self,\n        prompt: str,\n        negative_prompt: str = \"\",\n        num_inference_steps: int = 50,\n        guidance_scale: float = 1.2,\n        delta: float = 1.0,\n    ) -> None:\n        \"\"\"\n        Prepares the model for inference.\n\n        Parameters\n        ----------\n        prompt : str\n            The prompt to generate images from.\n        num_inference_steps : int, optional\n            The number of inference steps to perform, by default 50.\n        guidance_scale : float, optional\n            The guidance scale to use, by default 1.2.\n        delta : float, optional\n            The delta multiplier of virtual residual noise,\n            by default 1.0.\n        \"\"\"\n        self.stream.prepare(\n            prompt,\n            negative_prompt,\n            num_inference_steps=num_inference_steps,\n            guidance_scale=guidance_scale,\n            delta=delta,\n        )\n\n    def __call__(\n        self,\n        image: Optional[Union[str, Image.Image, torch.Tensor]] = None,\n        prompt: Optional[str] = None,\n    ) -> Union[Image.Image, List[Image.Image]]:\n        \"\"\"\n        Performs img2img or txt2img based on the mode.\n\n        Parameters\n        ----------\n        image : Optional[Union[str, Image.Image, torch.Tensor]]\n            The image to generate from.\n        prompt : Optional[str]\n            The prompt to generate images from.\n\n        Returns\n        -------\n        Union[Image.Image, List[Image.Image]]\n            The generated image.\n        \"\"\"\n        if self.mode == \"img2img\":\n            return self.img2img(image)\n        else:\n            return self.txt2img(prompt)\n\n    def txt2img(\n        self, prompt: Optional[str] = None\n    ) -> Union[Image.Image, List[Image.Image], torch.Tensor, np.ndarray]:\n        \"\"\"\n        Performs txt2img.\n\n        Parameters\n        ----------\n        prompt : Optional[str]\n            The prompt to generate images from.\n\n        Returns\n        -------\n        Union[Image.Image, List[Image.Image]]\n            The generated image.\n        \"\"\"\n        if prompt is not None:\n            self.stream.update_prompt(prompt)\n\n        if self.sd_turbo:\n            image_tensor = self.stream.txt2img_sd_turbo(self.batch_size)\n        else:\n            image_tensor = self.stream.txt2img(self.frame_buffer_size)\n        image = self.postprocess_image(image_tensor, output_type=self.output_type)\n\n        if self.use_safety_checker:\n            safety_checker_input = self.feature_extractor(\n                image, return_tensors=\"pt\"\n            ).to(self.device)\n            _, has_nsfw_concept = self.safety_checker(\n                images=image_tensor.to(self.dtype),\n                clip_input=safety_checker_input.pixel_values.to(self.dtype),\n            )\n            image = self.nsfw_fallback_img if has_nsfw_concept[0] else image\n\n        return image\n\n    def img2img(\n        self, image: Union[str, Image.Image, torch.Tensor]\n    ) -> Union[Image.Image, List[Image.Image], torch.Tensor, np.ndarray]:\n        \"\"\"\n        Performs img2img.\n\n        Parameters\n        ----------\n        image : Union[str, Image.Image, torch.Tensor]\n            The image to generate from.\n\n        Returns\n        -------\n        Image.Image\n            The generated image.\n        \"\"\"\n        if isinstance(image, str) or isinstance(image, Image.Image):\n            image = self.preprocess_image(image)\n\n        image_tensor = self.stream(image)\n        image = self.postprocess_image(image_tensor, output_type=self.output_type)\n\n        if self.use_safety_checker:\n            safety_checker_input = self.feature_extractor(\n                image, return_tensors=\"pt\"\n            ).to(self.device)\n            _, has_nsfw_concept = self.safety_checker(\n                images=image_tensor.to(self.dtype),\n                clip_input=safety_checker_input.pixel_values.to(self.dtype),\n            )\n            image = self.nsfw_fallback_img if has_nsfw_concept[0] else image\n\n        return image\n\n    def preprocess_image(self, image: Union[str, Image.Image]) -> torch.Tensor:\n        \"\"\"\n        Preprocesses the image.\n\n        Parameters\n        ----------\n        image : Union[str, Image.Image, torch.Tensor]\n            The image to preprocess.\n\n        Returns\n        -------\n        torch.Tensor\n            The preprocessed image.\n        \"\"\"\n        if isinstance(image, str):\n            image = Image.open(image).convert(\"RGB\").resize((self.width, self.height))\n        if isinstance(image, Image.Image):\n            image = image.convert(\"RGB\").resize((self.width, self.height))\n\n        return self.stream.image_processor.preprocess(\n            image, self.height, self.width\n        ).to(device=self.device, dtype=self.dtype)\n\n    def postprocess_image(\n        self, image_tensor: torch.Tensor, output_type: str = \"pil\"\n    ) -> Union[Image.Image, List[Image.Image], torch.Tensor, np.ndarray]:\n        \"\"\"\n        Postprocesses the image.\n\n        Parameters\n        ----------\n        image_tensor : torch.Tensor\n            The image tensor to postprocess.\n\n        Returns\n        -------\n        Union[Image.Image, List[Image.Image]]\n            The postprocessed image.\n        \"\"\"\n        if self.frame_buffer_size > 1:\n            return postprocess_image(image_tensor.cpu(), output_type=output_type)\n        else:\n            return postprocess_image(image_tensor.cpu(), output_type=output_type)[0]\n\n    def _load_model(\n        self,\n        model_id_or_path: str,\n        t_index_list: List[int],\n        lora_dict: Optional[Dict[str, float]] = None,\n        lcm_lora_id: Optional[str] = None,\n        vae_id: Optional[str] = None,\n        acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"tensorrt\",\n        warmup: int = 10,\n        do_add_noise: bool = True,\n        use_lcm_lora: bool = True,\n        use_tiny_vae: bool = True,\n        cfg_type: Literal[\"none\", \"full\", \"self\", \"initialize\"] = \"self\",\n        seed: int = 2,\n    ) -> StreamDiffusion:\n        \"\"\"\n        Loads the model.\n\n        This method does the following:\n\n        1. Loads the model from the model_id_or_path.\n        2. Loads and fuses the LCM-LoRA model from the lcm_lora_id if needed.\n        3. Loads the VAE model from the vae_id if needed.\n        4. Enables acceleration if needed.\n        5. Prepares the model for inference.\n        6. Load the safety checker if needed.\n\n        Parameters\n        ----------\n        model_id_or_path : str\n            The model id or path to load.\n        t_index_list : List[int]\n            The t_index_list to use for inference.\n        lora_dict : Optional[Dict[str, float]], optional\n            The lora_dict to load, by default None.\n            Keys are the LoRA names and values are the LoRA scales.\n            Example: {\"LoRA_1\" : 0.5 , \"LoRA_2\" : 0.7 ,...}\n        lcm_lora_id : Optional[str], optional\n            The lcm_lora_id to load, by default None.\n        vae_id : Optional[str], optional\n            The vae_id to load, by default None.\n        acceleration : Literal[\"none\", \"xfomers\", \"sfast\", \"tensorrt\"], optional\n            The acceleration method, by default \"tensorrt\".\n        warmup : int, optional\n            The number of warmup steps to perform, by default 10.\n        do_add_noise : bool, optional\n            Whether to add noise for following denoising steps or not,\n            by default True.\n        use_lcm_lora : bool, optional\n            Whether to use LCM-LoRA or not, by default True.\n        use_tiny_vae : bool, optional\n            Whether to use TinyVAE or not, by default True.\n        cfg_type : Literal[\"none\", \"full\", \"self\", \"initialize\"],\n        optional\n            The cfg_type for img2img mode, by default \"self\".\n            You cannot use anything other than \"none\" for txt2img mode.\n        seed : int, optional\n            The seed, by default 2.\n\n        Returns\n        -------\n        StreamDiffusion\n            The loaded model.\n        \"\"\"\n\n        try:  # Load from local directory\n            pipe: StableDiffusionPipeline = StableDiffusionPipeline.from_pretrained(\n                model_id_or_path,\n            ).to(device=self.device, dtype=self.dtype)\n\n        except ValueError:  # Load from huggingface\n            pipe: StableDiffusionPipeline = StableDiffusionPipeline.from_single_file(\n                model_id_or_path,\n            ).to(device=self.device, dtype=self.dtype)\n        except Exception:  # No model found\n            traceback.print_exc()\n            print(\"Model load has failed. Doesn't exist.\")\n            exit()\n\n        stream = StreamDiffusion(\n            pipe=pipe,\n            t_index_list=t_index_list,\n            torch_dtype=self.dtype,\n            width=self.width,\n            height=self.height,\n            do_add_noise=do_add_noise,\n            frame_buffer_size=self.frame_buffer_size,\n            use_denoising_batch=self.use_denoising_batch,\n            cfg_type=cfg_type,\n        )\n        if not self.sd_turbo:\n            if use_lcm_lora:\n                if lcm_lora_id is not None:\n                    stream.load_lcm_lora(\n                        pretrained_model_name_or_path_or_dict=lcm_lora_id\n                    )\n                else:\n                    stream.load_lcm_lora()\n                stream.fuse_lora()\n\n            if lora_dict is not None:\n                for lora_name, lora_scale in lora_dict.items():\n                    stream.load_lora(lora_name)\n                    stream.fuse_lora(lora_scale=lora_scale)\n                    print(f\"Use LoRA: {lora_name} in weights {lora_scale}\")\n\n        if use_tiny_vae:\n            if vae_id is not None:\n                stream.vae = AutoencoderTiny.from_pretrained(vae_id).to(\n                    device=pipe.device, dtype=pipe.dtype\n                )\n            else:\n                stream.vae = AutoencoderTiny.from_pretrained(\"madebyollin/taesd\").to(\n                    device=pipe.device, dtype=pipe.dtype\n                )\n\n        try:\n            if acceleration == \"xformers\":\n                stream.pipe.enable_xformers_memory_efficient_attention()\n            if acceleration == \"tensorrt\":\n                from polygraphy import cuda\n                from streamdiffusion.acceleration.tensorrt import (\n                    TorchVAEEncoder,\n                    compile_unet,\n                    compile_vae_decoder,\n                    compile_vae_encoder,\n                )\n                from streamdiffusion.acceleration.tensorrt.engine import (\n                    AutoencoderKLEngine,\n                    UNet2DConditionModelEngine,\n                )\n                from streamdiffusion.acceleration.tensorrt.models import (\n                    VAE,\n                    UNet,\n                    VAEEncoder,\n                )\n\n                def create_prefix(\n                    model_id_or_path: str,\n                    max_batch_size: int,\n                    min_batch_size: int,\n                ):\n                    maybe_path = Path(model_id_or_path)\n                    if maybe_path.exists():\n                        return f\"{maybe_path.stem}--lcm_lora-{use_lcm_lora}--tiny_vae-{use_tiny_vae}--max_batch-{max_batch_size}--min_batch-{min_batch_size}--mode-{self.mode}\"\n                    else:\n                        return f\"{model_id_or_path}--lcm_lora-{use_lcm_lora}--tiny_vae-{use_tiny_vae}--max_batch-{max_batch_size}--min_batch-{min_batch_size}--mode-{self.mode}\"\n\n                engine_dir = os.path.join(\"engines\")\n                unet_path = os.path.join(\n                    engine_dir,\n                    create_prefix(\n                        model_id_or_path=model_id_or_path,\n                        max_batch_size=stream.trt_unet_batch_size,\n                        min_batch_size=stream.trt_unet_batch_size,\n                    ),\n                    \"unet.engine\",\n                )\n                vae_encoder_path = os.path.join(\n                    engine_dir,\n                    create_prefix(\n                        model_id_or_path=model_id_or_path,\n                        max_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                        min_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                    ),\n                    \"vae_encoder.engine\",\n                )\n                vae_decoder_path = os.path.join(\n                    engine_dir,\n                    create_prefix(\n                        model_id_or_path=model_id_or_path,\n                        max_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                        min_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                    ),\n                    \"vae_decoder.engine\",\n                )\n\n                if not os.path.exists(unet_path):\n                    os.makedirs(os.path.dirname(unet_path), exist_ok=True)\n                    unet_model = UNet(\n                        fp16=True,\n                        device=stream.device,\n                        max_batch_size=stream.trt_unet_batch_size,\n                        min_batch_size=stream.trt_unet_batch_size,\n                        embedding_dim=stream.text_encoder.config.hidden_size,\n                        unet_dim=stream.unet.config.in_channels,\n                    )\n                    compile_unet(\n                        stream.unet,\n                        unet_model,\n                        unet_path + \".onnx\",\n                        unet_path + \".opt.onnx\",\n                        unet_path,\n                        opt_batch_size=stream.trt_unet_batch_size,\n                    )\n\n                if not os.path.exists(vae_decoder_path):\n                    os.makedirs(os.path.dirname(vae_decoder_path), exist_ok=True)\n                    stream.vae.forward = stream.vae.decode\n                    vae_decoder_model = VAE(\n                        device=stream.device,\n                        max_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                        min_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                    )\n                    compile_vae_decoder(\n                        stream.vae,\n                        vae_decoder_model,\n                        vae_decoder_path + \".onnx\",\n                        vae_decoder_path + \".opt.onnx\",\n                        vae_decoder_path,\n                        opt_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                    )\n                    delattr(stream.vae, \"forward\")\n\n                if not os.path.exists(vae_encoder_path):\n                    os.makedirs(os.path.dirname(vae_encoder_path), exist_ok=True)\n                    vae_encoder = TorchVAEEncoder(stream.vae).to(torch.device(\"cuda\"))\n                    vae_encoder_model = VAEEncoder(\n                        device=stream.device,\n                        max_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                        min_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                    )\n                    compile_vae_encoder(\n                        vae_encoder,\n                        vae_encoder_model,\n                        vae_encoder_path + \".onnx\",\n                        vae_encoder_path + \".opt.onnx\",\n                        vae_encoder_path,\n                        opt_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                    )\n\n                cuda_steram = cuda.Stream()\n\n                vae_config = stream.vae.config\n                vae_dtype = stream.vae.dtype\n\n                stream.unet = UNet2DConditionModelEngine(\n                    unet_path, cuda_steram, use_cuda_graph=False\n                )\n                stream.vae = AutoencoderKLEngine(\n                    vae_encoder_path,\n                    vae_decoder_path,\n                    cuda_steram,\n                    stream.pipe.vae_scale_factor,\n                    use_cuda_graph=False,\n                )\n                setattr(stream.vae, \"config\", vae_config)\n                setattr(stream.vae, \"dtype\", vae_dtype)\n\n                gc.collect()\n                torch.cuda.empty_cache()\n\n                print(\"TensorRT acceleration enabled.\")\n            if acceleration == \"sfast\":\n                from streamdiffusion.acceleration.sfast import (\n                    accelerate_with_stable_fast,\n                )\n\n                stream = accelerate_with_stable_fast(stream)\n                print(\"StableFast acceleration enabled.\")\n        except Exception:\n            traceback.print_exc()\n            print(\"Acceleration has failed. Falling back to normal mode.\")\n\n        if seed < 0: # Random seed\n            seed = np.random.randint(0, 1000000)\n\n        stream.prepare(\n            \"\",\n            \"\",\n            num_inference_steps=50,\n            guidance_scale=1.1\n            if stream.cfg_type in [\"full\", \"self\", \"initialize\"]\n            else 1.0,\n            generator=torch.manual_seed(seed),\n            seed=seed,\n        )\n\n        if self.use_safety_checker:\n            from transformers import CLIPFeatureExtractor\n            from diffusers.pipelines.stable_diffusion.safety_checker import (\n                StableDiffusionSafetyChecker,\n            )\n\n            self.safety_checker = StableDiffusionSafetyChecker.from_pretrained(\n                \"CompVis/stable-diffusion-safety-checker\"\n            ).to(pipe.device)\n            self.feature_extractor = CLIPFeatureExtractor.from_pretrained(\n                \"openai/clip-vit-base-patch32\"\n            )\n            self.nsfw_fallback_img = Image.new(\"RGB\", (512, 512), (0, 0, 0))\n\n        return stream\n","file":"/utils/wrapper.py","fileHash":"4fc2c1f8d34f59a019136fa442db9ed0ff27859512221442ea1df929458d10e7","hash":"4fc2c1f8d34f59a019136fa442db9ed0ff27859512221442ea1df929458d10e7","processedContent":"import gc #undefined\nimport os #undefined\nfrom pathlib import Path\n\"\"\"\nscip-python python python-stdlib 3.11 pathlib/__init__:: undefined\nscip-python python python-stdlib 3.11 pathlib/Path#: undefined\n\"\"\"\nimport traceback #undefined\nfrom typing import List, Literal, Optional, Union, Dict\n\"\"\"\nscip-python python python-stdlib 3.11 typing/__init__:: undefined\nscip-python python python-stdlib 3.11 typing/List.: undefined\nscip-python python python-stdlib 3.11 typing/Literal.: undefined\nscip-python python python-stdlib 3.11 typing/Optional.: undefined\nscip-python python python-stdlib 3.11 typing/Union.: undefined\nscip-python python python-stdlib 3.11 typing/Dict.: undefined\n\"\"\"\n\nimport numpy as np #undefined\nimport torch\nfrom diffusers import AutoencoderTiny, StableDiffusionPipeline #undefined\nfrom PIL import Image\n\"\"\"\nscip-python python Pillow 10.0.0 PIL/__init__:: undefined\nscip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\n\"\"\"\n\nfrom streamdiffusion import StreamDiffusion #undefined\nfrom streamdiffusion.image_utils import postprocess_image #undefined\n\n\ntorch.set_grad_enabled(False)\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\n\n\nclass StreamDiffusionWrapper:\n    \"\"\"This code defines a class called `StreamDiffusionWrapper` that loads a model from a given path or ID, performs various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference. It also defines various methods for performing image generation and manipulation.\"\"\"\n    pass\n","documentation":"This code defines a class called `StreamDiffusionWrapper` that loads a model for image generation and manipulation, enabling acceleration, and preparing the model for inference."}},{"key":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#","attributes":{"symbol":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#","language":"python","range":[20,0,656,21],"content":"class StreamDiffusionWrapper:\n    def __init__(\n        self,\n        model_id_or_path: str,\n        t_index_list: List[int],\n        lora_dict: Optional[Dict[str, float]] = None,\n        mode: Literal[\"img2img\", \"txt2img\"] = \"img2img\",\n        output_type: Literal[\"pil\", \"pt\", \"np\", \"latent\"] = \"pil\",\n        lcm_lora_id: Optional[str] = None,\n        vae_id: Optional[str] = None,\n        device: Literal[\"cpu\", \"cuda\"] = \"cuda\",\n        dtype: torch.dtype = torch.float16,\n        frame_buffer_size: int = 1,\n        width: int = 512,\n        height: int = 512,\n        warmup: int = 10,\n        acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"tensorrt\",\n        do_add_noise: bool = True,\n        device_ids: Optional[List[int]] = None,\n        use_lcm_lora: bool = True,\n        use_tiny_vae: bool = True,\n        enable_similar_image_filter: bool = False,\n        similar_image_filter_threshold: float = 0.98,\n        similar_image_filter_max_skip_frame: int = 10,\n        use_denoising_batch: bool = True,\n        cfg_type: Literal[\"none\", \"full\", \"self\", \"initialize\"] = \"self\",\n        seed: int = 2,\n        use_safety_checker: bool = False,\n    ):\n        \"\"\"\n        Initializes the StreamDiffusionWrapper.\n\n        Parameters\n        ----------\n        model_id_or_path : str\n            The model id or path to load.\n        t_index_list : List[int]\n            The t_index_list to use for inference.\n        lora_dict : Optional[Dict[str, float]], optional\n            The lora_dict to load, by default None.\n            Keys are the LoRA names and values are the LoRA scales.\n            Example: {\"LoRA_1\" : 0.5 , \"LoRA_2\" : 0.7 ,...}\n        mode : Literal[\"img2img\", \"txt2img\"], optional\n            txt2img or img2img, by default \"img2img\".\n        output_type : Literal[\"pil\", \"pt\", \"np\", \"latent\"], optional\n            The output type of image, by default \"pil\".\n        lcm_lora_id : Optional[str], optional\n            The lcm_lora_id to load, by default None.\n            If None, the default LCM-LoRA\n            (\"latent-consistency/lcm-lora-sdv1-5\") will be used.\n        vae_id : Optional[str], optional\n            The vae_id to load, by default None.\n            If None, the default TinyVAE\n            (\"madebyollin/taesd\") will be used.\n        device : Literal[\"cpu\", \"cuda\"], optional\n            The device to use for inference, by default \"cuda\".\n        dtype : torch.dtype, optional\n            The dtype for inference, by default torch.float16.\n        frame_buffer_size : int, optional\n            The frame buffer size for denoising batch, by default 1.\n        width : int, optional\n            The width of the image, by default 512.\n        height : int, optional\n            The height of the image, by default 512.\n        warmup : int, optional\n            The number of warmup steps to perform, by default 10.\n        acceleration : Literal[\"none\", \"xformers\", \"tensorrt\"], optional\n            The acceleration method, by default \"tensorrt\".\n        do_add_noise : bool, optional\n            Whether to add noise for following denoising steps or not,\n            by default True.\n        device_ids : Optional[List[int]], optional\n            The device ids to use for DataParallel, by default None.\n        use_lcm_lora : bool, optional\n            Whether to use LCM-LoRA or not, by default True.\n        use_tiny_vae : bool, optional\n            Whether to use TinyVAE or not, by default True.\n        enable_similar_image_filter : bool, optional\n            Whether to enable similar image filter or not,\n            by default False.\n        similar_image_filter_threshold : float, optional\n            The threshold for similar image filter, by default 0.98.\n        similar_image_filter_max_skip_frame : int, optional\n            The max skip frame for similar image filter, by default 10.\n        use_denoising_batch : bool, optional\n            Whether to use denoising batch or not, by default True.\n        cfg_type : Literal[\"none\", \"full\", \"self\", \"initialize\"],\n        optional\n            The cfg_type for img2img mode, by default \"self\".\n            You cannot use anything other than \"none\" for txt2img mode.\n        seed : int, optional\n            The seed, by default 2.\n        use_safety_checker : bool, optional\n            Whether to use safety checker or not, by default False.\n        \"\"\"\n        self.sd_turbo = \"turbo\" in model_id_or_path\n\n        if mode == \"txt2img\":\n            if cfg_type != \"none\":\n                raise ValueError(\n                    f\"txt2img mode accepts only cfg_type = 'none', but got {cfg_type}\"\n                )\n            if use_denoising_batch and frame_buffer_size > 1:\n                if not self.sd_turbo:\n                    raise ValueError(\n                        \"txt2img mode cannot use denoising batch with frame_buffer_size > 1.\"\n                    )\n\n        if mode == \"img2img\":\n            if not use_denoising_batch:\n                raise NotImplementedError(\n                    \"img2img mode must use denoising batch for now.\"\n                )\n\n        self.device = device\n        self.dtype = dtype\n        self.width = width\n        self.height = height\n        self.mode = mode\n        self.output_type = output_type\n        self.frame_buffer_size = frame_buffer_size\n        self.batch_size = (\n            len(t_index_list) * frame_buffer_size\n            if use_denoising_batch\n            else frame_buffer_size\n        )\n\n        self.use_denoising_batch = use_denoising_batch\n        self.use_safety_checker = use_safety_checker\n\n        self.stream: StreamDiffusion = self._load_model(\n            model_id_or_path=model_id_or_path,\n            lora_dict=lora_dict,\n            lcm_lora_id=lcm_lora_id,\n            vae_id=vae_id,\n            t_index_list=t_index_list,\n            acceleration=acceleration,\n            warmup=warmup,\n            do_add_noise=do_add_noise,\n            use_lcm_lora=use_lcm_lora,\n            use_tiny_vae=use_tiny_vae,\n            cfg_type=cfg_type,\n            seed=seed,\n        )\n\n        if device_ids is not None:\n            self.stream.unet = torch.nn.DataParallel(\n                self.stream.unet, device_ids=device_ids\n            )\n\n        if enable_similar_image_filter:\n            self.stream.enable_similar_image_filter(similar_image_filter_threshold, similar_image_filter_max_skip_frame)\n\n    def prepare(\n        self,\n        prompt: str,\n        negative_prompt: str = \"\",\n        num_inference_steps: int = 50,\n        guidance_scale: float = 1.2,\n        delta: float = 1.0,\n    ) -> None:\n        \"\"\"\n        Prepares the model for inference.\n\n        Parameters\n        ----------\n        prompt : str\n            The prompt to generate images from.\n        num_inference_steps : int, optional\n            The number of inference steps to perform, by default 50.\n        guidance_scale : float, optional\n            The guidance scale to use, by default 1.2.\n        delta : float, optional\n            The delta multiplier of virtual residual noise,\n            by default 1.0.\n        \"\"\"\n        self.stream.prepare(\n            prompt,\n            negative_prompt,\n            num_inference_steps=num_inference_steps,\n            guidance_scale=guidance_scale,\n            delta=delta,\n        )\n\n    def __call__(\n        self,\n        image: Optional[Union[str, Image.Image, torch.Tensor]] = None,\n        prompt: Optional[str] = None,\n    ) -> Union[Image.Image, List[Image.Image]]:\n        \"\"\"\n        Performs img2img or txt2img based on the mode.\n\n        Parameters\n        ----------\n        image : Optional[Union[str, Image.Image, torch.Tensor]]\n            The image to generate from.\n        prompt : Optional[str]\n            The prompt to generate images from.\n\n        Returns\n        -------\n        Union[Image.Image, List[Image.Image]]\n            The generated image.\n        \"\"\"\n        if self.mode == \"img2img\":\n            return self.img2img(image)\n        else:\n            return self.txt2img(prompt)\n\n    def txt2img(\n        self, prompt: Optional[str] = None\n    ) -> Union[Image.Image, List[Image.Image], torch.Tensor, np.ndarray]:\n        \"\"\"\n        Performs txt2img.\n\n        Parameters\n        ----------\n        prompt : Optional[str]\n            The prompt to generate images from.\n\n        Returns\n        -------\n        Union[Image.Image, List[Image.Image]]\n            The generated image.\n        \"\"\"\n        if prompt is not None:\n            self.stream.update_prompt(prompt)\n\n        if self.sd_turbo:\n            image_tensor = self.stream.txt2img_sd_turbo(self.batch_size)\n        else:\n            image_tensor = self.stream.txt2img(self.frame_buffer_size)\n        image = self.postprocess_image(image_tensor, output_type=self.output_type)\n\n        if self.use_safety_checker:\n            safety_checker_input = self.feature_extractor(\n                image, return_tensors=\"pt\"\n            ).to(self.device)\n            _, has_nsfw_concept = self.safety_checker(\n                images=image_tensor.to(self.dtype),\n                clip_input=safety_checker_input.pixel_values.to(self.dtype),\n            )\n            image = self.nsfw_fallback_img if has_nsfw_concept[0] else image\n\n        return image\n\n    def img2img(\n        self, image: Union[str, Image.Image, torch.Tensor]\n    ) -> Union[Image.Image, List[Image.Image], torch.Tensor, np.ndarray]:\n        \"\"\"\n        Performs img2img.\n\n        Parameters\n        ----------\n        image : Union[str, Image.Image, torch.Tensor]\n            The image to generate from.\n\n        Returns\n        -------\n        Image.Image\n            The generated image.\n        \"\"\"\n        if isinstance(image, str) or isinstance(image, Image.Image):\n            image = self.preprocess_image(image)\n\n        image_tensor = self.stream(image)\n        image = self.postprocess_image(image_tensor, output_type=self.output_type)\n\n        if self.use_safety_checker:\n            safety_checker_input = self.feature_extractor(\n                image, return_tensors=\"pt\"\n            ).to(self.device)\n            _, has_nsfw_concept = self.safety_checker(\n                images=image_tensor.to(self.dtype),\n                clip_input=safety_checker_input.pixel_values.to(self.dtype),\n            )\n            image = self.nsfw_fallback_img if has_nsfw_concept[0] else image\n\n        return image\n\n    def preprocess_image(self, image: Union[str, Image.Image]) -> torch.Tensor:\n        \"\"\"\n        Preprocesses the image.\n\n        Parameters\n        ----------\n        image : Union[str, Image.Image, torch.Tensor]\n            The image to preprocess.\n\n        Returns\n        -------\n        torch.Tensor\n            The preprocessed image.\n        \"\"\"\n        if isinstance(image, str):\n            image = Image.open(image).convert(\"RGB\").resize((self.width, self.height))\n        if isinstance(image, Image.Image):\n            image = image.convert(\"RGB\").resize((self.width, self.height))\n\n        return self.stream.image_processor.preprocess(\n            image, self.height, self.width\n        ).to(device=self.device, dtype=self.dtype)\n\n    def postprocess_image(\n        self, image_tensor: torch.Tensor, output_type: str = \"pil\"\n    ) -> Union[Image.Image, List[Image.Image], torch.Tensor, np.ndarray]:\n        \"\"\"\n        Postprocesses the image.\n\n        Parameters\n        ----------\n        image_tensor : torch.Tensor\n            The image tensor to postprocess.\n\n        Returns\n        -------\n        Union[Image.Image, List[Image.Image]]\n            The postprocessed image.\n        \"\"\"\n        if self.frame_buffer_size > 1:\n            return postprocess_image(image_tensor.cpu(), output_type=output_type)\n        else:\n            return postprocess_image(image_tensor.cpu(), output_type=output_type)[0]\n\n    def _load_model(\n        self,\n        model_id_or_path: str,\n        t_index_list: List[int],\n        lora_dict: Optional[Dict[str, float]] = None,\n        lcm_lora_id: Optional[str] = None,\n        vae_id: Optional[str] = None,\n        acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"tensorrt\",\n        warmup: int = 10,\n        do_add_noise: bool = True,\n        use_lcm_lora: bool = True,\n        use_tiny_vae: bool = True,\n        cfg_type: Literal[\"none\", \"full\", \"self\", \"initialize\"] = \"self\",\n        seed: int = 2,\n    ) -> StreamDiffusion:\n        \"\"\"\n        Loads the model.\n\n        This method does the following:\n\n        1. Loads the model from the model_id_or_path.\n        2. Loads and fuses the LCM-LoRA model from the lcm_lora_id if needed.\n        3. Loads the VAE model from the vae_id if needed.\n        4. Enables acceleration if needed.\n        5. Prepares the model for inference.\n        6. Load the safety checker if needed.\n\n        Parameters\n        ----------\n        model_id_or_path : str\n            The model id or path to load.\n        t_index_list : List[int]\n            The t_index_list to use for inference.\n        lora_dict : Optional[Dict[str, float]], optional\n            The lora_dict to load, by default None.\n            Keys are the LoRA names and values are the LoRA scales.\n            Example: {\"LoRA_1\" : 0.5 , \"LoRA_2\" : 0.7 ,...}\n        lcm_lora_id : Optional[str], optional\n            The lcm_lora_id to load, by default None.\n        vae_id : Optional[str], optional\n            The vae_id to load, by default None.\n        acceleration : Literal[\"none\", \"xfomers\", \"sfast\", \"tensorrt\"], optional\n            The acceleration method, by default \"tensorrt\".\n        warmup : int, optional\n            The number of warmup steps to perform, by default 10.\n        do_add_noise : bool, optional\n            Whether to add noise for following denoising steps or not,\n            by default True.\n        use_lcm_lora : bool, optional\n            Whether to use LCM-LoRA or not, by default True.\n        use_tiny_vae : bool, optional\n            Whether to use TinyVAE or not, by default True.\n        cfg_type : Literal[\"none\", \"full\", \"self\", \"initialize\"],\n        optional\n            The cfg_type for img2img mode, by default \"self\".\n            You cannot use anything other than \"none\" for txt2img mode.\n        seed : int, optional\n            The seed, by default 2.\n\n        Returns\n        -------\n        StreamDiffusion\n            The loaded model.\n        \"\"\"\n\n        try:  # Load from local directory\n            pipe: StableDiffusionPipeline = StableDiffusionPipeline.from_pretrained(\n                model_id_or_path,\n            ).to(device=self.device, dtype=self.dtype)\n\n        except ValueError:  # Load from huggingface\n            pipe: StableDiffusionPipeline = StableDiffusionPipeline.from_single_file(\n                model_id_or_path,\n            ).to(device=self.device, dtype=self.dtype)\n        except Exception:  # No model found\n            traceback.print_exc()\n            print(\"Model load has failed. Doesn't exist.\")\n            exit()\n\n        stream = StreamDiffusion(\n            pipe=pipe,\n            t_index_list=t_index_list,\n            torch_dtype=self.dtype,\n            width=self.width,\n            height=self.height,\n            do_add_noise=do_add_noise,\n            frame_buffer_size=self.frame_buffer_size,\n            use_denoising_batch=self.use_denoising_batch,\n            cfg_type=cfg_type,\n        )\n        if not self.sd_turbo:\n            if use_lcm_lora:\n                if lcm_lora_id is not None:\n                    stream.load_lcm_lora(\n                        pretrained_model_name_or_path_or_dict=lcm_lora_id\n                    )\n                else:\n                    stream.load_lcm_lora()\n                stream.fuse_lora()\n\n            if lora_dict is not None:\n                for lora_name, lora_scale in lora_dict.items():\n                    stream.load_lora(lora_name)\n                    stream.fuse_lora(lora_scale=lora_scale)\n                    print(f\"Use LoRA: {lora_name} in weights {lora_scale}\")\n\n        if use_tiny_vae:\n            if vae_id is not None:\n                stream.vae = AutoencoderTiny.from_pretrained(vae_id).to(\n                    device=pipe.device, dtype=pipe.dtype\n                )\n            else:\n                stream.vae = AutoencoderTiny.from_pretrained(\"madebyollin/taesd\").to(\n                    device=pipe.device, dtype=pipe.dtype\n                )\n\n        try:\n            if acceleration == \"xformers\":\n                stream.pipe.enable_xformers_memory_efficient_attention()\n            if acceleration == \"tensorrt\":\n                from polygraphy import cuda\n                from streamdiffusion.acceleration.tensorrt import (\n                    TorchVAEEncoder,\n                    compile_unet,\n                    compile_vae_decoder,\n                    compile_vae_encoder,\n                )\n                from streamdiffusion.acceleration.tensorrt.engine import (\n                    AutoencoderKLEngine,\n                    UNet2DConditionModelEngine,\n                )\n                from streamdiffusion.acceleration.tensorrt.models import (\n                    VAE,\n                    UNet,\n                    VAEEncoder,\n                )\n\n                def create_prefix(\n                    model_id_or_path: str,\n                    max_batch_size: int,\n                    min_batch_size: int,\n                ):\n                    maybe_path = Path(model_id_or_path)\n                    if maybe_path.exists():\n                        return f\"{maybe_path.stem}--lcm_lora-{use_lcm_lora}--tiny_vae-{use_tiny_vae}--max_batch-{max_batch_size}--min_batch-{min_batch_size}--mode-{self.mode}\"\n                    else:\n                        return f\"{model_id_or_path}--lcm_lora-{use_lcm_lora}--tiny_vae-{use_tiny_vae}--max_batch-{max_batch_size}--min_batch-{min_batch_size}--mode-{self.mode}\"\n\n                engine_dir = os.path.join(\"engines\")\n                unet_path = os.path.join(\n                    engine_dir,\n                    create_prefix(\n                        model_id_or_path=model_id_or_path,\n                        max_batch_size=stream.trt_unet_batch_size,\n                        min_batch_size=stream.trt_unet_batch_size,\n                    ),\n                    \"unet.engine\",\n                )\n                vae_encoder_path = os.path.join(\n                    engine_dir,\n                    create_prefix(\n                        model_id_or_path=model_id_or_path,\n                        max_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                        min_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                    ),\n                    \"vae_encoder.engine\",\n                )\n                vae_decoder_path = os.path.join(\n                    engine_dir,\n                    create_prefix(\n                        model_id_or_path=model_id_or_path,\n                        max_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                        min_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                    ),\n                    \"vae_decoder.engine\",\n                )\n\n                if not os.path.exists(unet_path):\n                    os.makedirs(os.path.dirname(unet_path), exist_ok=True)\n                    unet_model = UNet(\n                        fp16=True,\n                        device=stream.device,\n                        max_batch_size=stream.trt_unet_batch_size,\n                        min_batch_size=stream.trt_unet_batch_size,\n                        embedding_dim=stream.text_encoder.config.hidden_size,\n                        unet_dim=stream.unet.config.in_channels,\n                    )\n                    compile_unet(\n                        stream.unet,\n                        unet_model,\n                        unet_path + \".onnx\",\n                        unet_path + \".opt.onnx\",\n                        unet_path,\n                        opt_batch_size=stream.trt_unet_batch_size,\n                    )\n\n                if not os.path.exists(vae_decoder_path):\n                    os.makedirs(os.path.dirname(vae_decoder_path), exist_ok=True)\n                    stream.vae.forward = stream.vae.decode\n                    vae_decoder_model = VAE(\n                        device=stream.device,\n                        max_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                        min_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                    )\n                    compile_vae_decoder(\n                        stream.vae,\n                        vae_decoder_model,\n                        vae_decoder_path + \".onnx\",\n                        vae_decoder_path + \".opt.onnx\",\n                        vae_decoder_path,\n                        opt_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                    )\n                    delattr(stream.vae, \"forward\")\n\n                if not os.path.exists(vae_encoder_path):\n                    os.makedirs(os.path.dirname(vae_encoder_path), exist_ok=True)\n                    vae_encoder = TorchVAEEncoder(stream.vae).to(torch.device(\"cuda\"))\n                    vae_encoder_model = VAEEncoder(\n                        device=stream.device,\n                        max_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                        min_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                    )\n                    compile_vae_encoder(\n                        vae_encoder,\n                        vae_encoder_model,\n                        vae_encoder_path + \".onnx\",\n                        vae_encoder_path + \".opt.onnx\",\n                        vae_encoder_path,\n                        opt_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                    )\n\n                cuda_steram = cuda.Stream()\n\n                vae_config = stream.vae.config\n                vae_dtype = stream.vae.dtype\n\n                stream.unet = UNet2DConditionModelEngine(\n                    unet_path, cuda_steram, use_cuda_graph=False\n                )\n                stream.vae = AutoencoderKLEngine(\n                    vae_encoder_path,\n                    vae_decoder_path,\n                    cuda_steram,\n                    stream.pipe.vae_scale_factor,\n                    use_cuda_graph=False,\n                )\n                setattr(stream.vae, \"config\", vae_config)\n                setattr(stream.vae, \"dtype\", vae_dtype)\n\n                gc.collect()\n                torch.cuda.empty_cache()\n\n                print(\"TensorRT acceleration enabled.\")\n            if acceleration == \"sfast\":\n                from streamdiffusion.acceleration.sfast import (\n                    accelerate_with_stable_fast,\n                )\n\n                stream = accelerate_with_stable_fast(stream)\n                print(\"StableFast acceleration enabled.\")\n        except Exception:\n            traceback.print_exc()\n            print(\"Acceleration has failed. Falling back to normal mode.\")\n\n        if seed < 0: # Random seed\n            seed = np.random.randint(0, 1000000)\n\n        stream.prepare(\n            \"\",\n            \"\",\n            num_inference_steps=50,\n            guidance_scale=1.1\n            if stream.cfg_type in [\"full\", \"self\", \"initialize\"]\n            else 1.0,\n            generator=torch.manual_seed(seed),\n            seed=seed,\n        )\n\n        if self.use_safety_checker:\n            from transformers import CLIPFeatureExtractor\n            from diffusers.pipelines.stable_diffusion.safety_checker import (\n                StableDiffusionSafetyChecker,\n            )\n\n            self.safety_checker = StableDiffusionSafetyChecker.from_pretrained(\n                \"CompVis/stable-diffusion-safety-checker\"\n            ).to(pipe.device)\n            self.feature_extractor = CLIPFeatureExtractor.from_pretrained(\n                \"openai/clip-vit-base-patch32\"\n            )\n            self.nsfw_fallback_img = Image.new(\"RGB\", (512, 512), (0, 0, 0))\n\n        return stream","file":"/utils/wrapper.py","fileHash":"4fc2c1f8d34f59a019136fa442db9ed0ff27859512221442ea1df929458d10e7","hash":"537ff0b2ab4e7e48d3881fe47255ba9ab09f1229bfce45654cd0e94862435aaf","processedContent":"class StreamDiffusionWrapper:\n    def __init__(\n        \"\"\"This code defines a class called `StreamDiffusionWrapper` that loads a model from a given path or ID, performs various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference. It also defines various methods for performing image generation and manipulation.\"\"\"\n        pass\n\n    def prepare(\n        \"\"\"This code prepares the model for inference by setting up the prompt, number of steps, and guidance scale.\"\"\"\n        pass\n\n    def __call__(\n        \"\"\"This code defines a function called `__call__` that takes an image or prompt as input and returns a postprocessed image. Depending on the mode, it either calls the `img2img` function or the `txt2img` function to generate the output.\"\"\"\n        pass\n\n    def txt2img(\n        \"\"\"The code defines a function called `txt2img` that takes a prompt as input and generates an image based on the prompt. It uses a StreamDiffusion model to generate the image, and then applies post-processing to the output.\"\"\"\n        pass\n\n    def img2img(\n        \"\"\"This code defines a function called `img2img` that takes an image as input and returns a postprocessed image. The function preprocesses the image, passes it through a neural network for further processing, and then returns the resulting image.\"\"\"\n        pass\n\n    def preprocess_image(self, image: Union[str, Image.Image]) -> torch.Tensor:\n        \"\"\"This code preprocesses an image by converting it to RGB and resizing it to a specific width and height. It then passes the preprocessed image through a neural network for further processing.\"\"\"\n        pass\n\n    def postprocess_image(\n        \"\"\"This code defines a function called `postprocess_image` that takes an image tensor and returns a postprocessed image. The function checks if the frame buffer size is greater than 1, and if so, it calls itself recursively with the same input. Otherwise, it returns the first element of the output of the recursive call.\"\"\"\n        pass\n\n    def _load_model(\n        self,\n        model_id_or_path: str,\n        t_index_list: List[int],\n        lora_dict: Optional[Dict[str, float]] = None,\n        lcm_lora_id: Optional[str] = None,\n        vae_id: Optional[str] = None,\n        acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"tensorrt\",\n        warmup: int = 10,\n        do_add_noise: bool = True,\n        use_lcm_lora: bool = True,\n        use_tiny_vae: bool = True,\n        cfg_type: Literal[\"none\", \"full\", \"self\", \"initialize\"] = \"self\",\n        seed: int = 2,\n    ) -> StreamDiffusion:\n        \"\"\"\n        Loads the model.\n\n        This method does the following:\n\n        1. Loads the model from the model_id_or_path.\n        2. Loads and fuses the LCM-LoRA model from the lcm_lora_id if needed.\n        3. Loads the VAE model from the vae_id if needed.\n        4. Enables acceleration if needed.\n        5. Prepares the model for inference.\n        6. Load the safety checker if needed.\n\n        Parameters\n        ----------\n        model_id_or_path : str\n            The model id or path to load.\n        t_index_list : List[int]\n            The t_index_list to use for inference.\n        lora_dict : Optional[Dict[str, float]], optional\n            The lora_dict to load, by default None.\n            Keys are the LoRA names and values are the LoRA scales.\n            Example: {\"LoRA_1\" : 0.5 , \"LoRA_2\" : 0.7 ,...}\n        lcm_lora_id : Optional[str], optional\n            The lcm_lora_id to load, by default None.\n        vae_id : Optional[str], optional\n            The vae_id to load, by default None.\n        acceleration : Literal[\"none\", \"xfomers\", \"sfast\", \"tensorrt\"], optional\n            The acceleration method, by default \"tensorrt\".\n        warmup : int, optional\n            The number of warmup steps to perform, by default 10.\n        do_add_noise : bool, optional\n            Whether to add noise for following denoising steps or not,\n            by default True.\n        use_lcm_lora : bool, optional\n            Whether to use LCM-LoRA or not, by default True.\n        use_tiny_vae : bool, optional\n            Whether to use TinyVAE or not, by default True.\n        cfg_type : Literal[\"none\", \"full\", \"self\", \"initialize\"],\n        optional\n            The cfg_type for img2img mode, by default \"self\".\n            You cannot use anything other than \"none\" for txt2img mode.\n        seed : int, optional\n            The seed, by default 2.\n\n        Returns\n        -------\n        StreamDiffusion\n            The loaded model.\n        \"\"\"\n\n        try:  # Load from local directory\n            pipe: StableDiffusionPipeline = StableDiffusionPipeline.from_pretrained(\n                model_id_or_path,\n            ).to(device=self.device, dtype=self.dtype)\n\n        except ValueError:  # Load from huggingface\n            pipe: StableDiffusionPipeline = StableDiffusionPipeline.from_single_file(\n                model_id_or_path,\n            ).to(device=self.device, dtype=self.dtype)\n        except Exception:  # No model found\n            traceback.print_exc()\n            print(\"Model load has failed. Doesn't exist.\")\n            exit()\n\n        stream = StreamDiffusion(\n            pipe=pipe,\n            t_index_list=t_index_list,\n            torch_dtype=self.dtype,\n            width=self.width,\n            height=self.height,\n            do_add_noise=do_add_noise,\n            frame_buffer_size=self.frame_buffer_size,\n            use_denoising_batch=self.use_denoising_batch,\n            cfg_type=cfg_type,\n        )\n        if not self.sd_turbo:\n            if use_lcm_lora:\n                if lcm_lora_id is not None:\n                    stream.load_lcm_lora(\n                        pretrained_model_name_or_path_or_dict=lcm_lora_id\n                    )\n                else:\n                    stream.load_lcm_lora()\n                stream.fuse_lora()\n\n            if lora_dict is not None:\n                for lora_name, lora_scale in lora_dict.items():\n                    stream.load_lora(lora_name)\n                    stream.fuse_lora(lora_scale=lora_scale)\n                    print(f\"Use LoRA: {lora_name} in weights {lora_scale}\")\n\n        if use_tiny_vae:\n            if vae_id is not None:\n                stream.vae = AutoencoderTiny.from_pretrained(vae_id).to(\n                    device=pipe.device, dtype=pipe.dtype\n                )\n            else:\n                stream.vae = AutoencoderTiny.from_pretrained(\"madebyollin/taesd\").to(\n                    device=pipe.device, dtype=pipe.dtype\n                )\n\n        try:\n            if acceleration == \"xformers\":\n                stream.pipe.enable_xformers_memory_efficient_attention()\n            if acceleration == \"tensorrt\":\n                from polygraphy import cuda\n                from streamdiffusion.acceleration.tensorrt import (\n                    TorchVAEEncoder,\n                    compile_unet,\n                    compile_vae_decoder,\n                    compile_vae_encoder,\n                )\n                from streamdiffusion.acceleration.tensorrt.engine import (\n                    AutoencoderKLEngine,\n                    UNet2DConditionModelEngine,\n                )\n                from streamdiffusion.acceleration.tensorrt.models import (\n                    VAE,\n                    UNet,\n                    VAEEncoder,\n                )\n\n                def create_prefix(\n                    model_id_or_path: str,\n                    max_batch_size: int,\n                    min_batch_size: int,\n                ):\n                    maybe_path = Path(model_id_or_path)\n                    if maybe_path.exists():\n                        return f\"{maybe_path.stem}--lcm_lora-{use_lcm_lora}--tiny_vae-{use_tiny_vae}--max_batch-{max_batch_size}--min_batch-{min_batch_size}--mode-{self.mode}\"\n                    else:\n                        return f\"{model_id_or_path}--lcm_lora-{use_lcm_lora}--tiny_vae-{use_tiny_vae}--max_batch-{max_batch_size}--min_batch-{min_batch_size}--mode-{self.mode}\"\n\n                engine_dir = os.path.join(\"engines\")\n                unet_path = os.path.join(\n                    engine_dir,\n                    create_prefix(\n                        model_id_or_path=model_id_or_path,\n                        max_batch_size=stream.trt_unet_batch_size,\n                        min_batch_size=stream.trt_unet_batch_size,\n                    ),\n                    \"unet.engine\",\n                )\n                vae_encoder_path = os.path.join(\n                    engine_dir,\n                    create_prefix(\n                        model_id_or_path=model_id_or_path,\n                        max_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                        min_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                    ),\n                    \"vae_encoder.engine\",\n                )\n                vae_decoder_path = os.path.join(\n                    engine_dir,\n                    create_prefix(\n                        model_id_or_path=model_id_or_path,\n                        max_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                        min_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                    ),\n                    \"vae_decoder.engine\",\n                )\n\n                if not os.path.exists(unet_path):\n                    os.makedirs(os.path.dirname(unet_path), exist_ok=True)\n                    unet_model = UNet(\n                        fp16=True,\n                        device=stream.device,\n                        max_batch_size=stream.trt_unet_batch_size,\n                        min_batch_size=stream.trt_unet_batch_size,\n                        embedding_dim=stream.text_encoder.config.hidden_size,\n                        unet_dim=stream.unet.config.in_channels,\n                    )\n                    compile_unet(\n                        stream.unet,\n                        unet_model,\n                        unet_path + \".onnx\",\n                        unet_path + \".opt.onnx\",\n                        unet_path,\n                        opt_batch_size=stream.trt_unet_batch_size,\n                    )\n\n                if not os.path.exists(vae_decoder_path):\n                    os.makedirs(os.path.dirname(vae_decoder_path), exist_ok=True)\n                    stream.vae.forward = stream.vae.decode\n                    vae_decoder_model = VAE(\n                        device=stream.device,\n                        max_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                        min_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                    )\n                    compile_vae_decoder(\n                        stream.vae,\n                        vae_decoder_model,\n                        vae_decoder_path + \".onnx\",\n                        vae_decoder_path + \".opt.onnx\",\n                        vae_decoder_path,\n                        opt_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                    )\n                    delattr(stream.vae, \"forward\")\n\n                if not os.path.exists(vae_encoder_path):\n                    os.makedirs(os.path.dirname(vae_encoder_path), exist_ok=True)\n                    vae_encoder = TorchVAEEncoder(stream.vae).to(torch.device(\"cuda\"))\n                    vae_encoder_model = VAEEncoder(\n                        device=stream.device,\n                        max_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                        min_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                    )\n                    compile_vae_encoder(\n                        vae_encoder,\n                        vae_encoder_model,\n                        vae_encoder_path + \".onnx\",\n                        vae_encoder_path + \".opt.onnx\",\n                        vae_encoder_path,\n                        opt_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                    )\n\n                cuda_steram = cuda.Stream()\n\n                vae_config = stream.vae.config\n                vae_dtype = stream.vae.dtype\n\n                stream.unet = UNet2DConditionModelEngine(\n                    unet_path, cuda_steram, use_cuda_graph=False\n                )\n                stream.vae = AutoencoderKLEngine(\n                    vae_encoder_path,\n                    vae_decoder_path,\n                    cuda_steram,\n                    stream.pipe.vae_scale_factor,\n                    use_cuda_graph=False,\n                )\n                setattr(stream.vae, \"config\", vae_config)\n                setattr(stream.vae, \"dtype\", vae_dtype)\n\n                gc.collect()\n                torch.cuda.empty_cache()\n\n                print(\"TensorRT acceleration enabled.\")\n            if acceleration == \"sfast\":\n                from streamdiffusion.acceleration.sfast import (\n                    accelerate_with_stable_fast,\n                )\n\n                stream = accelerate_with_stable_fast(stream)\n                print(\"StableFast acceleration enabled.\")\n        except Exception:\n            traceback.print_exc()\n            print(\"Acceleration has failed. Falling back to normal mode.\")\n\n        if seed < 0: # Random seed\n            seed = np.random.randint(0, 1000000)\n\n        stream.prepare(\n            \"\",\n            \"\",\n            num_inference_steps=50,\n            guidance_scale=1.1\n            if stream.cfg_type in [\"full\", \"self\", \"initialize\"]\n            else 1.0,\n            generator=torch.manual_seed(seed),\n            seed=seed,\n        )\n\n        if self.use_safety_checker:\n            from transformers import CLIPFeatureExtractor\n            from diffusers.pipelines.stable_diffusion.safety_checker import (\n                StableDiffusionSafetyChecker,\n            )\n\n            self.safety_checker = StableDiffusionSafetyChecker.from_pretrained(\n                \"CompVis/stable-diffusion-safety-checker\"\n            ).to(pipe.device)\n            self.feature_extractor = CLIPFeatureExtractor.from_pretrained(\n                \"openai/clip-vit-base-patch32\"\n            )\n            self.nsfw_fallback_img = Image.new(\"RGB\", (512, 512), (0, 0, 0))\n\n        return stream","documentation":"This code defines a class called `StreamDiffusionWrapper` that loads a model from a given path or ID, performs various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference. It also defines various methods for performing image generation and manipulation."}},{"key":"scip-python python python-stdlib 3.11 logging/getLogger().","attributes":{"symbol":"scip-python python python-stdlib 3.11 logging/getLogger().","language":"python"}},{"key":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/","attributes":{"symbol":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/","language":"python"}},{"key":"scip-python python python-stdlib 3.11 pathlib/PurePath#parent().","attributes":{"symbol":"scip-python python python-stdlib 3.11 pathlib/PurePath#parent().","language":"python"}},{"key":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#mode.","attributes":{"symbol":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#mode.","language":"python"}},{"key":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#model_id_or_path.","attributes":{"symbol":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#model_id_or_path.","language":"python"}},{"key":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#lcm_lora_id.","attributes":{"symbol":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#lcm_lora_id.","language":"python"}},{"key":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#vae_id.","attributes":{"symbol":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#vae_id.","language":"python"}},{"key":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#device.","attributes":{"symbol":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#device.","language":"python"}},{"key":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#dtype.","attributes":{"symbol":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#dtype.","language":"python"}},{"key":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#acceleration.","attributes":{"symbol":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#acceleration.","language":"python"}},{"key":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#t_index_list.","attributes":{"symbol":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#t_index_list.","language":"python"}},{"key":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#warmup.","attributes":{"symbol":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#warmup.","language":"python"}},{"key":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#use_safety_checker.","attributes":{"symbol":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#use_safety_checker.","language":"python"}},{"key":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#app.","attributes":{"symbol":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#app.","language":"python"}},{"key":"scip-python python temp indexer `asyncio.locks`/Lock#","attributes":{"symbol":"scip-python python temp indexer `asyncio.locks`/Lock#","language":"python"}},{"key":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_predict_lock.","attributes":{"symbol":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_predict_lock.","language":"python"}},{"key":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#stream_diffusion.","attributes":{"symbol":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#stream_diffusion.","language":"python"}},{"key":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/PredictInputModel#prompt.","attributes":{"symbol":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/PredictInputModel#prompt.","language":"python"}},{"key":"scip-python python Pillow 10.0.0 `pil.image`/Image#","attributes":{"symbol":"scip-python python Pillow 10.0.0 `pil.image`/Image#","language":"python"}},{"key":"scip-python python Pillow 10.0.0 `pil.image`/Image#convert().","attributes":{"symbol":"scip-python python Pillow 10.0.0 `pil.image`/Image#convert().","language":"python"}},{"key":"scip-python python Pillow 10.0.0 `pil.image`/Image#save().","attributes":{"symbol":"scip-python python Pillow 10.0.0 `pil.image`/Image#save().","language":"python"}},{"key":"scip-python python python-stdlib 3.11 base64/b64encode().","attributes":{"symbol":"scip-python python python-stdlib 3.11 base64/b64encode().","language":"python"}},{"key":"scip-python python python-stdlib 3.11 io/BytesIO#getvalue().","attributes":{"symbol":"scip-python python python-stdlib 3.11 io/BytesIO#getvalue().","language":"python"}},{"key":"scip-python python Pillow 10.0.0 `pil.image`/open().","attributes":{"symbol":"scip-python python Pillow 10.0.0 `pil.image`/open().","language":"python"}},{"key":"scip-python python python-stdlib 3.11 base64/b64decode().","attributes":{"symbol":"scip-python python python-stdlib 3.11 base64/b64decode().","language":"python"}},{"key":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/config.","attributes":{"symbol":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/config.","language":"python"}},{"key":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#host.","attributes":{"symbol":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#host.","language":"python"}},{"key":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#port.","attributes":{"symbol":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#port.","language":"python"}},{"key":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#workers.","attributes":{"symbol":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#workers.","language":"python"}},{"key":"scip-python python temp indexer `examples.benchmark.multi`/_postprocess_image().","attributes":{"range":[20,0,27,18],"symbol":"scip-python python temp indexer `examples.benchmark.multi`/_postprocess_image().","content":"def _postprocess_image(queue: Queue) -> None:\n    while True:\n        try:\n            if not queue.empty():\n                output = postprocess_image(queue.get(block=False), output_type=\"pil\")[0]\n            time.sleep(0.0005)\n        except KeyboardInterrupt:\n            return","file":"/examples/benchmark/multi.py","language":"python","fileHash":"db4b8c400f4c5751cae4a3ea99b759d7985abe42e1983a39013480e390ea485e","hash":"0ab39b987b3fbbc8c0bb1b77c619b159ecbed63376fc00a63b8f2ab8c1665c6d","processedContent":"def _postprocess_image(queue: Queue) -> None:\n    while True:\n        try:\n            if not queue.empty(): #undefined\n                output = postprocess_image(queue.get(block=False), output_type=\"pil\")[0]\n            time.sleep(0.0005)\n            \"\"\"\n            scip-python python python-stdlib 3.11 time/__init__:: undefined\n            scip-python python python-stdlib 3.11 time/sleep().: undefined\n            \"\"\"\n        except KeyboardInterrupt:\n            return","documentation":"This code is a function that continuously checks if there are any images in a queue and processes them using the `postprocess_image` function. It uses the `time.sleep` function to wait for 0.0005 seconds before checking again."}},{"key":"scip-python python temp indexer `examples.benchmark.multi`/download_image().","attributes":{"range":[30,0,33,16],"symbol":"scip-python python temp indexer `examples.benchmark.multi`/download_image().","content":"def download_image(url: str):\n    response = requests.get(url)\n    image = PIL.Image.open(io.BytesIO(response.content))\n    return image","file":"/examples/benchmark/multi.py","language":"python","fileHash":"db4b8c400f4c5751cae4a3ea99b759d7985abe42e1983a39013480e390ea485e","hash":"20a94ac9c375a37973ea7e1afc65cc4fdd9dd4b3d0b6d484680f6c064019c01a","processedContent":"def download_image(url: str):\n    response = requests.get(url)\n    \"\"\"\n    scip-python python requests 2.31.0 requests/__init__:: undefined\n    scip-python python requests 2.31.0 `requests.api`/get().: undefined\n    \"\"\"\n    image = PIL.Image.open(io.BytesIO(response.content))\n    \"\"\"\n    scip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\n    scip-python python Pillow 10.0.0 `pil.image`/open().: undefined\n    scip-python python python-stdlib 3.11 io/__init__:: undefined\n    scip-python python python-stdlib 3.11 io/BytesIO#: undefined\n    scip-python python requests 2.31.0 `requests.models`/Response#content().: undefined\n    \"\"\"\n    return image","documentation":"This code defines a function called `download_image` that takes a URL as an argument and returns a PIL.Image object representing the image at that URL. It uses the requests library to make an HTTP GET request to the URL, then uses the Pillow library to open the response content as an image file."}},{"key":"scip-python python temp indexer `examples.benchmark.multi`/run().","attributes":{"range":[36,0,141,65],"symbol":"scip-python python temp indexer `examples.benchmark.multi`/run().","content":"def run(\n    iterations: int = 100,\n    model_id_or_path: str = \"KBlueLeaf/kohaku-v2.1\",\n    lora_dict: Optional[Dict[str, float]] = None,\n    prompt: str = \"1girl with brown dog hair, thick glasses, smiling\",\n    negative_prompt: str = \"bad image , bad quality\",\n    use_lcm_lora: bool = True,\n    use_tiny_vae: bool = True,\n    width: int = 512,\n    height: int = 512,\n    warmup: int = 10,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"xformers\",\n    device_ids: Optional[List[int]] = None,\n    use_denoising_batch: bool = True,\n    seed: int = 2,\n):\n    \"\"\"\n    Initializes the StreamDiffusionWrapper.\n\n    Parameters\n    ----------\n    iterations : int, optional\n        The number of iterations to run, by default 100.\n    model_id_or_path : str\n        The model id or path to load.\n    lora_dict : Optional[Dict[str, float]], optional\n        The lora_dict to load, by default None.\n        Keys are the LoRA names and values are the LoRA scales.\n        Example: {\"LoRA_1\" : 0.5 , \"LoRA_2\" : 0.7 ,...}\n    prompt : str, optional\n        The prompt to use, by default \"1girl with brown dog hair, thick glasses, smiling\".\n    negative_prompt : str, optional\n        The negative prompt to use, by default \"bad image , bad quality\".\n    use_lcm_lora : bool, optional\n        Whether to use LCM-LoRA or not, by default True.\n    use_tiny_vae : bool, optional\n        Whether to use TinyVAE or not, by default True.\n    width : int, optional\n        The width of the image, by default 512.\n    height : int, optional\n        The height of the image, by default 512.\n    warmup : int, optional\n        The number of warmup steps to perform, by default 10.\n    acceleration : Literal[\"none\", \"xformers\", \"tensorrt\"], optional\n        The acceleration method, by default \"tensorrt\".\n    device_ids : Optional[List[int]], optional\n        The device ids to use for DataParallel, by default None.\n    use_denoising_batch : bool, optional\n        Whether to use denoising batch or not, by default True.\n    seed : int, optional\n        The seed, by default 2. if -1, use random seed.\n    \"\"\"        \n    stream = StreamDiffusionWrapper(\n        model_id_or_path=model_id_or_path,\n        t_index_list=[32, 45],\n        lora_dict=lora_dict,\n        mode=\"img2img\",\n        frame_buffer_size=1,\n        width=width,\n        height=height,\n        warmup=warmup,\n        acceleration=acceleration,\n        device_ids=device_ids,\n        use_lcm_lora=use_lcm_lora,\n        use_tiny_vae=use_tiny_vae,\n        enable_similar_image_filter=False,\n        similar_image_filter_threshold=0.98,\n        use_denoising_batch=use_denoising_batch,\n        cfg_type=\"self\",  # initialize, full, self , none\n        seed=seed,\n    )\n\n    stream.prepare(\n        prompt=prompt,\n        negative_prompt=negative_prompt,\n        num_inference_steps=50,\n        guidance_scale=1.2,\n        delta=0.5,\n    )\n\n    image = download_image(\"https://github.com/ddpn08.png\").resize((width, height))\n    image_tensor = stream.preprocess_image(image)\n\n    # warmup\n    for _ in range(warmup):\n        stream.stream(image_tensor)\n\n    queue = Queue()\n    p = Process(target=_postprocess_image, args=(queue,))\n    p.start()\n\n    results = []\n\n    start = torch.cuda.Event(enable_timing=True)\n    end = torch.cuda.Event(enable_timing=True)\n    for _ in tqdm(range(iterations)):\n        start.record()\n        out_tensor = stream.stream(image_tensor).cpu()\n        queue.put(out_tensor)\n        end.record()\n\n        torch.cuda.synchronize()\n        results.append(start.elapsed_time(end))\n\n    print(f\"Average time: {sum(results) / len(results)}ms\")\n    print(f\"Average FPS: {1000 / (sum(results) / len(results))}\")","file":"/examples/benchmark/multi.py","language":"python","fileHash":"db4b8c400f4c5751cae4a3ea99b759d7985abe42e1983a39013480e390ea485e","hash":"4e01f638b2e4b6d90f49ff3472dca75d2638c0eb1e5d8682d4972139762f274a","processedContent":"def run(\n    iterations: int = 100,\n    model_id_or_path: str = \"KBlueLeaf/kohaku-v2.1\",\n    lora_dict: Optional[Dict[str, float]] = None,\n    \"\"\"\n    scip-python python python-stdlib 3.11 typing/Optional.: undefined\n    scip-python python python-stdlib 3.11 typing/Dict.: undefined\n    \"\"\"\n    prompt: str = \"1girl with brown dog hair, thick glasses, smiling\",\n    negative_prompt: str = \"bad image , bad quality\",\n    use_lcm_lora: bool = True,\n    use_tiny_vae: bool = True,\n    width: int = 512,\n    height: int = 512,\n    warmup: int = 10,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"xformers\", #undefined\n    device_ids: Optional[List[int]] = None,\n    \"\"\"\n    scip-python python python-stdlib 3.11 typing/Optional.: undefined\n    scip-python python python-stdlib 3.11 typing/List.: undefined\n    \"\"\"\n    use_denoising_batch: bool = True,\n    seed: int = 2,\n):\n    \"\"\"\n    Initializes the StreamDiffusionWrapper.\n\n    Parameters\n    ----------\n    iterations : int, optional\n        The number of iterations to run, by default 100.\n    model_id_or_path : str\n        The model id or path to load.\n    lora_dict : Optional[Dict[str, float]], optional\n        The lora_dict to load, by default None.\n        Keys are the LoRA names and values are the LoRA scales.\n        Example: {\"LoRA_1\" : 0.5 , \"LoRA_2\" : 0.7 ,...}\n    prompt : str, optional\n        The prompt to use, by default \"1girl with brown dog hair, thick glasses, smiling\".\n    negative_prompt : str, optional\n        The negative prompt to use, by default \"bad image , bad quality\".\n    use_lcm_lora : bool, optional\n        Whether to use LCM-LoRA or not, by default True.\n    use_tiny_vae : bool, optional\n        Whether to use TinyVAE or not, by default True.\n    width : int, optional\n        The width of the image, by default 512.\n    height : int, optional\n        The height of the image, by default 512.\n    warmup : int, optional\n        The number of warmup steps to perform, by default 10.\n    acceleration : Literal[\"none\", \"xformers\", \"tensorrt\"], optional\n        The acceleration method, by default \"tensorrt\".\n    device_ids : Optional[List[int]], optional\n        The device ids to use for DataParallel, by default None.\n    use_denoising_batch : bool, optional\n        Whether to use denoising batch or not, by default True.\n    seed : int, optional\n        The seed, by default 2. if -1, use random seed.\n    \"\"\"        \n    stream = StreamDiffusionWrapper( #This code defines a class called `StreamDiffusionWrapper` that loads a model from a given path or ID, performs various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference. It also defines various methods for performing image generation and manipulation.\n        model_id_or_path=model_id_or_path,\n        t_index_list=[32, 45],\n        lora_dict=lora_dict,\n        mode=\"img2img\",\n        frame_buffer_size=1,\n        width=width,\n        height=height,\n        warmup=warmup,\n        acceleration=acceleration,\n        device_ids=device_ids,\n        use_lcm_lora=use_lcm_lora,\n        use_tiny_vae=use_tiny_vae,\n        enable_similar_image_filter=False,\n        similar_image_filter_threshold=0.98,\n        use_denoising_batch=use_denoising_batch,\n        cfg_type=\"self\",  # initialize, full, self , none\n        seed=seed,\n    )\n\n    stream.prepare( #This code prepares the model for inference by setting up the prompt, number of steps, and guidance scale.\n        prompt=prompt,\n        negative_prompt=negative_prompt,\n        num_inference_steps=50,\n        guidance_scale=1.2,\n        delta=0.5,\n    )\n\n    image = download_image(\"https://github.com/ddpn08.png\").resize((width, height))\n    \"\"\"\n    scip-python python temp indexer `examples.benchmark.multi`/download_image().: This code defines a function called `download_image` that takes a URL as an argument and returns a PIL.Image object representing the image at that URL. It uses the requests library to make an HTTP GET request to the URL, then uses the Pillow library to open the response content as an image file.\n    scip-python python Pillow 10.0.0 `pil.image`/Image#resize().: undefined\n    \"\"\"\n    image_tensor = stream.preprocess_image(image) #This code preprocesses an image by converting it to RGB and resizing it to a specific width and height. It then passes the preprocessed image through a neural network for further processing.\n\n    # warmup\n    for _ in range(warmup):\n        stream.stream(image_tensor) #undefined\n\n    queue = Queue()\n    p = Process(target=_postprocess_image, args=(queue,)) #This code is a function that continuously checks if there are any images in a queue and processes them using the `postprocess_image` function. It uses the `time.sleep` function to wait for 0.0005 seconds before checking again.\n    p.start()\n\n    results = []\n\n    start = torch.cuda.Event(enable_timing=True)\n    end = torch.cuda.Event(enable_timing=True)\n    for _ in tqdm(range(iterations)): #undefined\n        start.record()\n        out_tensor = stream.stream(image_tensor).cpu() #undefined\n        queue.put(out_tensor)\n        end.record()\n\n        torch.cuda.synchronize()\n        results.append(start.elapsed_time(end))\n\n    print(f\"Average time: {sum(results) / len(results)}ms\")\n    print(f\"Average FPS: {1000 / (sum(results) / len(results))}\")","documentation":"The code defines a class called `StreamDiffusionWrapper` that loads a model, performs various tasks, and prepares the model for inference. It also defines methods for performing image generation and manipulation."}},{"key":"scip-python python temp indexer `examples.benchmark.multi`/__init__:","attributes":{"symbol":"scip-python python temp indexer `examples.benchmark.multi`/__init__:","range":[0,0,146,0],"content":"import io\nimport os\nimport sys\nimport time\nfrom multiprocessing import Process, Queue\nfrom typing import List, Literal, Optional, Dict\n\nimport fire\nimport PIL.Image\nimport requests\nimport torch\nfrom tqdm import tqdm\n\nfrom streamdiffusion.image_utils import postprocess_image\n\nsys.path.append(os.path.join(os.path.dirname(__file__), \"..\", \"..\"))\n\nfrom utils.wrapper import StreamDiffusionWrapper\n\n\ndef _postprocess_image(queue: Queue) -> None:\n    while True:\n        try:\n            if not queue.empty():\n                output = postprocess_image(queue.get(block=False), output_type=\"pil\")[0]\n            time.sleep(0.0005)\n        except KeyboardInterrupt:\n            return\n\n\ndef download_image(url: str):\n    response = requests.get(url)\n    image = PIL.Image.open(io.BytesIO(response.content))\n    return image\n\n\ndef run(\n    iterations: int = 100,\n    model_id_or_path: str = \"KBlueLeaf/kohaku-v2.1\",\n    lora_dict: Optional[Dict[str, float]] = None,\n    prompt: str = \"1girl with brown dog hair, thick glasses, smiling\",\n    negative_prompt: str = \"bad image , bad quality\",\n    use_lcm_lora: bool = True,\n    use_tiny_vae: bool = True,\n    width: int = 512,\n    height: int = 512,\n    warmup: int = 10,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"xformers\",\n    device_ids: Optional[List[int]] = None,\n    use_denoising_batch: bool = True,\n    seed: int = 2,\n):\n    \"\"\"\n    Initializes the StreamDiffusionWrapper.\n\n    Parameters\n    ----------\n    iterations : int, optional\n        The number of iterations to run, by default 100.\n    model_id_or_path : str\n        The model id or path to load.\n    lora_dict : Optional[Dict[str, float]], optional\n        The lora_dict to load, by default None.\n        Keys are the LoRA names and values are the LoRA scales.\n        Example: {\"LoRA_1\" : 0.5 , \"LoRA_2\" : 0.7 ,...}\n    prompt : str, optional\n        The prompt to use, by default \"1girl with brown dog hair, thick glasses, smiling\".\n    negative_prompt : str, optional\n        The negative prompt to use, by default \"bad image , bad quality\".\n    use_lcm_lora : bool, optional\n        Whether to use LCM-LoRA or not, by default True.\n    use_tiny_vae : bool, optional\n        Whether to use TinyVAE or not, by default True.\n    width : int, optional\n        The width of the image, by default 512.\n    height : int, optional\n        The height of the image, by default 512.\n    warmup : int, optional\n        The number of warmup steps to perform, by default 10.\n    acceleration : Literal[\"none\", \"xformers\", \"tensorrt\"], optional\n        The acceleration method, by default \"tensorrt\".\n    device_ids : Optional[List[int]], optional\n        The device ids to use for DataParallel, by default None.\n    use_denoising_batch : bool, optional\n        Whether to use denoising batch or not, by default True.\n    seed : int, optional\n        The seed, by default 2. if -1, use random seed.\n    \"\"\"        \n    stream = StreamDiffusionWrapper(\n        model_id_or_path=model_id_or_path,\n        t_index_list=[32, 45],\n        lora_dict=lora_dict,\n        mode=\"img2img\",\n        frame_buffer_size=1,\n        width=width,\n        height=height,\n        warmup=warmup,\n        acceleration=acceleration,\n        device_ids=device_ids,\n        use_lcm_lora=use_lcm_lora,\n        use_tiny_vae=use_tiny_vae,\n        enable_similar_image_filter=False,\n        similar_image_filter_threshold=0.98,\n        use_denoising_batch=use_denoising_batch,\n        cfg_type=\"self\",  # initialize, full, self , none\n        seed=seed,\n    )\n\n    stream.prepare(\n        prompt=prompt,\n        negative_prompt=negative_prompt,\n        num_inference_steps=50,\n        guidance_scale=1.2,\n        delta=0.5,\n    )\n\n    image = download_image(\"https://github.com/ddpn08.png\").resize((width, height))\n    image_tensor = stream.preprocess_image(image)\n\n    # warmup\n    for _ in range(warmup):\n        stream.stream(image_tensor)\n\n    queue = Queue()\n    p = Process(target=_postprocess_image, args=(queue,))\n    p.start()\n\n    results = []\n\n    start = torch.cuda.Event(enable_timing=True)\n    end = torch.cuda.Event(enable_timing=True)\n    for _ in tqdm(range(iterations)):\n        start.record()\n        out_tensor = stream.stream(image_tensor).cpu()\n        queue.put(out_tensor)\n        end.record()\n\n        torch.cuda.synchronize()\n        results.append(start.elapsed_time(end))\n\n    print(f\"Average time: {sum(results) / len(results)}ms\")\n    print(f\"Average FPS: {1000 / (sum(results) / len(results))}\")\n\n\nif __name__ == \"__main__\":\n    fire.Fire(run)\n","file":"/examples/benchmark/multi.py","language":"python","fileHash":"db4b8c400f4c5751cae4a3ea99b759d7985abe42e1983a39013480e390ea485e","hash":"db4b8c400f4c5751cae4a3ea99b759d7985abe42e1983a39013480e390ea485e","processedContent":"import io #undefined\nimport os #undefined\nimport sys #undefined\nimport time #undefined\nfrom multiprocessing import Process, Queue #undefined\nfrom typing import List, Literal, Optional, Dict\n\"\"\"\nscip-python python python-stdlib 3.11 typing/__init__:: undefined\nscip-python python python-stdlib 3.11 typing/List.: undefined\nscip-python python python-stdlib 3.11 typing/Literal.: undefined\nscip-python python python-stdlib 3.11 typing/Optional.: undefined\nscip-python python python-stdlib 3.11 typing/Dict.: undefined\n\"\"\"\n\nimport fire\nimport PIL.Image #undefined\nimport requests #undefined\nimport torch\nfrom tqdm import tqdm\n\"\"\"\nscip-python python tqdm 4.66.1 tqdm/__init__:: undefined\nscip-python python tqdm 4.66.1 `tqdm.std`/tqdm#: undefined\n\"\"\"\n\nfrom streamdiffusion.image_utils import postprocess_image #undefined\n\nsys.path.append(os.path.join(os.path.dirname(__file__), \"..\", \"..\"))\n\"\"\"\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 sys/__init__:: undefined\nscip-python python python-stdlib 3.11 sys/path.path.: undefined\nscip-python python python-stdlib 3.11 os/path.: undefined\nscip-python python python-stdlib 3.11 os/path.: undefined\nscip-python python python-stdlib 3.11 ntpath/join().: undefined\nscip-python python python-stdlib 3.11 ntpath/join().: undefined\n\"\"\"\n\nfrom utils.wrapper import StreamDiffusionWrapper\n\"\"\"\nscip-python python temp indexer `utils.wrapper`/__init__:: This code defines a class called `StreamDiffusionWrapper` that loads a model for image generation and manipulation, enabling acceleration, and preparing the model for inference.\nscip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#: This code defines a class called `StreamDiffusionWrapper` that loads a model from a given path or ID, performs various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference. It also defines various methods for performing image generation and manipulation.\n\"\"\"\n\n\ndef _postprocess_image(queue: Queue) -> None:\n    \"\"\"This code is a function that continuously checks if there are any images in a queue and processes them using the `postprocess_image` function. It uses the `time.sleep` function to wait for 0.0005 seconds before checking again.\"\"\"\n    pass\n\n\ndef download_image(url: str):\n    \"\"\"This code defines a function called `download_image` that takes a URL as an argument and returns a PIL.Image object representing the image at that URL. It uses the requests library to make an HTTP GET request to the URL, then uses the Pillow library to open the response content as an image file.\"\"\"\n    pass\n\n\ndef run(\n    \"\"\"The code defines a class called `StreamDiffusionWrapper` that loads a model, performs various tasks, and prepares the model for inference. It also defines methods for performing image generation and manipulation.\"\"\"\n    pass\n\n\nif __name__ == \"__main__\": #undefined\n    fire.Fire(run) #The code defines a class called `StreamDiffusionWrapper` that loads a model, performs various tasks, and prepares the model for inference. It also defines methods for performing image generation and manipulation.\n","documentation":"This code defines a class called `StreamDiffusionWrapper` that loads a model for image generation and manipulation, enabling acceleration, and preparing the model for inference."}},{"key":"scip-python python python-stdlib 3.11 time/__init__:","attributes":{"symbol":"scip-python python python-stdlib 3.11 time/__init__:","language":"python"}},{"key":"scip-python python python-stdlib 3.11 multiprocessing/__init__:","attributes":{"symbol":"scip-python python python-stdlib 3.11 multiprocessing/__init__:","language":"python"}},{"key":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"symbol":"scip-python python python-stdlib 3.11 typing/Optional.","language":"python"}},{"key":"scip-python python python-stdlib 3.11 typing/Dict.","attributes":{"symbol":"scip-python python python-stdlib 3.11 typing/Dict.","language":"python"}},{"key":"scip-python python requests 2.31.0 requests/__init__:","attributes":{"symbol":"scip-python python requests 2.31.0 requests/__init__:","language":"python"}},{"key":"scip-python python tqdm 4.66.1 tqdm/__init__:","attributes":{"symbol":"scip-python python tqdm 4.66.1 tqdm/__init__:","language":"python"}},{"key":"scip-python python tqdm 4.66.1 `tqdm.std`/tqdm#","attributes":{"symbol":"scip-python python tqdm 4.66.1 `tqdm.std`/tqdm#","language":"python"}},{"key":"scip-python python temp indexer `streamdiffusion.image_utils`/__init__:","attributes":{"symbol":"scip-python python temp indexer `streamdiffusion.image_utils`/__init__:","language":"python"}},{"key":"scip-python python python-stdlib 3.11 queue/Queue#empty().","attributes":{"symbol":"scip-python python python-stdlib 3.11 queue/Queue#empty().","language":"python"}},{"key":"scip-python python python-stdlib 3.11 time/sleep().","attributes":{"symbol":"scip-python python python-stdlib 3.11 time/sleep().","language":"python"}},{"key":"scip-python python requests 2.31.0 `requests.api`/get().","attributes":{"symbol":"scip-python python requests 2.31.0 `requests.api`/get().","language":"python"}},{"key":"scip-python python requests 2.31.0 `requests.models`/Response#content().","attributes":{"symbol":"scip-python python requests 2.31.0 `requests.models`/Response#content().","language":"python"}},{"key":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#prepare().","attributes":{"symbol":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#prepare().","language":"python","range":[173,4,202,9],"content":"def prepare(\n        self,\n        prompt: str,\n        negative_prompt: str = \"\",\n        num_inference_steps: int = 50,\n        guidance_scale: float = 1.2,\n        delta: float = 1.0,\n    ) -> None:\n        \"\"\"\n        Prepares the model for inference.\n\n        Parameters\n        ----------\n        prompt : str\n            The prompt to generate images from.\n        num_inference_steps : int, optional\n            The number of inference steps to perform, by default 50.\n        guidance_scale : float, optional\n            The guidance scale to use, by default 1.2.\n        delta : float, optional\n            The delta multiplier of virtual residual noise,\n            by default 1.0.\n        \"\"\"\n        self.stream.prepare(\n            prompt,\n            negative_prompt,\n            num_inference_steps=num_inference_steps,\n            guidance_scale=guidance_scale,\n            delta=delta,\n        )","file":"/utils/wrapper.py","fileHash":"4fc2c1f8d34f59a019136fa442db9ed0ff27859512221442ea1df929458d10e7","hash":"3c776921c048463a0be0377cbda23a0e56524debcdfc8267d2be280926bb03a4","processedContent":"def prepare(\n        self,\n        prompt: str,\n        negative_prompt: str = \"\",\n        num_inference_steps: int = 50,\n        guidance_scale: float = 1.2,\n        delta: float = 1.0,\n    ) -> None:\n        \"\"\"\n        Prepares the model for inference.\n\n        Parameters\n        ----------\n        prompt : str\n            The prompt to generate images from.\n        num_inference_steps : int, optional\n            The number of inference steps to perform, by default 50.\n        guidance_scale : float, optional\n            The guidance scale to use, by default 1.2.\n        delta : float, optional\n            The delta multiplier of virtual residual noise,\n            by default 1.0.\n        \"\"\"\n        self.stream.prepare( #undefined\n            prompt,\n            negative_prompt,\n            num_inference_steps=num_inference_steps,\n            guidance_scale=guidance_scale,\n            delta=delta,\n        )","documentation":"This code prepares the model for inference by setting up the prompt, number of steps, and guidance scale."}},{"key":"scip-python python Pillow 10.0.0 `pil.image`/Image#resize().","attributes":{"symbol":"scip-python python Pillow 10.0.0 `pil.image`/Image#resize().","language":"python"}},{"key":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#preprocess_image().","attributes":{"symbol":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#preprocess_image().","language":"python","range":[300,4,321,50],"content":"def preprocess_image(self, image: Union[str, Image.Image]) -> torch.Tensor:\n        \"\"\"\n        Preprocesses the image.\n\n        Parameters\n        ----------\n        image : Union[str, Image.Image, torch.Tensor]\n            The image to preprocess.\n\n        Returns\n        -------\n        torch.Tensor\n            The preprocessed image.\n        \"\"\"\n        if isinstance(image, str):\n            image = Image.open(image).convert(\"RGB\").resize((self.width, self.height))\n        if isinstance(image, Image.Image):\n            image = image.convert(\"RGB\").resize((self.width, self.height))\n\n        return self.stream.image_processor.preprocess(\n            image, self.height, self.width\n        ).to(device=self.device, dtype=self.dtype)","file":"/utils/wrapper.py","fileHash":"4fc2c1f8d34f59a019136fa442db9ed0ff27859512221442ea1df929458d10e7","hash":"a3af90e760cd2ad9b0afc4368c3c62000a2d4b2063b7d2dacd6b31efc0d6fbf3","processedContent":"def preprocess_image(self, image: Union[str, Image.Image]) -> torch.Tensor:\n        \"\"\"\n        Preprocesses the image.\n\n        Parameters\n        ----------\n        image : Union[str, Image.Image, torch.Tensor]\n            The image to preprocess.\n\n        Returns\n        -------\n        torch.Tensor\n            The preprocessed image.\n        \"\"\"\n        if isinstance(image, str):\n            image = Image.open(image).convert(\"RGB\").resize((self.width, self.height))\n            \"\"\"\n            scip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\n            scip-python python Pillow 10.0.0 `pil.image`/open().: undefined\n            scip-python python Pillow 10.0.0 `pil.image`/Image#convert().: undefined\n            scip-python python Pillow 10.0.0 `pil.image`/Image#resize().: undefined\n            scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#width.: undefined\n            scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#height.: undefined\n            \"\"\"\n        if isinstance(image, Image.Image):\n        \"\"\"\n        scip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\n        scip-python python Pillow 10.0.0 `pil.image`/Image#: undefined\n        \"\"\"\n            image = image.convert(\"RGB\").resize((self.width, self.height))\n            \"\"\"\n            scip-python python Pillow 10.0.0 `pil.image`/Image#convert().: undefined\n            scip-python python Pillow 10.0.0 `pil.image`/Image#resize().: undefined\n            scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#width.: undefined\n            scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#height.: undefined\n            \"\"\"\n\n        return self.stream.image_processor.preprocess( #undefined\n            image, self.height, self.width\n            \"\"\"\n            scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#width.: undefined\n            scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#height.: undefined\n            \"\"\"\n        ).to(device=self.device, dtype=self.dtype)","documentation":"This code preprocesses an image by converting it to RGB and resizing it to a specific width and height. It then passes the preprocessed image through a neural network for further processing."}},{"key":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#stream.","attributes":{"symbol":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#stream.","language":"python"}},{"key":"scip-python python temp indexer `examples.benchmark.multi`/","attributes":{"symbol":"scip-python python temp indexer `examples.benchmark.multi`/","language":"python"}},{"key":"scip-python python temp indexer `examples.benchmark.single`/download_image().","attributes":{"range":[17,0,20,16],"symbol":"scip-python python temp indexer `examples.benchmark.single`/download_image().","content":"def download_image(url: str):\n    response = requests.get(url)\n    image = PIL.Image.open(io.BytesIO(response.content))\n    return image","file":"/examples/benchmark/single.py","language":"python","fileHash":"a38f7fb71e48999e4f7157db5926eb4ccf194c88dfcfcb2e4c4cbc8b682d19f4","hash":"20a94ac9c375a37973ea7e1afc65cc4fdd9dd4b3d0b6d484680f6c064019c01a","processedContent":"def download_image(url: str):\n    response = requests.get(url)\n    \"\"\"\n    scip-python python requests 2.31.0 requests/__init__:: undefined\n    scip-python python requests 2.31.0 `requests.api`/get().: undefined\n    \"\"\"\n    image = PIL.Image.open(io.BytesIO(response.content))\n    \"\"\"\n    scip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\n    scip-python python Pillow 10.0.0 `pil.image`/open().: undefined\n    scip-python python python-stdlib 3.11 io/__init__:: undefined\n    scip-python python python-stdlib 3.11 io/BytesIO#: undefined\n    scip-python python requests 2.31.0 `requests.models`/Response#content().: undefined\n    \"\"\"\n    return image","documentation":"This code defines a function called `download_image` that takes a URL as an argument and returns a PIL.Image object representing the image at that URL. It uses the requests library to make an HTTP GET request to the URL, then uses the Pillow library to open the response content as an image file."}},{"key":"scip-python python temp indexer `examples.benchmark.single`/run().","attributes":{"range":[23,0,133,36],"symbol":"scip-python python temp indexer `examples.benchmark.single`/run().","content":"def run(\n    iterations: int = 100,\n    model_id_or_path: str = \"KBlueLeaf/kohaku-v2.1\",\n    lora_dict: Optional[Dict[str, float]] = None,\n    prompt: str = \"1girl with brown dog hair, thick glasses, smiling\",\n    negative_prompt: str = \"bad image , bad quality\",\n    use_lcm_lora: bool = True,\n    use_tiny_vae: bool = True,\n    width: int = 512,\n    height: int = 512,\n    warmup: int = 10,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"xformers\",\n    device_ids: Optional[List[int]] = None,\n    use_denoising_batch: bool = True,\n    seed: int = 2,\n):\n    \"\"\"\n    Initializes the StreamDiffusionWrapper.\n\n    Parameters\n    ----------\n    iterations : int, optional\n        The number of iterations to run, by default 100.\n    model_id_or_path : str\n        The model id or path to load.\n    lora_dict : Optional[Dict[str, float]], optional\n        The lora_dict to load, by default None.\n        Keys are the LoRA names and values are the LoRA scales.\n        Example: {\"LoRA_1\" : 0.5 , \"LoRA_2\" : 0.7 ,...}\n    prompt : str, optional\n        The prompt to use, by default \"1girl with brown dog hair, thick glasses, smiling\".\n    negative_prompt : str, optional\n        The negative prompt to use, by default \"bad image , bad quality\".\n    use_lcm_lora : bool, optional\n        Whether to use LCM-LoRA or not, by default True.\n    use_tiny_vae : bool, optional\n        Whether to use TinyVAE or not, by default True.\n    width : int, optional\n        The width of the image, by default 512.\n    height : int, optional\n        The height of the image, by default 512.\n    warmup : int, optional\n        The number of warmup steps to perform, by default 10.\n    acceleration : Literal[\"none\", \"xformers\", \"tensorrt\"], optional\n        The acceleration method, by default \"tensorrt\".\n    device_ids : Optional[List[int]], optional\n        The device ids to use for DataParallel, by default None.\n    use_denoising_batch : bool, optional\n        Whether to use denoising batch or not, by default True.\n    seed : int, optional\n        The seed, by default 2. if -1, use random seed.\n    \"\"\"       \n    stream = StreamDiffusionWrapper(\n        model_id_or_path=model_id_or_path,\n        t_index_list=[32, 45],\n        lora_dict=lora_dict,\n        mode=\"img2img\",\n        frame_buffer_size=1,\n        width=width,\n        height=height,\n        warmup=warmup,\n        acceleration=acceleration,\n        device_ids=device_ids,\n        use_lcm_lora=use_lcm_lora,\n        use_tiny_vae=use_tiny_vae,\n        enable_similar_image_filter=False,\n        similar_image_filter_threshold=0.98,\n        use_denoising_batch=use_denoising_batch,\n        cfg_type=\"initialize\",  # initialize, full, self , none\n        seed=seed,\n    )\n\n    stream.prepare(\n        prompt=prompt,\n        negative_prompt=negative_prompt,\n        num_inference_steps=50,\n        guidance_scale=1.4,\n        delta=0.5,\n    )\n\n    downloaded_image = download_image(\"https://github.com/ddpn08.png\").resize(\n        (width, height)\n    )\n\n    # warmup\n    for _ in range(warmup):\n        image_tensor = stream.preprocess_image(downloaded_image)\n        stream(image=image_tensor)\n\n    results = []\n\n    start = torch.cuda.Event(enable_timing=True)\n    end = torch.cuda.Event(enable_timing=True)\n\n    for _ in tqdm(range(iterations)):\n        start.record()\n        image_tensor = stream.preprocess_image(downloaded_image)\n        stream(image=image_tensor)\n        end.record()\n\n        torch.cuda.synchronize()\n        results.append(start.elapsed_time(end))\n\n    print(f\"Average time: {sum(results) / len(results)}ms\")\n    print(f\"Average FPS: {1000 / (sum(results) / len(results))}\")\n    import numpy as np\n\n    fps_arr = 1000 / np.array(results)\n    print(f\"Max FPS: {np.max(fps_arr)}\")\n    print(f\"Min FPS: {np.min(fps_arr)}\")\n    print(f\"Std: {np.std(fps_arr)}\")","file":"/examples/benchmark/single.py","language":"python","fileHash":"a38f7fb71e48999e4f7157db5926eb4ccf194c88dfcfcb2e4c4cbc8b682d19f4","hash":"3d9a79281baaccee2ea3a1af50c95003b77eee9264880f829878ecdc7aa865ab","processedContent":"def run(\n    iterations: int = 100,\n    model_id_or_path: str = \"KBlueLeaf/kohaku-v2.1\",\n    lora_dict: Optional[Dict[str, float]] = None,\n    \"\"\"\n    scip-python python python-stdlib 3.11 typing/Optional.: undefined\n    scip-python python python-stdlib 3.11 typing/Dict.: undefined\n    \"\"\"\n    prompt: str = \"1girl with brown dog hair, thick glasses, smiling\",\n    negative_prompt: str = \"bad image , bad quality\",\n    use_lcm_lora: bool = True,\n    use_tiny_vae: bool = True,\n    width: int = 512,\n    height: int = 512,\n    warmup: int = 10,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"xformers\", #undefined\n    device_ids: Optional[List[int]] = None,\n    \"\"\"\n    scip-python python python-stdlib 3.11 typing/Optional.: undefined\n    scip-python python python-stdlib 3.11 typing/List.: undefined\n    \"\"\"\n    use_denoising_batch: bool = True,\n    seed: int = 2,\n):\n    \"\"\"\n    Initializes the StreamDiffusionWrapper.\n\n    Parameters\n    ----------\n    iterations : int, optional\n        The number of iterations to run, by default 100.\n    model_id_or_path : str\n        The model id or path to load.\n    lora_dict : Optional[Dict[str, float]], optional\n        The lora_dict to load, by default None.\n        Keys are the LoRA names and values are the LoRA scales.\n        Example: {\"LoRA_1\" : 0.5 , \"LoRA_2\" : 0.7 ,...}\n    prompt : str, optional\n        The prompt to use, by default \"1girl with brown dog hair, thick glasses, smiling\".\n    negative_prompt : str, optional\n        The negative prompt to use, by default \"bad image , bad quality\".\n    use_lcm_lora : bool, optional\n        Whether to use LCM-LoRA or not, by default True.\n    use_tiny_vae : bool, optional\n        Whether to use TinyVAE or not, by default True.\n    width : int, optional\n        The width of the image, by default 512.\n    height : int, optional\n        The height of the image, by default 512.\n    warmup : int, optional\n        The number of warmup steps to perform, by default 10.\n    acceleration : Literal[\"none\", \"xformers\", \"tensorrt\"], optional\n        The acceleration method, by default \"tensorrt\".\n    device_ids : Optional[List[int]], optional\n        The device ids to use for DataParallel, by default None.\n    use_denoising_batch : bool, optional\n        Whether to use denoising batch or not, by default True.\n    seed : int, optional\n        The seed, by default 2. if -1, use random seed.\n    \"\"\"       \n    stream = StreamDiffusionWrapper( #This code defines a class called `StreamDiffusionWrapper` that loads a model from a given path or ID, performs various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference. It also defines various methods for performing image generation and manipulation.\n        model_id_or_path=model_id_or_path,\n        t_index_list=[32, 45],\n        lora_dict=lora_dict,\n        mode=\"img2img\",\n        frame_buffer_size=1,\n        width=width,\n        height=height,\n        warmup=warmup,\n        acceleration=acceleration,\n        device_ids=device_ids,\n        use_lcm_lora=use_lcm_lora,\n        use_tiny_vae=use_tiny_vae,\n        enable_similar_image_filter=False,\n        similar_image_filter_threshold=0.98,\n        use_denoising_batch=use_denoising_batch,\n        cfg_type=\"initialize\",  # initialize, full, self , none\n        seed=seed,\n    )\n\n    stream.prepare( #This code prepares the model for inference by setting up the prompt, number of steps, and guidance scale.\n        prompt=prompt,\n        negative_prompt=negative_prompt,\n        num_inference_steps=50,\n        guidance_scale=1.4,\n        delta=0.5,\n    )\n\n    downloaded_image = download_image(\"https://github.com/ddpn08.png\").resize(\n    \"\"\"\n    scip-python python temp indexer `examples.benchmark.single`/download_image().: This code defines a function called `download_image` that takes a URL as an argument and returns a PIL.Image object representing the image at that URL. It uses the requests library to make an HTTP GET request to the URL, then uses the Pillow library to open the response content as an image file.\n    scip-python python Pillow 10.0.0 `pil.image`/Image#resize().: undefined\n    \"\"\"\n        (width, height)\n    )\n\n    # warmup\n    for _ in range(warmup):\n        image_tensor = stream.preprocess_image(downloaded_image) #This code preprocesses an image by converting it to RGB and resizing it to a specific width and height. It then passes the preprocessed image through a neural network for further processing.\n        stream(image=image_tensor)\n\n    results = []\n\n    start = torch.cuda.Event(enable_timing=True)\n    end = torch.cuda.Event(enable_timing=True)\n\n    for _ in tqdm(range(iterations)): #undefined\n        start.record()\n        image_tensor = stream.preprocess_image(downloaded_image) #This code preprocesses an image by converting it to RGB and resizing it to a specific width and height. It then passes the preprocessed image through a neural network for further processing.\n        stream(image=image_tensor)\n        end.record()\n\n        torch.cuda.synchronize()\n        results.append(start.elapsed_time(end))\n\n    print(f\"Average time: {sum(results) / len(results)}ms\")\n    print(f\"Average FPS: {1000 / (sum(results) / len(results))}\")\n    import numpy as np #undefined\n\n    fps_arr = 1000 / np.array(results) #undefined\n    print(f\"Max FPS: {np.max(fps_arr)}\") #undefined\n    print(f\"Min FPS: {np.min(fps_arr)}\") #undefined\n    print(f\"Std: {np.std(fps_arr)}\")","documentation":"The code defines a class called `StreamDiffusionWrapper` that loads a model from a given path or ID, performs various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference. It also defines various methods for performing image generation and manipulation."}},{"key":"scip-python python temp indexer `examples.benchmark.single`/__init__:","attributes":{"symbol":"scip-python python temp indexer `examples.benchmark.single`/__init__:","range":[0,0,138,0],"content":"import io\nimport os\nimport sys\nfrom typing import List, Literal, Optional, Dict\n\nimport fire\nimport PIL.Image\nimport requests\nimport torch\nfrom tqdm import tqdm\n\n\nsys.path.append(os.path.join(os.path.dirname(__file__), \"..\", \"..\"))\n\nfrom utils.wrapper import StreamDiffusionWrapper\n\n\ndef download_image(url: str):\n    response = requests.get(url)\n    image = PIL.Image.open(io.BytesIO(response.content))\n    return image\n\n\ndef run(\n    iterations: int = 100,\n    model_id_or_path: str = \"KBlueLeaf/kohaku-v2.1\",\n    lora_dict: Optional[Dict[str, float]] = None,\n    prompt: str = \"1girl with brown dog hair, thick glasses, smiling\",\n    negative_prompt: str = \"bad image , bad quality\",\n    use_lcm_lora: bool = True,\n    use_tiny_vae: bool = True,\n    width: int = 512,\n    height: int = 512,\n    warmup: int = 10,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"xformers\",\n    device_ids: Optional[List[int]] = None,\n    use_denoising_batch: bool = True,\n    seed: int = 2,\n):\n    \"\"\"\n    Initializes the StreamDiffusionWrapper.\n\n    Parameters\n    ----------\n    iterations : int, optional\n        The number of iterations to run, by default 100.\n    model_id_or_path : str\n        The model id or path to load.\n    lora_dict : Optional[Dict[str, float]], optional\n        The lora_dict to load, by default None.\n        Keys are the LoRA names and values are the LoRA scales.\n        Example: {\"LoRA_1\" : 0.5 , \"LoRA_2\" : 0.7 ,...}\n    prompt : str, optional\n        The prompt to use, by default \"1girl with brown dog hair, thick glasses, smiling\".\n    negative_prompt : str, optional\n        The negative prompt to use, by default \"bad image , bad quality\".\n    use_lcm_lora : bool, optional\n        Whether to use LCM-LoRA or not, by default True.\n    use_tiny_vae : bool, optional\n        Whether to use TinyVAE or not, by default True.\n    width : int, optional\n        The width of the image, by default 512.\n    height : int, optional\n        The height of the image, by default 512.\n    warmup : int, optional\n        The number of warmup steps to perform, by default 10.\n    acceleration : Literal[\"none\", \"xformers\", \"tensorrt\"], optional\n        The acceleration method, by default \"tensorrt\".\n    device_ids : Optional[List[int]], optional\n        The device ids to use for DataParallel, by default None.\n    use_denoising_batch : bool, optional\n        Whether to use denoising batch or not, by default True.\n    seed : int, optional\n        The seed, by default 2. if -1, use random seed.\n    \"\"\"       \n    stream = StreamDiffusionWrapper(\n        model_id_or_path=model_id_or_path,\n        t_index_list=[32, 45],\n        lora_dict=lora_dict,\n        mode=\"img2img\",\n        frame_buffer_size=1,\n        width=width,\n        height=height,\n        warmup=warmup,\n        acceleration=acceleration,\n        device_ids=device_ids,\n        use_lcm_lora=use_lcm_lora,\n        use_tiny_vae=use_tiny_vae,\n        enable_similar_image_filter=False,\n        similar_image_filter_threshold=0.98,\n        use_denoising_batch=use_denoising_batch,\n        cfg_type=\"initialize\",  # initialize, full, self , none\n        seed=seed,\n    )\n\n    stream.prepare(\n        prompt=prompt,\n        negative_prompt=negative_prompt,\n        num_inference_steps=50,\n        guidance_scale=1.4,\n        delta=0.5,\n    )\n\n    downloaded_image = download_image(\"https://github.com/ddpn08.png\").resize(\n        (width, height)\n    )\n\n    # warmup\n    for _ in range(warmup):\n        image_tensor = stream.preprocess_image(downloaded_image)\n        stream(image=image_tensor)\n\n    results = []\n\n    start = torch.cuda.Event(enable_timing=True)\n    end = torch.cuda.Event(enable_timing=True)\n\n    for _ in tqdm(range(iterations)):\n        start.record()\n        image_tensor = stream.preprocess_image(downloaded_image)\n        stream(image=image_tensor)\n        end.record()\n\n        torch.cuda.synchronize()\n        results.append(start.elapsed_time(end))\n\n    print(f\"Average time: {sum(results) / len(results)}ms\")\n    print(f\"Average FPS: {1000 / (sum(results) / len(results))}\")\n    import numpy as np\n\n    fps_arr = 1000 / np.array(results)\n    print(f\"Max FPS: {np.max(fps_arr)}\")\n    print(f\"Min FPS: {np.min(fps_arr)}\")\n    print(f\"Std: {np.std(fps_arr)}\")\n\n\nif __name__ == \"__main__\":\n    fire.Fire(run)\n","file":"/examples/benchmark/single.py","language":"python","fileHash":"a38f7fb71e48999e4f7157db5926eb4ccf194c88dfcfcb2e4c4cbc8b682d19f4","hash":"a38f7fb71e48999e4f7157db5926eb4ccf194c88dfcfcb2e4c4cbc8b682d19f4","processedContent":"import io #undefined\nimport os #undefined\nimport sys #undefined\nfrom typing import List, Literal, Optional, Dict\n\"\"\"\nscip-python python python-stdlib 3.11 typing/__init__:: undefined\nscip-python python python-stdlib 3.11 typing/List.: undefined\nscip-python python python-stdlib 3.11 typing/Literal.: undefined\nscip-python python python-stdlib 3.11 typing/Optional.: undefined\nscip-python python python-stdlib 3.11 typing/Dict.: undefined\n\"\"\"\n\nimport fire\nimport PIL.Image #undefined\nimport requests #undefined\nimport torch\nfrom tqdm import tqdm\n\"\"\"\nscip-python python tqdm 4.66.1 tqdm/__init__:: undefined\nscip-python python tqdm 4.66.1 `tqdm.std`/tqdm#: undefined\n\"\"\"\n\n\nsys.path.append(os.path.join(os.path.dirname(__file__), \"..\", \"..\"))\n\"\"\"\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 sys/__init__:: undefined\nscip-python python python-stdlib 3.11 sys/path.path.: undefined\nscip-python python python-stdlib 3.11 os/path.: undefined\nscip-python python python-stdlib 3.11 os/path.: undefined\nscip-python python python-stdlib 3.11 ntpath/join().: undefined\nscip-python python python-stdlib 3.11 ntpath/join().: undefined\n\"\"\"\n\nfrom utils.wrapper import StreamDiffusionWrapper\n\"\"\"\nscip-python python temp indexer `utils.wrapper`/__init__:: This code defines a class called `StreamDiffusionWrapper` that loads a model for image generation and manipulation, enabling acceleration, and preparing the model for inference.\nscip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#: This code defines a class called `StreamDiffusionWrapper` that loads a model from a given path or ID, performs various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference. It also defines various methods for performing image generation and manipulation.\n\"\"\"\n\n\ndef download_image(url: str):\n    \"\"\"This code defines a function called `download_image` that takes a URL as an argument and returns a PIL.Image object representing the image at that URL. It uses the requests library to make an HTTP GET request to the URL, then uses the Pillow library to open the response content as an image file.\"\"\"\n    pass\n\n\ndef run(\n    \"\"\"The code defines a class called `StreamDiffusionWrapper` that loads a model from a given path or ID, performs various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference. It also defines various methods for performing image generation and manipulation.\"\"\"\n    pass\n\n\nif __name__ == \"__main__\": #undefined\n    fire.Fire(run) #The code defines a class called `StreamDiffusionWrapper` that loads a model from a given path or ID, performs various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference. It also defines various methods for performing image generation and manipulation.\n","documentation":"This code defines a class called `StreamDiffusionWrapper` that loads a model for image generation and manipulation, enabling acceleration, and preparing the model for inference."}},{"key":"scip-python python numpy 1.25.2 numpy/__init__:","attributes":{"symbol":"scip-python python numpy 1.25.2 numpy/__init__:","language":"python"}},{"key":"scip-python python numpy 1.25.2 `numpy.core.multiarray`/array().","attributes":{"symbol":"scip-python python numpy 1.25.2 `numpy.core.multiarray`/array().","language":"python"}},{"key":"scip-python python numpy 1.25.2 `numpy.core.fromnumeric`/max.","attributes":{"symbol":"scip-python python numpy 1.25.2 `numpy.core.fromnumeric`/max.","language":"python"}},{"key":"scip-python python numpy 1.25.2 `numpy.core.fromnumeric`/min.","attributes":{"symbol":"scip-python python numpy 1.25.2 `numpy.core.fromnumeric`/min.","language":"python"}},{"key":"scip-python python numpy 1.25.2 `numpy.core.fromnumeric`/std().","attributes":{"symbol":"scip-python python numpy 1.25.2 `numpy.core.fromnumeric`/std().","language":"python"}},{"key":"scip-python python temp indexer `examples.benchmark.single`/","attributes":{"symbol":"scip-python python temp indexer `examples.benchmark.single`/","language":"python"}},{"key":"scip-python python temp indexer `examples.img2img.multi`/main().","attributes":{"range":[15,0,118,66],"symbol":"scip-python python temp indexer `examples.img2img.multi`/main().","content":"def main(\n    input: str = os.path.join(CURRENT_DIR, \"..\", \"..\", \"images\", \"inputs\"),\n    output: str = os.path.join(CURRENT_DIR, \"..\", \"..\", \"images\", \"outputs\"),\n    model_id_or_path: str = \"KBlueLeaf/kohaku-v2.1\",\n    lora_dict: Optional[Dict[str, float]] = None,\n    prompt: str = \"1girl with brown dog hair, thick glasses, smiling\",\n    negative_prompt: str = \"low quality, bad quality, blurry, low resolution\",\n    width: int = 512,\n    height: int = 512,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"xformers\",\n    use_denoising_batch: bool = True,\n    guidance_scale: float = 1.2,\n    cfg_type: Literal[\"none\", \"full\", \"self\", \"initialize\"] = \"self\",\n    seed: int = 2,\n    delta: float = 0.5,\n):\n    \"\"\"\n    Initializes the StreamDiffusionWrapper.\n\n    Parameters\n    ----------\n    input : str, optional\n        The input directory to load images from.\n    output : str, optional\n        The output directory to save images to.\n    model_id_or_path : str\n        The model id or path to load.\n    lora_dict : Optional[Dict[str, float]], optional\n        The lora_dict to load, by default None.\n        Keys are the LoRA names and values are the LoRA scales.\n        Example: {\"LoRA_1\" : 0.5 , \"LoRA_2\" : 0.7 ,...}\n    prompt : str\n        The prompt to generate images from.\n    negative_prompt : str, optional\n        The negative prompt to use.\n    width : int, optional\n        The width of the image, by default 512.\n    height : int, optional\n        The height of the image, by default 512.\n    acceleration : Literal[\"none\", \"xformers\", \"tensorrt\"], optional\n        The acceleration method, by default \"tensorrt\".\n    use_denoising_batch : bool, optional\n        Whether to use denoising batch or not, by default True.\n    guidance_scale : float, optional\n        The CFG scale, by default 1.2.\n    cfg_type : Literal[\"none\", \"full\", \"self\", \"initialize\"],\n    optional\n        The cfg_type for img2img mode, by default \"self\".\n        You cannot use anything other than \"none\" for txt2img mode.\n    seed : int, optional\n        The seed, by default 2. if -1, use random seed.\n    delta : float, optional\n        The delta multiplier of virtual residual noise,\n        by default 1.0.\n    \"\"\"\n\n    if not os.path.exists(output):\n        os.makedirs(output, exist_ok=True)\n\n    if guidance_scale <= 1.0:\n        cfg_type = \"none\"\n\n    stream = StreamDiffusionWrapper(\n        model_id_or_path=model_id_or_path,\n        lora_dict=lora_dict,\n        t_index_list=[32, 40, 45],\n        frame_buffer_size=1,\n        width=width,\n        height=height,\n        warmup=10,\n        acceleration=acceleration,\n        mode=\"img2img\",\n        use_denoising_batch=use_denoising_batch,\n        cfg_type=cfg_type,\n        seed=seed,\n    )\n\n    stream.prepare(\n        prompt=prompt,\n        negative_prompt=negative_prompt,\n        num_inference_steps=50,\n        guidance_scale=guidance_scale,\n        delta=delta,\n    )\n\n    images = glob.glob(os.path.join(input, \"*\"))\n    images = images + [images[-1]] * (stream.batch_size - 1)\n    outputs = []\n\n    for i in range(stream.batch_size - 1):\n        image = images.pop(0)\n        outputs.append(image)\n        output_image = stream(image=image)\n\n    for image in images:\n        outputs.append(image)\n        try:\n            output_image = stream(image=image)\n        except Exception:\n            continue\n\n        name = outputs.pop(0)\n        basename = os.path.splitext(os.path.basename(name))[0]\n        output_image.save(os.path.join(output, f\"{basename}.png\"))","file":"/examples/img2img/multi.py","language":"python","fileHash":"171949ad20055d11ddb58e227152a12e672be9b08dd3679dc54049e8840a51f2","hash":"edef9c78b059c12afc14b731b64b528c6cc1781c8eadd25c08af4d3196866ead","processedContent":"def main(\n    input: str = os.path.join(CURRENT_DIR, \"..\", \"..\", \"images\", \"inputs\"),\n    \"\"\"\n    scip-python python python-stdlib 3.11 os/__init__:: undefined\n    scip-python python python-stdlib 3.11 os/path.: undefined\n    scip-python python python-stdlib 3.11 ntpath/join().: undefined\n    scip-python python temp indexer `examples.img2img.multi`/CURRENT_DIR.: undefined\n    \"\"\"\n    output: str = os.path.join(CURRENT_DIR, \"..\", \"..\", \"images\", \"outputs\"),\n    \"\"\"\n    scip-python python python-stdlib 3.11 os/__init__:: undefined\n    scip-python python python-stdlib 3.11 os/path.: undefined\n    scip-python python python-stdlib 3.11 ntpath/join().: undefined\n    scip-python python temp indexer `examples.img2img.multi`/CURRENT_DIR.: undefined\n    \"\"\"\n    model_id_or_path: str = \"KBlueLeaf/kohaku-v2.1\",\n    lora_dict: Optional[Dict[str, float]] = None,\n    \"\"\"\n    scip-python python python-stdlib 3.11 typing/Optional.: undefined\n    scip-python python python-stdlib 3.11 typing/Dict.: undefined\n    \"\"\"\n    prompt: str = \"1girl with brown dog hair, thick glasses, smiling\",\n    negative_prompt: str = \"low quality, bad quality, blurry, low resolution\",\n    width: int = 512,\n    height: int = 512,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"xformers\", #undefined\n    use_denoising_batch: bool = True,\n    guidance_scale: float = 1.2,\n    cfg_type: Literal[\"none\", \"full\", \"self\", \"initialize\"] = \"self\", #undefined\n    seed: int = 2,\n    delta: float = 0.5,\n):\n    \"\"\"\n    Initializes the StreamDiffusionWrapper.\n\n    Parameters\n    ----------\n    input : str, optional\n        The input directory to load images from.\n    output : str, optional\n        The output directory to save images to.\n    model_id_or_path : str\n        The model id or path to load.\n    lora_dict : Optional[Dict[str, float]], optional\n        The lora_dict to load, by default None.\n        Keys are the LoRA names and values are the LoRA scales.\n        Example: {\"LoRA_1\" : 0.5 , \"LoRA_2\" : 0.7 ,...}\n    prompt : str\n        The prompt to generate images from.\n    negative_prompt : str, optional\n        The negative prompt to use.\n    width : int, optional\n        The width of the image, by default 512.\n    height : int, optional\n        The height of the image, by default 512.\n    acceleration : Literal[\"none\", \"xformers\", \"tensorrt\"], optional\n        The acceleration method, by default \"tensorrt\".\n    use_denoising_batch : bool, optional\n        Whether to use denoising batch or not, by default True.\n    guidance_scale : float, optional\n        The CFG scale, by default 1.2.\n    cfg_type : Literal[\"none\", \"full\", \"self\", \"initialize\"],\n    optional\n        The cfg_type for img2img mode, by default \"self\".\n        You cannot use anything other than \"none\" for txt2img mode.\n    seed : int, optional\n        The seed, by default 2. if -1, use random seed.\n    delta : float, optional\n        The delta multiplier of virtual residual noise,\n        by default 1.0.\n    \"\"\"\n\n    if not os.path.exists(output):\n    \"\"\"\n    scip-python python python-stdlib 3.11 os/__init__:: undefined\n    scip-python python python-stdlib 3.11 os/path.: undefined\n    scip-python python python-stdlib 3.11 ntpath/join().: undefined\n    \"\"\"\n        os.makedirs(output, exist_ok=True)\n        \"\"\"\n        scip-python python python-stdlib 3.11 os/__init__:: undefined\n        scip-python python python-stdlib 3.11 os/makedirs().: undefined\n        \"\"\"\n\n    if guidance_scale <= 1.0:\n        cfg_type = \"none\"\n\n    stream = StreamDiffusionWrapper( #This code defines a class called `StreamDiffusionWrapper` that loads a model from a given path or ID, performs various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference. It also defines various methods for performing image generation and manipulation.\n        model_id_or_path=model_id_or_path,\n        lora_dict=lora_dict,\n        t_index_list=[32, 40, 45],\n        frame_buffer_size=1,\n        width=width,\n        height=height,\n        warmup=10,\n        acceleration=acceleration,\n        mode=\"img2img\",\n        use_denoising_batch=use_denoising_batch,\n        cfg_type=cfg_type,\n        seed=seed,\n    )\n\n    stream.prepare( #This code prepares the model for inference by setting up the prompt, number of steps, and guidance scale.\n        prompt=prompt,\n        negative_prompt=negative_prompt,\n        num_inference_steps=50,\n        guidance_scale=guidance_scale,\n        delta=delta,\n    )\n\n    images = glob.glob(os.path.join(input, \"*\"))\n    \"\"\"\n    scip-python python python-stdlib 3.11 os/__init__:: undefined\n    scip-python python python-stdlib 3.11 os/path.: undefined\n    scip-python python python-stdlib 3.11 ntpath/join().: undefined\n    scip-python python python-stdlib 3.11 glob/__init__:: undefined\n    scip-python python python-stdlib 3.11 glob/glob().: undefined\n    \"\"\"\n    images = images + [images[-1]] * (stream.batch_size - 1) #undefined\n    outputs = []\n\n    for i in range(stream.batch_size - 1): #undefined\n        image = images.pop(0)\n        outputs.append(image)\n        output_image = stream(image=image)\n\n    for image in images:\n        outputs.append(image)\n        try:\n            output_image = stream(image=image)\n        except Exception:\n            continue\n\n        name = outputs.pop(0)\n        basename = os.path.splitext(os.path.basename(name))[0]\n        \"\"\"\n        scip-python python python-stdlib 3.11 os/__init__:: undefined\n        scip-python python python-stdlib 3.11 os/__init__:: undefined\n        scip-python python python-stdlib 3.11 os/path.: undefined\n        scip-python python python-stdlib 3.11 os/path.: undefined\n        scip-python python python-stdlib 3.11 ntpath/join().: undefined\n        scip-python python python-stdlib 3.11 ntpath/join().: undefined\n        \"\"\"\n        output_image.save(os.path.join(output, f\"{basename}.png\"))","documentation":"This code defines a function called `main` that initializes a StreamDiffusionWrapper class, prepares the model for inference, and generates images based on a prompt. It also handles the output directory and file names."}},{"key":"scip-python python temp indexer `examples.img2img.multi`/__init__:","attributes":{"symbol":"scip-python python temp indexer `examples.img2img.multi`/__init__:","range":[0,0,123,0],"content":"import glob\nimport os\nimport sys\nfrom typing import Literal, Dict, Optional\n\nimport fire\n\n\nsys.path.append(os.path.join(os.path.dirname(__file__), \"..\", \"..\"))\n\nfrom utils.wrapper import StreamDiffusionWrapper\n\nCURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n\n\ndef main(\n    input: str = os.path.join(CURRENT_DIR, \"..\", \"..\", \"images\", \"inputs\"),\n    output: str = os.path.join(CURRENT_DIR, \"..\", \"..\", \"images\", \"outputs\"),\n    model_id_or_path: str = \"KBlueLeaf/kohaku-v2.1\",\n    lora_dict: Optional[Dict[str, float]] = None,\n    prompt: str = \"1girl with brown dog hair, thick glasses, smiling\",\n    negative_prompt: str = \"low quality, bad quality, blurry, low resolution\",\n    width: int = 512,\n    height: int = 512,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"xformers\",\n    use_denoising_batch: bool = True,\n    guidance_scale: float = 1.2,\n    cfg_type: Literal[\"none\", \"full\", \"self\", \"initialize\"] = \"self\",\n    seed: int = 2,\n    delta: float = 0.5,\n):\n    \"\"\"\n    Initializes the StreamDiffusionWrapper.\n\n    Parameters\n    ----------\n    input : str, optional\n        The input directory to load images from.\n    output : str, optional\n        The output directory to save images to.\n    model_id_or_path : str\n        The model id or path to load.\n    lora_dict : Optional[Dict[str, float]], optional\n        The lora_dict to load, by default None.\n        Keys are the LoRA names and values are the LoRA scales.\n        Example: {\"LoRA_1\" : 0.5 , \"LoRA_2\" : 0.7 ,...}\n    prompt : str\n        The prompt to generate images from.\n    negative_prompt : str, optional\n        The negative prompt to use.\n    width : int, optional\n        The width of the image, by default 512.\n    height : int, optional\n        The height of the image, by default 512.\n    acceleration : Literal[\"none\", \"xformers\", \"tensorrt\"], optional\n        The acceleration method, by default \"tensorrt\".\n    use_denoising_batch : bool, optional\n        Whether to use denoising batch or not, by default True.\n    guidance_scale : float, optional\n        The CFG scale, by default 1.2.\n    cfg_type : Literal[\"none\", \"full\", \"self\", \"initialize\"],\n    optional\n        The cfg_type for img2img mode, by default \"self\".\n        You cannot use anything other than \"none\" for txt2img mode.\n    seed : int, optional\n        The seed, by default 2. if -1, use random seed.\n    delta : float, optional\n        The delta multiplier of virtual residual noise,\n        by default 1.0.\n    \"\"\"\n\n    if not os.path.exists(output):\n        os.makedirs(output, exist_ok=True)\n\n    if guidance_scale <= 1.0:\n        cfg_type = \"none\"\n\n    stream = StreamDiffusionWrapper(\n        model_id_or_path=model_id_or_path,\n        lora_dict=lora_dict,\n        t_index_list=[32, 40, 45],\n        frame_buffer_size=1,\n        width=width,\n        height=height,\n        warmup=10,\n        acceleration=acceleration,\n        mode=\"img2img\",\n        use_denoising_batch=use_denoising_batch,\n        cfg_type=cfg_type,\n        seed=seed,\n    )\n\n    stream.prepare(\n        prompt=prompt,\n        negative_prompt=negative_prompt,\n        num_inference_steps=50,\n        guidance_scale=guidance_scale,\n        delta=delta,\n    )\n\n    images = glob.glob(os.path.join(input, \"*\"))\n    images = images + [images[-1]] * (stream.batch_size - 1)\n    outputs = []\n\n    for i in range(stream.batch_size - 1):\n        image = images.pop(0)\n        outputs.append(image)\n        output_image = stream(image=image)\n\n    for image in images:\n        outputs.append(image)\n        try:\n            output_image = stream(image=image)\n        except Exception:\n            continue\n\n        name = outputs.pop(0)\n        basename = os.path.splitext(os.path.basename(name))[0]\n        output_image.save(os.path.join(output, f\"{basename}.png\"))\n\n\nif __name__ == \"__main__\":\n    fire.Fire(main)\n","file":"/examples/img2img/multi.py","language":"python","fileHash":"171949ad20055d11ddb58e227152a12e672be9b08dd3679dc54049e8840a51f2","hash":"171949ad20055d11ddb58e227152a12e672be9b08dd3679dc54049e8840a51f2","processedContent":"import glob #undefined\nimport os #undefined\nimport sys #undefined\nfrom typing import Literal, Dict, Optional\n\"\"\"\nscip-python python python-stdlib 3.11 typing/__init__:: undefined\nscip-python python python-stdlib 3.11 typing/Literal.: undefined\nscip-python python python-stdlib 3.11 typing/Dict.: undefined\nscip-python python python-stdlib 3.11 typing/Optional.: undefined\n\"\"\"\n\nimport fire\n\n\nsys.path.append(os.path.join(os.path.dirname(__file__), \"..\", \"..\"))\n\"\"\"\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 sys/__init__:: undefined\nscip-python python python-stdlib 3.11 sys/path.path.: undefined\nscip-python python python-stdlib 3.11 os/path.: undefined\nscip-python python python-stdlib 3.11 os/path.: undefined\nscip-python python python-stdlib 3.11 ntpath/join().: undefined\nscip-python python python-stdlib 3.11 ntpath/join().: undefined\n\"\"\"\n\nfrom utils.wrapper import StreamDiffusionWrapper\n\"\"\"\nscip-python python temp indexer `utils.wrapper`/__init__:: This code defines a class called `StreamDiffusionWrapper` that loads a model for image generation and manipulation, enabling acceleration, and preparing the model for inference.\nscip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#: This code defines a class called `StreamDiffusionWrapper` that loads a model from a given path or ID, performs various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference. It also defines various methods for performing image generation and manipulation.\n\"\"\"\n\nCURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n\"\"\"\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 os/path.: undefined\nscip-python python python-stdlib 3.11 os/path.: undefined\nscip-python python python-stdlib 3.11 ntpath/join().: undefined\nscip-python python python-stdlib 3.11 ntpath/join().: undefined\nscip-python python temp indexer `examples.img2img.multi`/: undefined\n\"\"\"\n\n\ndef main(\n    \"\"\"This code defines a function called `main` that initializes a StreamDiffusionWrapper class, prepares the model for inference, and generates images based on a prompt. It also handles the output directory and file names.\"\"\"\n    pass\n\n\nif __name__ == \"__main__\": #undefined\n    fire.Fire(main) #This code defines a function called `main` that initializes a StreamDiffusionWrapper class, prepares the model for inference, and generates images based on a prompt. It also handles the output directory and file names.\n","documentation":"This code defines a function called `main` that initializes a StreamDiffusionWrapper class, prepares the model for inference, and generates images based on a prompt. It also handles the output directory and file names."}},{"key":"scip-python python python-stdlib 3.11 glob/__init__:","attributes":{"symbol":"scip-python python python-stdlib 3.11 glob/__init__:","language":"python"}},{"key":"scip-python python temp indexer `examples.img2img.multi`/","attributes":{"symbol":"scip-python python temp indexer `examples.img2img.multi`/","language":"python"}},{"key":"scip-python python temp indexer `examples.img2img.multi`/CURRENT_DIR.","attributes":{"symbol":"scip-python python temp indexer `examples.img2img.multi`/CURRENT_DIR.","language":"python"}},{"key":"scip-python python python-stdlib 3.11 os/makedirs().","attributes":{"symbol":"scip-python python python-stdlib 3.11 os/makedirs().","language":"python"}},{"key":"scip-python python python-stdlib 3.11 glob/glob().","attributes":{"symbol":"scip-python python python-stdlib 3.11 glob/glob().","language":"python"}},{"key":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#batch_size.","attributes":{"symbol":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#batch_size.","language":"python"}},{"key":"scip-python python temp indexer `examples.img2img.single`/main().","attributes":{"range":[14,0,102,29],"symbol":"scip-python python temp indexer `examples.img2img.single`/main().","content":"def main(\n    input: str = os.path.join(CURRENT_DIR, \"..\", \"..\", \"images\", \"inputs\", \"input.png\"),\n    output: str = os.path.join(CURRENT_DIR, \"..\", \"..\", \"images\", \"outputs\", \"output.png\"),\n    model_id_or_path: str = \"KBlueLeaf/kohaku-v2.1\",\n    lora_dict: Optional[Dict[str, float]] = None,\n    prompt: str = \"1girl with brown dog hair, thick glasses, smiling\",\n    negative_prompt: str = \"low quality, bad quality, blurry, low resolution\",\n    width: int = 512,\n    height: int = 512,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"xformers\",\n    use_denoising_batch: bool = True,\n    guidance_scale: float = 1.2,\n    cfg_type: Literal[\"none\", \"full\", \"self\", \"initialize\"] = \"self\",\n    seed: int = 2,\n    delta: float = 0.5,\n):\n    \"\"\"\n    Initializes the StreamDiffusionWrapper.\n\n    Parameters\n    ----------\n    input : str, optional\n        The input image file to load images from.\n    output : str, optional\n        The output image file to save images to.\n    model_id_or_path : str\n        The model id or path to load.\n    lora_dict : Optional[Dict[str, float]], optional\n        The lora_dict to load, by default None.\n        Keys are the LoRA names and values are the LoRA scales.\n        Example: {\"LoRA_1\" : 0.5 , \"LoRA_2\" : 0.7 ,...}\n    prompt : str\n        The prompt to generate images from.\n    negative_prompt : str, optional\n        The negative prompt to use.\n    width : int, optional\n        The width of the image, by default 512.\n    height : int, optional\n        The height of the image, by default 512.\n    acceleration : Literal[\"none\", \"xformers\", \"tensorrt\"], optional\n        The acceleration method, by default \"tensorrt\".\n    use_denoising_batch : bool, optional\n        Whether to use denoising batch or not, by default True.\n    guidance_scale : float, optional\n        The CFG scale, by default 1.2.\n    cfg_type : Literal[\"none\", \"full\", \"self\", \"initialize\"],\n    optional\n        The cfg_type for img2img mode, by default \"self\".\n        You cannot use anything other than \"none\" for txt2img mode.\n    seed : int, optional\n        The seed, by default 2. if -1, use random seed.\n    delta : float, optional\n        The delta multiplier of virtual residual noise,\n        by default 1.0.\n    \"\"\"\n\n    if guidance_scale <= 1.0:\n        cfg_type = \"none\"\n\n    stream = StreamDiffusionWrapper(\n        model_id_or_path=model_id_or_path,\n        lora_dict=lora_dict,\n        t_index_list=[22, 32, 45],\n        frame_buffer_size=1,\n        width=width,\n        height=height,\n        warmup=10,\n        acceleration=acceleration,\n        mode=\"img2img\",\n        use_denoising_batch=use_denoising_batch,\n        cfg_type=cfg_type,\n        seed=seed,\n    )\n\n    stream.prepare(\n        prompt=prompt,\n        negative_prompt=negative_prompt,\n        num_inference_steps=50,\n        guidance_scale=guidance_scale,\n        delta=delta,\n    )\n\n    image_tensor = stream.preprocess_image(input)\n\n    for _ in range(stream.batch_size - 1):\n        stream(image=image_tensor)\n\n    output_image = stream(image=image_tensor)\n    output_image.save(output)","file":"/examples/img2img/single.py","language":"python","fileHash":"47309c9265369f8cd803e703f72a7a1e153b1449cda74aa65aabb64bf9cdedf8","hash":"7f03cb3b06a8cd489a857169fcc98fa622a5e7fb2673c94be6d797fc7b22a3f4","processedContent":"def main(\n    input: str = os.path.join(CURRENT_DIR, \"..\", \"..\", \"images\", \"inputs\", \"input.png\"),\n    \"\"\"\n    scip-python python python-stdlib 3.11 os/__init__:: undefined\n    scip-python python python-stdlib 3.11 os/path.: undefined\n    scip-python python python-stdlib 3.11 ntpath/join().: undefined\n    scip-python python temp indexer `examples.img2img.single`/CURRENT_DIR.: undefined\n    \"\"\"\n    output: str = os.path.join(CURRENT_DIR, \"..\", \"..\", \"images\", \"outputs\", \"output.png\"),\n    \"\"\"\n    scip-python python python-stdlib 3.11 os/__init__:: undefined\n    scip-python python python-stdlib 3.11 os/path.: undefined\n    scip-python python python-stdlib 3.11 ntpath/join().: undefined\n    scip-python python temp indexer `examples.img2img.single`/CURRENT_DIR.: undefined\n    \"\"\"\n    model_id_or_path: str = \"KBlueLeaf/kohaku-v2.1\",\n    lora_dict: Optional[Dict[str, float]] = None,\n    \"\"\"\n    scip-python python python-stdlib 3.11 typing/Optional.: undefined\n    scip-python python python-stdlib 3.11 typing/Dict.: undefined\n    \"\"\"\n    prompt: str = \"1girl with brown dog hair, thick glasses, smiling\",\n    negative_prompt: str = \"low quality, bad quality, blurry, low resolution\",\n    width: int = 512,\n    height: int = 512,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"xformers\", #undefined\n    use_denoising_batch: bool = True,\n    guidance_scale: float = 1.2,\n    cfg_type: Literal[\"none\", \"full\", \"self\", \"initialize\"] = \"self\", #undefined\n    seed: int = 2,\n    delta: float = 0.5,\n):\n    \"\"\"\n    Initializes the StreamDiffusionWrapper.\n\n    Parameters\n    ----------\n    input : str, optional\n        The input image file to load images from.\n    output : str, optional\n        The output image file to save images to.\n    model_id_or_path : str\n        The model id or path to load.\n    lora_dict : Optional[Dict[str, float]], optional\n        The lora_dict to load, by default None.\n        Keys are the LoRA names and values are the LoRA scales.\n        Example: {\"LoRA_1\" : 0.5 , \"LoRA_2\" : 0.7 ,...}\n    prompt : str\n        The prompt to generate images from.\n    negative_prompt : str, optional\n        The negative prompt to use.\n    width : int, optional\n        The width of the image, by default 512.\n    height : int, optional\n        The height of the image, by default 512.\n    acceleration : Literal[\"none\", \"xformers\", \"tensorrt\"], optional\n        The acceleration method, by default \"tensorrt\".\n    use_denoising_batch : bool, optional\n        Whether to use denoising batch or not, by default True.\n    guidance_scale : float, optional\n        The CFG scale, by default 1.2.\n    cfg_type : Literal[\"none\", \"full\", \"self\", \"initialize\"],\n    optional\n        The cfg_type for img2img mode, by default \"self\".\n        You cannot use anything other than \"none\" for txt2img mode.\n    seed : int, optional\n        The seed, by default 2. if -1, use random seed.\n    delta : float, optional\n        The delta multiplier of virtual residual noise,\n        by default 1.0.\n    \"\"\"\n\n    if guidance_scale <= 1.0:\n        cfg_type = \"none\"\n\n    stream = StreamDiffusionWrapper( #This code defines a class called `StreamDiffusionWrapper` that loads a model from a given path or ID, performs various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference. It also defines various methods for performing image generation and manipulation.\n        model_id_or_path=model_id_or_path,\n        lora_dict=lora_dict,\n        t_index_list=[22, 32, 45],\n        frame_buffer_size=1,\n        width=width,\n        height=height,\n        warmup=10,\n        acceleration=acceleration,\n        mode=\"img2img\",\n        use_denoising_batch=use_denoising_batch,\n        cfg_type=cfg_type,\n        seed=seed,\n    )\n\n    stream.prepare( #This code prepares the model for inference by setting up the prompt, number of steps, and guidance scale.\n        prompt=prompt,\n        negative_prompt=negative_prompt,\n        num_inference_steps=50,\n        guidance_scale=guidance_scale,\n        delta=delta,\n    )\n\n    image_tensor = stream.preprocess_image(input) #This code preprocesses an image by converting it to RGB and resizing it to a specific width and height. It then passes the preprocessed image through a neural network for further processing.\n\n    for _ in range(stream.batch_size - 1): #undefined\n        stream(image=image_tensor)\n\n    output_image = stream(image=image_tensor)\n    output_image.save(output)","documentation":"This code defines a class called `StreamDiffusionWrapper` that loads a model for image generation and manipulation. It prepares the model for inference by setting up the prompt, number of steps, and guidance scale. The code then preprocesses an image, passes it through the model for further processing, and saves the output to a file."}},{"key":"scip-python python temp indexer `examples.img2img.single`/__init__:","attributes":{"symbol":"scip-python python temp indexer `examples.img2img.single`/__init__:","range":[0,0,107,0],"content":"import os\nimport sys\nfrom typing import Literal, Dict, Optional\n\nimport fire\n\n\nsys.path.append(os.path.join(os.path.dirname(__file__), \"..\", \"..\"))\n\nfrom utils.wrapper import StreamDiffusionWrapper\n\nCURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n\n\ndef main(\n    input: str = os.path.join(CURRENT_DIR, \"..\", \"..\", \"images\", \"inputs\", \"input.png\"),\n    output: str = os.path.join(CURRENT_DIR, \"..\", \"..\", \"images\", \"outputs\", \"output.png\"),\n    model_id_or_path: str = \"KBlueLeaf/kohaku-v2.1\",\n    lora_dict: Optional[Dict[str, float]] = None,\n    prompt: str = \"1girl with brown dog hair, thick glasses, smiling\",\n    negative_prompt: str = \"low quality, bad quality, blurry, low resolution\",\n    width: int = 512,\n    height: int = 512,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"xformers\",\n    use_denoising_batch: bool = True,\n    guidance_scale: float = 1.2,\n    cfg_type: Literal[\"none\", \"full\", \"self\", \"initialize\"] = \"self\",\n    seed: int = 2,\n    delta: float = 0.5,\n):\n    \"\"\"\n    Initializes the StreamDiffusionWrapper.\n\n    Parameters\n    ----------\n    input : str, optional\n        The input image file to load images from.\n    output : str, optional\n        The output image file to save images to.\n    model_id_or_path : str\n        The model id or path to load.\n    lora_dict : Optional[Dict[str, float]], optional\n        The lora_dict to load, by default None.\n        Keys are the LoRA names and values are the LoRA scales.\n        Example: {\"LoRA_1\" : 0.5 , \"LoRA_2\" : 0.7 ,...}\n    prompt : str\n        The prompt to generate images from.\n    negative_prompt : str, optional\n        The negative prompt to use.\n    width : int, optional\n        The width of the image, by default 512.\n    height : int, optional\n        The height of the image, by default 512.\n    acceleration : Literal[\"none\", \"xformers\", \"tensorrt\"], optional\n        The acceleration method, by default \"tensorrt\".\n    use_denoising_batch : bool, optional\n        Whether to use denoising batch or not, by default True.\n    guidance_scale : float, optional\n        The CFG scale, by default 1.2.\n    cfg_type : Literal[\"none\", \"full\", \"self\", \"initialize\"],\n    optional\n        The cfg_type for img2img mode, by default \"self\".\n        You cannot use anything other than \"none\" for txt2img mode.\n    seed : int, optional\n        The seed, by default 2. if -1, use random seed.\n    delta : float, optional\n        The delta multiplier of virtual residual noise,\n        by default 1.0.\n    \"\"\"\n\n    if guidance_scale <= 1.0:\n        cfg_type = \"none\"\n\n    stream = StreamDiffusionWrapper(\n        model_id_or_path=model_id_or_path,\n        lora_dict=lora_dict,\n        t_index_list=[22, 32, 45],\n        frame_buffer_size=1,\n        width=width,\n        height=height,\n        warmup=10,\n        acceleration=acceleration,\n        mode=\"img2img\",\n        use_denoising_batch=use_denoising_batch,\n        cfg_type=cfg_type,\n        seed=seed,\n    )\n\n    stream.prepare(\n        prompt=prompt,\n        negative_prompt=negative_prompt,\n        num_inference_steps=50,\n        guidance_scale=guidance_scale,\n        delta=delta,\n    )\n\n    image_tensor = stream.preprocess_image(input)\n\n    for _ in range(stream.batch_size - 1):\n        stream(image=image_tensor)\n\n    output_image = stream(image=image_tensor)\n    output_image.save(output)\n\n\nif __name__ == \"__main__\":\n    fire.Fire(main)\n","file":"/examples/img2img/single.py","language":"python","fileHash":"47309c9265369f8cd803e703f72a7a1e153b1449cda74aa65aabb64bf9cdedf8","hash":"47309c9265369f8cd803e703f72a7a1e153b1449cda74aa65aabb64bf9cdedf8","processedContent":"import os #undefined\nimport sys #undefined\nfrom typing import Literal, Dict, Optional\n\"\"\"\nscip-python python python-stdlib 3.11 typing/__init__:: undefined\nscip-python python python-stdlib 3.11 typing/Literal.: undefined\nscip-python python python-stdlib 3.11 typing/Dict.: undefined\nscip-python python python-stdlib 3.11 typing/Optional.: undefined\n\"\"\"\n\nimport fire\n\n\nsys.path.append(os.path.join(os.path.dirname(__file__), \"..\", \"..\"))\n\"\"\"\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 sys/__init__:: undefined\nscip-python python python-stdlib 3.11 sys/path.path.: undefined\nscip-python python python-stdlib 3.11 os/path.: undefined\nscip-python python python-stdlib 3.11 os/path.: undefined\nscip-python python python-stdlib 3.11 ntpath/join().: undefined\nscip-python python python-stdlib 3.11 ntpath/join().: undefined\n\"\"\"\n\nfrom utils.wrapper import StreamDiffusionWrapper\n\"\"\"\nscip-python python temp indexer `utils.wrapper`/__init__:: This code defines a class called `StreamDiffusionWrapper` that loads a model for image generation and manipulation, enabling acceleration, and preparing the model for inference.\nscip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#: This code defines a class called `StreamDiffusionWrapper` that loads a model from a given path or ID, performs various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference. It also defines various methods for performing image generation and manipulation.\n\"\"\"\n\nCURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n\"\"\"\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 os/path.: undefined\nscip-python python python-stdlib 3.11 os/path.: undefined\nscip-python python python-stdlib 3.11 ntpath/join().: undefined\nscip-python python python-stdlib 3.11 ntpath/join().: undefined\nscip-python python temp indexer `examples.img2img.single`/: undefined\n\"\"\"\n\n\ndef main(\n    \"\"\"This code defines a class called `StreamDiffusionWrapper` that loads a model for image generation and manipulation. It prepares the model for inference by setting up the prompt, number of steps, and guidance scale. The code then preprocesses an image, passes it through the model for further processing, and saves the output to a file.\"\"\"\n    pass\n\n\nif __name__ == \"__main__\": #undefined\n    fire.Fire(main) #This code defines a class called `StreamDiffusionWrapper` that loads a model for image generation and manipulation. It prepares the model for inference by setting up the prompt, number of steps, and guidance scale. The code then preprocesses an image, passes it through the model for further processing, and saves the output to a file.\n","documentation":"This code defines a class called `StreamDiffusionWrapper` that loads a model for image generation and manipulation, enabling acceleration, and preparing the model for inference."}},{"key":"scip-python python temp indexer `examples.img2img.single`/","attributes":{"symbol":"scip-python python temp indexer `examples.img2img.single`/","language":"python"}},{"key":"scip-python python temp indexer `examples.img2img.single`/CURRENT_DIR.","attributes":{"symbol":"scip-python python temp indexer `examples.img2img.single`/CURRENT_DIR.","language":"python"}},{"key":"scip-python python temp indexer `examples.optimal-performance.multi`/update_image().","attributes":{"range":[22,0,41,26],"symbol":"scip-python python temp indexer `examples.optimal-performance.multi`/update_image().","content":"def update_image(image_data: Image.Image, labels: List[tk.Label]) -> None:\n    \"\"\"\n    Update the image displayed on a Tkinter label.\n\n    Parameters\n    ----------\n    image_data : Image.Image\n        The image to be displayed.\n    labels : List[tk.Label]\n        The list of labels where the image will be updated.\n    \"\"\"\n    global image_update_counter\n    label = labels[image_update_counter % len(labels)]\n    image_update_counter += 1\n\n    width = 320\n    height = 320\n    tk_image = ImageTk.PhotoImage(image_data.resize((width, height)), size=width)\n    label.configure(image=tk_image, width=width, height=height)\n    label.image = tk_image","file":"/examples/optimal-performance/multi.py","language":"python","fileHash":"3286a4b460b8972cbc6c8f1fdcbb460e87cc085a11d09ee45f4df66018addd86","hash":"c90af4e8b0bb2d0a182d2715d52a6b86f2e157f0c11f92950a0981dafbe85afe","processedContent":"def update_image(image_data: Image.Image, labels: List[tk.Label]) -> None:\n\"\"\"\nscip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\nscip-python python Pillow 10.0.0 `pil.image`/Image#: undefined\nscip-python python python-stdlib 3.11 typing/List.: undefined\nscip-python python python-stdlib 3.11 tkinter/Label#: undefined\n\"\"\"\n    \"\"\"\n    Update the image displayed on a Tkinter label.\n\n    Parameters\n    ----------\n    image_data : Image.Image\n        The image to be displayed.\n    labels : List[tk.Label]\n        The list of labels where the image will be updated.\n    \"\"\"\n    global image_update_counter #undefined\n    label = labels[image_update_counter % len(labels)] #undefined\n    image_update_counter += 1 #undefined\n\n    width = 320\n    height = 320\n    tk_image = ImageTk.PhotoImage(image_data.resize((width, height)), size=width)\n    \"\"\"\n    scip-python python Pillow 10.0.0 `PIL.ImageTk`/__init__:: undefined\n    scip-python python Pillow 10.0.0 `pil.imagetk`/PhotoImage#: undefined\n    scip-python python Pillow 10.0.0 `pil.image`/Image#resize().: undefined\n    \"\"\"\n    label.configure(image=tk_image, width=width, height=height) #undefined\n    label.image = tk_image","documentation":"This code updates an image displayed on a Tkinter label by resizing the image and configuring the label with the new image."}},{"key":"scip-python python temp indexer `examples.optimal-performance.multi`/image_generation_process().","attributes":{"range":[44,0,98,18],"symbol":"scip-python python temp indexer `examples.optimal-performance.multi`/image_generation_process().","content":"def image_generation_process(\n    queue: Queue,\n    fps_queue: Queue,\n    prompt: str,\n    model_id_or_path: str,\n    batch_size: int = 10,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"tensorrt\",\n) -> None:\n    \"\"\"\n    Process for generating images based on a prompt using a specified model.\n\n    Parameters\n    ----------\n    queue : Queue\n        The queue to put the generated images in.\n    fps_queue : Queue\n        The queue to put the calculated fps.\n    prompt : str\n        The prompt to generate images from.\n    model_id_or_path : str\n        The name of the model to use for image generation.\n    batch_size : int\n        The batch size to use for image generation.\n    acceleration : Literal[\"none\", \"xformers\", \"tensorrt\"]\n        The type of acceleration to use for image generation.\n    \"\"\"\n    stream = StreamDiffusionWrapper(\n        model_id_or_path=model_id_or_path,\n        t_index_list=[0],\n        frame_buffer_size=batch_size,\n        warmup=10,\n        acceleration=acceleration,\n        use_lcm_lora=False,\n        mode=\"txt2img\",\n        cfg_type=\"none\",\n        use_denoising_batch=True,\n    )\n\n    stream.prepare(\n        prompt=prompt,\n        num_inference_steps=50,\n    )\n\n    while True:\n        try:\n            start_time = time.time()\n\n            x_outputs = stream.stream.txt2img_sd_turbo(batch_size).cpu()\n            queue.put(x_outputs, block=False)\n\n            fps = 1 / (time.time() - start_time) * batch_size\n            fps_queue.put(fps)\n        except KeyboardInterrupt:\n            print(f\"fps: {fps}\")\n            return","file":"/examples/optimal-performance/multi.py","language":"python","fileHash":"3286a4b460b8972cbc6c8f1fdcbb460e87cc085a11d09ee45f4df66018addd86","hash":"df34c1b9e23d5fde136d4431963df8837f1ce549542414a125cf21073323790f","processedContent":"def image_generation_process(\n    queue: Queue,\n    fps_queue: Queue,\n    prompt: str,\n    model_id_or_path: str,\n    batch_size: int = 10,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"tensorrt\", #undefined\n) -> None:\n    \"\"\"\n    Process for generating images based on a prompt using a specified model.\n\n    Parameters\n    ----------\n    queue : Queue\n        The queue to put the generated images in.\n    fps_queue : Queue\n        The queue to put the calculated fps.\n    prompt : str\n        The prompt to generate images from.\n    model_id_or_path : str\n        The name of the model to use for image generation.\n    batch_size : int\n        The batch size to use for image generation.\n    acceleration : Literal[\"none\", \"xformers\", \"tensorrt\"]\n        The type of acceleration to use for image generation.\n    \"\"\"\n    stream = StreamDiffusionWrapper( #This code defines a class called `StreamDiffusionWrapper` that loads a model from a given path or ID, performs various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference. It also defines various methods for performing image generation and manipulation.\n        model_id_or_path=model_id_or_path,\n        t_index_list=[0],\n        frame_buffer_size=batch_size,\n        warmup=10,\n        acceleration=acceleration,\n        use_lcm_lora=False,\n        mode=\"txt2img\",\n        cfg_type=\"none\",\n        use_denoising_batch=True,\n    )\n\n    stream.prepare( #This code prepares the model for inference by setting up the prompt, number of steps, and guidance scale.\n        prompt=prompt,\n        num_inference_steps=50,\n    )\n\n    while True:\n        try:\n            start_time = time.time()\n            \"\"\"\n            scip-python python python-stdlib 3.11 time/__init__:: undefined\n            scip-python python python-stdlib 3.11 time/time().: undefined\n            \"\"\"\n\n            x_outputs = stream.stream.txt2img_sd_turbo(batch_size).cpu() #undefined\n            queue.put(x_outputs, block=False)\n\n            fps = 1 / (time.time() - start_time) * batch_size\n            \"\"\"\n            scip-python python python-stdlib 3.11 time/__init__:: undefined\n            scip-python python python-stdlib 3.11 time/time().: undefined\n            \"\"\"\n            fps_queue.put(fps)\n        except KeyboardInterrupt:\n            print(f\"fps: {fps}\")\n            return","documentation":"This code defines a function called `image_generation_process` that takes in a prompt and a model ID or path, and generates images based on the prompt using the specified model. The function uses a class called `StreamDiffusionWrapper` to perform various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference. It then generates images and calculates the frames per second (fps) for each generated image."}},{"key":"scip-python python temp indexer `examples.optimal-performance.multi`/_receive_images().","attributes":{"range":[101,0,132,18],"symbol":"scip-python python temp indexer `examples.optimal-performance.multi`/_receive_images().","content":"def _receive_images(\n    queue: Queue, fps_queue: Queue, labels: List[tk.Label], fps_label: tk.Label\n) -> None:\n    \"\"\"\n    Continuously receive images from a queue and update the labels.\n\n    Parameters\n    ----------\n    queue : Queue\n        The queue to receive images from.\n    fps_queue : Queue\n        The queue to put the calculated fps.\n    labels : List[tk.Label]\n        The list of labels to update with images.\n    fps_label : tk.Label\n        The label to show fps.\n    \"\"\"\n    while True:\n        try:\n            if not queue.empty():\n                [\n                    labels[0].after(0, update_image, image_data, labels)\n                    for image_data in postprocess_image(\n                        queue.get(block=False), output_type=\"pil\"\n                    )\n                ]\n            if not fps_queue.empty():\n                fps_label.config(text=f\"FPS: {fps_queue.get(block=False):.2f}\")\n\n            time.sleep(0.0005)\n        except KeyboardInterrupt:\n            return","file":"/examples/optimal-performance/multi.py","language":"python","fileHash":"3286a4b460b8972cbc6c8f1fdcbb460e87cc085a11d09ee45f4df66018addd86","hash":"2a287747092af17c829ef015a28aebd15734b122fa6a3061003112f11a81dd9b","processedContent":"def _receive_images(\n    queue: Queue, fps_queue: Queue, labels: List[tk.Label], fps_label: tk.Label\n    \"\"\"\n    scip-python python python-stdlib 3.11 typing/List.: undefined\n    scip-python python python-stdlib 3.11 tkinter/Label#: undefined\n    scip-python python python-stdlib 3.11 tkinter/Label#: undefined\n    \"\"\"\n) -> None:\n    \"\"\"\n    Continuously receive images from a queue and update the labels.\n\n    Parameters\n    ----------\n    queue : Queue\n        The queue to receive images from.\n    fps_queue : Queue\n        The queue to put the calculated fps.\n    labels : List[tk.Label]\n        The list of labels to update with images.\n    fps_label : tk.Label\n        The label to show fps.\n    \"\"\"\n    while True:\n        try:\n            if not queue.empty(): #undefined\n                [\n                    labels[0].after(0, update_image, image_data, labels)\n                    \"\"\"\n                    scip-python python python-stdlib 3.11 tkinter/Misc#after().: undefined\n                    scip-python python temp indexer `examples.optimal-performance.multi`/update_image().: This code updates an image displayed on a Tkinter label by resizing the image and configuring the label with the new image.\n                    \"\"\"\n                    for image_data in postprocess_image(\n                        queue.get(block=False), output_type=\"pil\"\n                    )\n                ]\n            if not fps_queue.empty(): #undefined\n                fps_label.config(text=f\"FPS: {fps_queue.get(block=False):.2f}\") #undefined\n\n            time.sleep(0.0005)\n            \"\"\"\n            scip-python python python-stdlib 3.11 time/__init__:: undefined\n            scip-python python python-stdlib 3.11 time/sleep().: undefined\n            \"\"\"\n        except KeyboardInterrupt:\n            return","documentation":"This code continuously receives images from a queue and updates the labels with the received images. It also calculates the frames per second (FPS) and displays it in a label."}},{"key":"scip-python python temp indexer `examples.optimal-performance.multi`/receive_images().","attributes":{"range":[135,0,164,14],"symbol":"scip-python python temp indexer `examples.optimal-performance.multi`/receive_images().","content":"def receive_images(queue: Queue, fps_queue: Queue) -> None:\n    \"\"\"\n    Setup the Tkinter window and start the thread to receive images.\n\n    Parameters\n    ----------\n    queue : Queue\n        The queue to receive images from.\n    fps_queue : Queue\n        The queue to put the calculated fps.\n    \"\"\"\n    root = tk.Tk()\n    root.title(\"Image Viewer\")\n    labels = [tk.Label(root) for _ in range(4)]\n    labels[0].grid(row=0, column=0)\n    labels[1].grid(row=0, column=1)\n    labels[2].grid(row=1, column=0)\n    labels[3].grid(row=1, column=1)\n    fps_label = tk.Label(root, text=\"FPS: 0\")\n    fps_label.grid(rows=2, columnspan=2)\n\n    thread = threading.Thread(\n        target=_receive_images, args=(queue, fps_queue, labels, fps_label), daemon=True\n    )\n    thread.start()\n\n    try:\n        root.mainloop()\n    except KeyboardInterrupt:\n        return","file":"/examples/optimal-performance/multi.py","language":"python","fileHash":"3286a4b460b8972cbc6c8f1fdcbb460e87cc085a11d09ee45f4df66018addd86","hash":"cfbaa90d90e7769990c94bd965a85ce9be4041a9b311db3b04d8d6e601800c68","processedContent":"def receive_images(queue: Queue, fps_queue: Queue) -> None:\n    \"\"\"\n    Setup the Tkinter window and start the thread to receive images.\n\n    Parameters\n    ----------\n    queue : Queue\n        The queue to receive images from.\n    fps_queue : Queue\n        The queue to put the calculated fps.\n    \"\"\"\n    root = tk.Tk() #undefined\n    root.title(\"Image Viewer\") #undefined\n    labels = [tk.Label(root) for _ in range(4)] #undefined\n    labels[0].grid(row=0, column=0) #undefined\n    labels[1].grid(row=0, column=1) #undefined\n    labels[2].grid(row=1, column=0) #undefined\n    labels[3].grid(row=1, column=1) #undefined\n    fps_label = tk.Label(root, text=\"FPS: 0\") #undefined\n    fps_label.grid(rows=2, columnspan=2) #undefined\n\n    thread = threading.Thread(\n    \"\"\"\n    scip-python python python-stdlib 3.11 threading/__init__:: undefined\n    scip-python python python-stdlib 3.11 threading/Thread#: undefined\n    \"\"\"\n        target=_receive_images, args=(queue, fps_queue, labels, fps_label), daemon=True #This code continuously receives images from a queue and updates the labels with the received images. It also calculates the frames per second (FPS) and displays it in a label.\n    )\n    thread.start() #undefined\n\n    try:\n        root.mainloop() #undefined\n    except KeyboardInterrupt:\n        return","documentation":"This code sets up a Tkinter window and starts a thread to continuously receive images from a queue and update labels with the received images, while also calculating and displaying the frames per second (FPS)."}},{"key":"scip-python python temp indexer `examples.optimal-performance.multi`/main().","attributes":{"range":[167,0,185,20],"symbol":"scip-python python temp indexer `examples.optimal-performance.multi`/main().","content":"def main(\n    prompt: str = \"cat with sunglasses and a hat, photoreal, 8K\",\n    model_id_or_path: str = \"stabilityai/sd-turbo\",\n    batch_size: int = 12,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"tensorrt\",\n) -> None:\n    \"\"\"\n    Main function to start the image generation and viewer processes.\n    \"\"\"\n    queue = Queue()\n    fps_queue = Queue()\n    process1 = Process(\n        target=image_generation_process,\n        args=(queue, fps_queue, prompt, model_id_or_path, batch_size, acceleration),\n    )\n    process1.start()\n\n    process2 = Process(target=receive_images, args=(queue, fps_queue))\n    process2.start()","file":"/examples/optimal-performance/multi.py","language":"python","fileHash":"3286a4b460b8972cbc6c8f1fdcbb460e87cc085a11d09ee45f4df66018addd86","hash":"6d3cc7c412607f2bde211bded2a8ed5cfc79e56239a983447c20f4edd114b3aa","processedContent":"def main(\n    prompt: str = \"cat with sunglasses and a hat, photoreal, 8K\",\n    model_id_or_path: str = \"stabilityai/sd-turbo\",\n    batch_size: int = 12,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"tensorrt\", #undefined\n) -> None:\n    \"\"\"\n    Main function to start the image generation and viewer processes.\n    \"\"\"\n    queue = Queue()\n    fps_queue = Queue()\n    process1 = Process(\n        target=image_generation_process, #This code defines a function called `image_generation_process` that takes in a prompt and a model ID or path, and generates images based on the prompt using the specified model. The function uses a class called `StreamDiffusionWrapper` to perform various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference. It then generates images and calculates the frames per second (fps) for each generated image.\n        args=(queue, fps_queue, prompt, model_id_or_path, batch_size, acceleration),\n    )\n    process1.start()\n\n    process2 = Process(target=receive_images, args=(queue, fps_queue)) #This code sets up a Tkinter window and starts a thread to continuously receive images from a queue and update labels with the received images, while also calculating and displaying the frames per second (FPS).\n    process2.start()","documentation":"The code defines a main function that starts two processes: one for generating images and another for receiving and displaying them in a Tkinter window."}},{"key":"scip-python python temp indexer `examples.optimal-performance.multi`/__init__:","attributes":{"symbol":"scip-python python temp indexer `examples.optimal-performance.multi`/__init__:","range":[0,0,190,0],"content":"import os\nimport sys\nimport threading\nimport time\nimport tkinter as tk\nfrom multiprocessing import Process, Queue\nfrom typing import List, Literal\n\nimport fire\nfrom PIL import Image, ImageTk\n\nfrom streamdiffusion.image_utils import postprocess_image\n\n\nsys.path.append(os.path.join(os.path.dirname(__file__), \"..\", \"..\"))\n\nfrom utils.wrapper import StreamDiffusionWrapper\n\n\nimage_update_counter = 0\n\n\ndef update_image(image_data: Image.Image, labels: List[tk.Label]) -> None:\n    \"\"\"\n    Update the image displayed on a Tkinter label.\n\n    Parameters\n    ----------\n    image_data : Image.Image\n        The image to be displayed.\n    labels : List[tk.Label]\n        The list of labels where the image will be updated.\n    \"\"\"\n    global image_update_counter\n    label = labels[image_update_counter % len(labels)]\n    image_update_counter += 1\n\n    width = 320\n    height = 320\n    tk_image = ImageTk.PhotoImage(image_data.resize((width, height)), size=width)\n    label.configure(image=tk_image, width=width, height=height)\n    label.image = tk_image  # keep a reference\n\n\ndef image_generation_process(\n    queue: Queue,\n    fps_queue: Queue,\n    prompt: str,\n    model_id_or_path: str,\n    batch_size: int = 10,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"tensorrt\",\n) -> None:\n    \"\"\"\n    Process for generating images based on a prompt using a specified model.\n\n    Parameters\n    ----------\n    queue : Queue\n        The queue to put the generated images in.\n    fps_queue : Queue\n        The queue to put the calculated fps.\n    prompt : str\n        The prompt to generate images from.\n    model_id_or_path : str\n        The name of the model to use for image generation.\n    batch_size : int\n        The batch size to use for image generation.\n    acceleration : Literal[\"none\", \"xformers\", \"tensorrt\"]\n        The type of acceleration to use for image generation.\n    \"\"\"\n    stream = StreamDiffusionWrapper(\n        model_id_or_path=model_id_or_path,\n        t_index_list=[0],\n        frame_buffer_size=batch_size,\n        warmup=10,\n        acceleration=acceleration,\n        use_lcm_lora=False,\n        mode=\"txt2img\",\n        cfg_type=\"none\",\n        use_denoising_batch=True,\n    )\n\n    stream.prepare(\n        prompt=prompt,\n        num_inference_steps=50,\n    )\n\n    while True:\n        try:\n            start_time = time.time()\n\n            x_outputs = stream.stream.txt2img_sd_turbo(batch_size).cpu()\n            queue.put(x_outputs, block=False)\n\n            fps = 1 / (time.time() - start_time) * batch_size\n            fps_queue.put(fps)\n        except KeyboardInterrupt:\n            print(f\"fps: {fps}\")\n            return\n\n\ndef _receive_images(\n    queue: Queue, fps_queue: Queue, labels: List[tk.Label], fps_label: tk.Label\n) -> None:\n    \"\"\"\n    Continuously receive images from a queue and update the labels.\n\n    Parameters\n    ----------\n    queue : Queue\n        The queue to receive images from.\n    fps_queue : Queue\n        The queue to put the calculated fps.\n    labels : List[tk.Label]\n        The list of labels to update with images.\n    fps_label : tk.Label\n        The label to show fps.\n    \"\"\"\n    while True:\n        try:\n            if not queue.empty():\n                [\n                    labels[0].after(0, update_image, image_data, labels)\n                    for image_data in postprocess_image(\n                        queue.get(block=False), output_type=\"pil\"\n                    )\n                ]\n            if not fps_queue.empty():\n                fps_label.config(text=f\"FPS: {fps_queue.get(block=False):.2f}\")\n\n            time.sleep(0.0005)\n        except KeyboardInterrupt:\n            return\n\n\ndef receive_images(queue: Queue, fps_queue: Queue) -> None:\n    \"\"\"\n    Setup the Tkinter window and start the thread to receive images.\n\n    Parameters\n    ----------\n    queue : Queue\n        The queue to receive images from.\n    fps_queue : Queue\n        The queue to put the calculated fps.\n    \"\"\"\n    root = tk.Tk()\n    root.title(\"Image Viewer\")\n    labels = [tk.Label(root) for _ in range(4)]\n    labels[0].grid(row=0, column=0)\n    labels[1].grid(row=0, column=1)\n    labels[2].grid(row=1, column=0)\n    labels[3].grid(row=1, column=1)\n    fps_label = tk.Label(root, text=\"FPS: 0\")\n    fps_label.grid(rows=2, columnspan=2)\n\n    thread = threading.Thread(\n        target=_receive_images, args=(queue, fps_queue, labels, fps_label), daemon=True\n    )\n    thread.start()\n\n    try:\n        root.mainloop()\n    except KeyboardInterrupt:\n        return\n\n\ndef main(\n    prompt: str = \"cat with sunglasses and a hat, photoreal, 8K\",\n    model_id_or_path: str = \"stabilityai/sd-turbo\",\n    batch_size: int = 12,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"tensorrt\",\n) -> None:\n    \"\"\"\n    Main function to start the image generation and viewer processes.\n    \"\"\"\n    queue = Queue()\n    fps_queue = Queue()\n    process1 = Process(\n        target=image_generation_process,\n        args=(queue, fps_queue, prompt, model_id_or_path, batch_size, acceleration),\n    )\n    process1.start()\n\n    process2 = Process(target=receive_images, args=(queue, fps_queue))\n    process2.start()\n\n\nif __name__ == \"__main__\":\n    fire.Fire(main)\n","file":"/examples/optimal-performance/multi.py","language":"python","fileHash":"3286a4b460b8972cbc6c8f1fdcbb460e87cc085a11d09ee45f4df66018addd86","hash":"3286a4b460b8972cbc6c8f1fdcbb460e87cc085a11d09ee45f4df66018addd86","processedContent":"import os #undefined\nimport sys #undefined\nimport threading #undefined\nimport time #undefined\nimport tkinter as tk #undefined\nfrom multiprocessing import Process, Queue #undefined\nfrom typing import List, Literal\n\"\"\"\nscip-python python python-stdlib 3.11 typing/__init__:: undefined\nscip-python python python-stdlib 3.11 typing/List.: undefined\nscip-python python python-stdlib 3.11 typing/Literal.: undefined\n\"\"\"\n\nimport fire\nfrom PIL import Image, ImageTk\n\"\"\"\nscip-python python Pillow 10.0.0 PIL/__init__:: undefined\nscip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\nscip-python python Pillow 10.0.0 `PIL.ImageTk`/__init__:: undefined\n\"\"\"\n\nfrom streamdiffusion.image_utils import postprocess_image #undefined\n\n\nsys.path.append(os.path.join(os.path.dirname(__file__), \"..\", \"..\"))\n\"\"\"\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 sys/__init__:: undefined\nscip-python python python-stdlib 3.11 sys/path.path.: undefined\nscip-python python python-stdlib 3.11 os/path.: undefined\nscip-python python python-stdlib 3.11 os/path.: undefined\nscip-python python python-stdlib 3.11 ntpath/join().: undefined\nscip-python python python-stdlib 3.11 ntpath/join().: undefined\n\"\"\"\n\nfrom utils.wrapper import StreamDiffusionWrapper\n\"\"\"\nscip-python python temp indexer `utils.wrapper`/__init__:: This code defines a class called `StreamDiffusionWrapper` that loads a model for image generation and manipulation, enabling acceleration, and preparing the model for inference.\nscip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#: This code defines a class called `StreamDiffusionWrapper` that loads a model from a given path or ID, performs various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference. It also defines various methods for performing image generation and manipulation.\n\"\"\"\n\n\nimage_update_counter = 0\n\n\ndef update_image(image_data: Image.Image, labels: List[tk.Label]) -> None:\n    \"\"\"This code updates an image displayed on a Tkinter label by resizing the image and configuring the label with the new image.\"\"\"\n    pass\n\n\ndef image_generation_process(\n    \"\"\"This code defines a function called `image_generation_process` that takes in a prompt and a model ID or path, and generates images based on the prompt using the specified model. The function uses a class called `StreamDiffusionWrapper` to perform various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference. It then generates images and calculates the frames per second (fps) for each generated image.\"\"\"\n    pass\n\n\ndef _receive_images(\n    \"\"\"This code continuously receives images from a queue and updates the labels with the received images. It also calculates the frames per second (FPS) and displays it in a label.\"\"\"\n    pass\n\n\ndef receive_images(queue: Queue, fps_queue: Queue) -> None:\n    \"\"\"This code sets up a Tkinter window and starts a thread to continuously receive images from a queue and update labels with the received images, while also calculating and displaying the frames per second (FPS).\"\"\"\n    pass\n\n\ndef main(\n    \"\"\"The code defines a main function that starts two processes: one for generating images and another for receiving and displaying them in a Tkinter window.\"\"\"\n    pass\n\n\nif __name__ == \"__main__\": #undefined\n    fire.Fire(main) #The code defines a main function that starts two processes: one for generating images and another for receiving and displaying them in a Tkinter window.\n","documentation":"The code is a Python script that uses the StreamDiffusion library to generate images based on prompts and display them in a Tkinter window. It also calculates the frames per second (FPS) for each generated image."}},{"key":"scip-python python python-stdlib 3.11 threading/__init__:","attributes":{"symbol":"scip-python python python-stdlib 3.11 threading/__init__:","language":"python"}},{"key":"scip-python python python-stdlib 3.11 tkinter/__init__:","attributes":{"symbol":"scip-python python python-stdlib 3.11 tkinter/__init__:","language":"python"}},{"key":"scip-python python Pillow 10.0.0 `PIL.ImageTk`/__init__:","attributes":{"symbol":"scip-python python Pillow 10.0.0 `PIL.ImageTk`/__init__:","language":"python"}},{"key":"scip-python python python-stdlib 3.11 tkinter/Label#","attributes":{"symbol":"scip-python python python-stdlib 3.11 tkinter/Label#","language":"python"}},{"key":"scip-python python temp indexer `examples.optimal-performance.multi`/image_update_counter.","attributes":{"symbol":"scip-python python temp indexer `examples.optimal-performance.multi`/image_update_counter.","language":"python"}},{"key":"scip-python python Pillow 10.0.0 `pil.imagetk`/PhotoImage#","attributes":{"symbol":"scip-python python Pillow 10.0.0 `pil.imagetk`/PhotoImage#","language":"python"}},{"key":"scip-python python python-stdlib 3.11 tkinter/Label#configure().","attributes":{"symbol":"scip-python python python-stdlib 3.11 tkinter/Label#configure().","language":"python"}},{"key":"scip-python python python-stdlib 3.11 time/time().","attributes":{"symbol":"scip-python python python-stdlib 3.11 time/time().","language":"python"}},{"key":"scip-python python python-stdlib 3.11 tkinter/Misc#after().","attributes":{"symbol":"scip-python python python-stdlib 3.11 tkinter/Misc#after().","language":"python"}},{"key":"scip-python python python-stdlib 3.11 tkinter/Label#config.","attributes":{"symbol":"scip-python python python-stdlib 3.11 tkinter/Label#config.","language":"python"}},{"key":"scip-python python python-stdlib 3.11 tkinter/Tk#","attributes":{"symbol":"scip-python python python-stdlib 3.11 tkinter/Tk#","language":"python"}},{"key":"scip-python python python-stdlib 3.11 tkinter/Wm#title.","attributes":{"symbol":"scip-python python python-stdlib 3.11 tkinter/Wm#title.","language":"python"}},{"key":"scip-python python python-stdlib 3.11 tkinter/Grid#grid.","attributes":{"symbol":"scip-python python python-stdlib 3.11 tkinter/Grid#grid.","language":"python"}},{"key":"scip-python python python-stdlib 3.11 threading/Thread#","attributes":{"symbol":"scip-python python python-stdlib 3.11 threading/Thread#","language":"python"}},{"key":"scip-python python python-stdlib 3.11 threading/Thread#start().","attributes":{"symbol":"scip-python python python-stdlib 3.11 threading/Thread#start().","language":"python"}},{"key":"scip-python python python-stdlib 3.11 tkinter/Misc#mainloop().","attributes":{"symbol":"scip-python python python-stdlib 3.11 tkinter/Misc#mainloop().","language":"python"}},{"key":"scip-python python temp indexer `examples.optimal-performance.multi`/","attributes":{"symbol":"scip-python python temp indexer `examples.optimal-performance.multi`/","language":"python"}},{"key":"scip-python python temp indexer `examples.optimal-performance.single`/image_generation_process().","attributes":{"range":[13,0,64,18],"symbol":"scip-python python temp indexer `examples.optimal-performance.single`/image_generation_process().","content":"def image_generation_process(\n    queue: Queue,\n    fps_queue: Queue,\n    prompt: str,\n    model_id_or_path: str,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"tensorrt\",\n) -> None:\n    \"\"\"\n    Process for generating images based on a prompt using a specified model.\n\n    Parameters\n    ----------\n    queue : Queue\n        The queue to put the generated images in.\n    fps_queue : Queue\n        The queue to put the calculated fps.\n    prompt : str\n        The prompt to generate images from.\n    model_id_or_path : str\n        The name of the model to use for image generation.\n    acceleration : Literal[\"none\", \"xformers\", \"tensorrt\"]\n        The type of acceleration to use for image generation.\n    \"\"\"\n    stream = StreamDiffusionWrapper(\n        model_id_or_path=model_id_or_path,\n        t_index_list=[0],\n        frame_buffer_size=1,\n        warmup=10,\n        acceleration=acceleration,\n        use_lcm_lora=False,\n        mode=\"txt2img\",\n        cfg_type=\"none\",\n        use_denoising_batch=True,\n    )\n\n    stream.prepare(\n        prompt=prompt,\n        num_inference_steps=50,\n    )\n\n    while True:\n        try:\n            start_time = time.time()\n\n            x_outputs = stream.stream.txt2img_sd_turbo(1).cpu()\n            queue.put(x_outputs, block=False)\n\n            fps = 1 / (time.time() - start_time)\n            fps_queue.put(fps)\n        except KeyboardInterrupt:\n            print(f\"fps: {fps}\")\n            return","file":"/examples/optimal-performance/single.py","language":"python","fileHash":"e15090e8b22732b6363f898f9c1b7a5a363ed5f673a6fd3c65db465e3050a64b","hash":"395f8749e4599ebd38d6deddf72321a23c7512002f5fb06fc54f384613b340ea","processedContent":"def image_generation_process(\n    queue: Queue,\n    fps_queue: Queue,\n    prompt: str,\n    model_id_or_path: str,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"tensorrt\", #undefined\n) -> None:\n    \"\"\"\n    Process for generating images based on a prompt using a specified model.\n\n    Parameters\n    ----------\n    queue : Queue\n        The queue to put the generated images in.\n    fps_queue : Queue\n        The queue to put the calculated fps.\n    prompt : str\n        The prompt to generate images from.\n    model_id_or_path : str\n        The name of the model to use for image generation.\n    acceleration : Literal[\"none\", \"xformers\", \"tensorrt\"]\n        The type of acceleration to use for image generation.\n    \"\"\"\n    stream = StreamDiffusionWrapper( #This code defines a class called `StreamDiffusionWrapper` that loads a model from a given path or ID, performs various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference. It also defines various methods for performing image generation and manipulation.\n        model_id_or_path=model_id_or_path,\n        t_index_list=[0],\n        frame_buffer_size=1,\n        warmup=10,\n        acceleration=acceleration,\n        use_lcm_lora=False,\n        mode=\"txt2img\",\n        cfg_type=\"none\",\n        use_denoising_batch=True,\n    )\n\n    stream.prepare( #This code prepares the model for inference by setting up the prompt, number of steps, and guidance scale.\n        prompt=prompt,\n        num_inference_steps=50,\n    )\n\n    while True:\n        try:\n            start_time = time.time()\n            \"\"\"\n            scip-python python python-stdlib 3.11 time/__init__:: undefined\n            scip-python python python-stdlib 3.11 time/time().: undefined\n            \"\"\"\n\n            x_outputs = stream.stream.txt2img_sd_turbo(1).cpu() #undefined\n            queue.put(x_outputs, block=False)\n\n            fps = 1 / (time.time() - start_time)\n            \"\"\"\n            scip-python python python-stdlib 3.11 time/__init__:: undefined\n            scip-python python python-stdlib 3.11 time/time().: undefined\n            \"\"\"\n            fps_queue.put(fps)\n        except KeyboardInterrupt:\n            print(f\"fps: {fps}\")\n            return","documentation":"This code defines a function called `image_generation_process` that takes in a prompt, model ID or path, and acceleration type as input. It then uses a class called `StreamDiffusionWrapper` to perform image generation based on the prompt and model, and puts the generated images into a queue. The function also calculates the frames per second (fps) and puts it into another queue."}},{"key":"scip-python python temp indexer `examples.optimal-performance.single`/main().","attributes":{"range":[66,0,83,20],"symbol":"scip-python python temp indexer `examples.optimal-performance.single`/main().","content":"def main(\n    prompt: str = \"cat with sunglasses and a hat, photoreal, 8K\",\n    model_id_or_path: str = \"stabilityai/sd-turbo\",\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"tensorrt\",\n) -> None:\n    \"\"\"\n    Main function to start the image generation and viewer processes.\n    \"\"\"\n    queue = Queue()\n    fps_queue = Queue()\n    process1 = Process(\n        target=image_generation_process,\n        args=(queue, fps_queue, prompt, model_id_or_path, acceleration),\n    )\n    process1.start()\n\n    process2 = Process(target=receive_images, args=(queue, fps_queue))\n    process2.start()","file":"/examples/optimal-performance/single.py","language":"python","fileHash":"e15090e8b22732b6363f898f9c1b7a5a363ed5f673a6fd3c65db465e3050a64b","hash":"9053431f5c97b02e15fdf51c648e65eb59a6dcc17d56971f6d5e8aabae387190","processedContent":"def main(\n    prompt: str = \"cat with sunglasses and a hat, photoreal, 8K\",\n    model_id_or_path: str = \"stabilityai/sd-turbo\",\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"tensorrt\", #undefined\n) -> None:\n    \"\"\"\n    Main function to start the image generation and viewer processes.\n    \"\"\"\n    queue = Queue()\n    fps_queue = Queue()\n    process1 = Process(\n        target=image_generation_process, #This code defines a function called `image_generation_process` that takes in a prompt, model ID or path, and acceleration type as input. It then uses a class called `StreamDiffusionWrapper` to perform image generation based on the prompt and model, and puts the generated images into a queue. The function also calculates the frames per second (fps) and puts it into another queue.\n        args=(queue, fps_queue, prompt, model_id_or_path, acceleration),\n    )\n    process1.start()\n\n    process2 = Process(target=receive_images, args=(queue, fps_queue)) #This code sets up a Tkinter window to display images and calculates the frames per second (FPS) of the received images. It also defines a function called `on_closing` that is triggered when the user closes the window, effectively closing the application.\n    process2.start()","documentation":"The code defines a main function that starts two processes: one for image generation and another for displaying the generated images in a Tkinter window. The first process generates images based on a prompt, model ID or path, and acceleration type, and puts them into a queue. The second process receives the images from the queue, calculates the frames per second (FPS), and displays them in the Tkinter window."}},{"key":"scip-python python temp indexer `examples.optimal-performance.single`/__init__:","attributes":{"symbol":"scip-python python temp indexer `examples.optimal-performance.single`/__init__:","range":[0,0,88,0],"content":"import os\nimport sys\nimport time\nfrom multiprocessing import Process, Queue\nfrom typing import Literal\n\nimport fire\n\nsys.path.append(os.path.join(os.path.dirname(__file__), \"..\", \"..\"))\n\nfrom utils.viewer import receive_images\nfrom utils.wrapper import StreamDiffusionWrapper\n\ndef image_generation_process(\n    queue: Queue,\n    fps_queue: Queue,\n    prompt: str,\n    model_id_or_path: str,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"tensorrt\",\n) -> None:\n    \"\"\"\n    Process for generating images based on a prompt using a specified model.\n\n    Parameters\n    ----------\n    queue : Queue\n        The queue to put the generated images in.\n    fps_queue : Queue\n        The queue to put the calculated fps.\n    prompt : str\n        The prompt to generate images from.\n    model_id_or_path : str\n        The name of the model to use for image generation.\n    acceleration : Literal[\"none\", \"xformers\", \"tensorrt\"]\n        The type of acceleration to use for image generation.\n    \"\"\"\n    stream = StreamDiffusionWrapper(\n        model_id_or_path=model_id_or_path,\n        t_index_list=[0],\n        frame_buffer_size=1,\n        warmup=10,\n        acceleration=acceleration,\n        use_lcm_lora=False,\n        mode=\"txt2img\",\n        cfg_type=\"none\",\n        use_denoising_batch=True,\n    )\n\n    stream.prepare(\n        prompt=prompt,\n        num_inference_steps=50,\n    )\n\n    while True:\n        try:\n            start_time = time.time()\n\n            x_outputs = stream.stream.txt2img_sd_turbo(1).cpu()\n            queue.put(x_outputs, block=False)\n\n            fps = 1 / (time.time() - start_time)\n            fps_queue.put(fps)\n        except KeyboardInterrupt:\n            print(f\"fps: {fps}\")\n            return\n\ndef main(\n    prompt: str = \"cat with sunglasses and a hat, photoreal, 8K\",\n    model_id_or_path: str = \"stabilityai/sd-turbo\",\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"tensorrt\",\n) -> None:\n    \"\"\"\n    Main function to start the image generation and viewer processes.\n    \"\"\"\n    queue = Queue()\n    fps_queue = Queue()\n    process1 = Process(\n        target=image_generation_process,\n        args=(queue, fps_queue, prompt, model_id_or_path, acceleration),\n    )\n    process1.start()\n\n    process2 = Process(target=receive_images, args=(queue, fps_queue))\n    process2.start()\n\n\nif __name__ == \"__main__\":\n    fire.Fire(main)\n","file":"/examples/optimal-performance/single.py","language":"python","fileHash":"e15090e8b22732b6363f898f9c1b7a5a363ed5f673a6fd3c65db465e3050a64b","hash":"e15090e8b22732b6363f898f9c1b7a5a363ed5f673a6fd3c65db465e3050a64b","processedContent":"import os #undefined\nimport sys #undefined\nimport time #undefined\nfrom multiprocessing import Process, Queue #undefined\nfrom typing import Literal\n\"\"\"\nscip-python python python-stdlib 3.11 typing/__init__:: undefined\nscip-python python python-stdlib 3.11 typing/Literal.: undefined\n\"\"\"\n\nimport fire\n\nsys.path.append(os.path.join(os.path.dirname(__file__), \"..\", \"..\"))\n\"\"\"\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 sys/__init__:: undefined\nscip-python python python-stdlib 3.11 sys/path.path.: undefined\nscip-python python python-stdlib 3.11 os/path.: undefined\nscip-python python python-stdlib 3.11 os/path.: undefined\nscip-python python python-stdlib 3.11 ntpath/join().: undefined\nscip-python python python-stdlib 3.11 ntpath/join().: undefined\n\"\"\"\n\nfrom utils.viewer import receive_images\n\"\"\"\nscip-python python temp indexer `utils.viewer`/__init__:: The code is a Python script that creates a Tkinter window to display images and calculates the frames per second (FPS) of the received images. It also defines a function called `on_closing` that is triggered when the user closes the window, effectively closing the application.\nscip-python python temp indexer `utils.viewer`/receive_images().: This code sets up a Tkinter window to display images and calculates the frames per second (FPS) of the received images. It also defines a function called `on_closing` that is triggered when the user closes the window, effectively closing the application.\n\"\"\"\nfrom utils.wrapper import StreamDiffusionWrapper\n\"\"\"\nscip-python python temp indexer `utils.wrapper`/__init__:: This code defines a class called `StreamDiffusionWrapper` that loads a model for image generation and manipulation, enabling acceleration, and preparing the model for inference.\nscip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#: This code defines a class called `StreamDiffusionWrapper` that loads a model from a given path or ID, performs various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference. It also defines various methods for performing image generation and manipulation.\n\"\"\"\n\ndef image_generation_process(\n    \"\"\"This code defines a function called `image_generation_process` that takes in a prompt, model ID or path, and acceleration type as input. It then uses a class called `StreamDiffusionWrapper` to perform image generation based on the prompt and model, and puts the generated images into a queue. The function also calculates the frames per second (fps) and puts it into another queue.\"\"\"\n    pass\n\ndef main(\n    \"\"\"The code defines a main function that starts two processes: one for image generation and another for displaying the generated images in a Tkinter window. The first process generates images based on a prompt, model ID or path, and acceleration type, and puts them into a queue. The second process receives the images from the queue, calculates the frames per second (FPS), and displays them in the Tkinter window.\"\"\"\n    pass\n\n\nif __name__ == \"__main__\": #undefined\n    fire.Fire(main) #The code defines a main function that starts two processes: one for image generation and another for displaying the generated images in a Tkinter window. The first process generates images based on a prompt, model ID or path, and acceleration type, and puts them into a queue. The second process receives the images from the queue, calculates the frames per second (FPS), and displays them in the Tkinter window.\n","documentation":"The code is a Python script that uses a Tkinter window to display images generated by a model for image generation and manipulation. It also calculates the frames per second (FPS) of the received images."}},{"key":"scip-python python temp indexer `utils.viewer`/__init__:","attributes":{"symbol":"scip-python python temp indexer `utils.viewer`/__init__:","language":"python","range":[0,0,98,0],"content":"import os\nimport sys\nimport threading\nimport time\nimport tkinter as tk\nfrom multiprocessing import  Queue\nfrom typing import List\nfrom PIL import Image, ImageTk\nfrom streamdiffusion.image_utils import postprocess_image\n\nsys.path.append(os.path.join(os.path.dirname(__file__), \"..\", \"..\"))\n\n\ndef update_image(image_data: Image.Image, label: tk.Label) -> None:\n    \"\"\"\n    Update the image displayed on a Tkinter label.\n\n    Parameters\n    ----------\n    image_data : Image.Image\n        The image to be displayed.\n    label : tk.Label\n        The labels where the image will be updated.\n    \"\"\"\n    width = 512\n    height = 512\n    tk_image = ImageTk.PhotoImage(image_data, size=width)\n    label.configure(image=tk_image, width=width, height=height)\n    label.image = tk_image  # keep a reference\n\ndef _receive_images(\n    queue: Queue, fps_queue: Queue, label: tk.Label, fps_label: tk.Label\n) -> None:\n    \"\"\"\n    Continuously receive images from a queue and update the labels.\n\n    Parameters\n    ----------\n    queue : Queue\n        The queue to receive images from.\n    fps_queue : Queue\n        The queue to put the calculated fps.\n    label : tk.Label\n        The label to update with images.\n    fps_label : tk.Label\n        The label to show fps.\n    \"\"\"\n    while True:\n        try:\n            if not queue.empty():\n                label.after(\n                    0,\n                    update_image,\n                    postprocess_image(queue.get(block=False), output_type=\"pil\")[0],\n                    label,\n                )\n            if not fps_queue.empty():\n                fps_label.config(text=f\"FPS: {fps_queue.get(block=False):.2f}\")\n\n            time.sleep(0.0005)\n        except KeyboardInterrupt:\n            return\n\n\ndef receive_images(queue: Queue, fps_queue: Queue) -> None:\n    \"\"\"\n    Setup the Tkinter window and start the thread to receive images.\n\n    Parameters\n    ----------\n    queue : Queue\n        The queue to receive images from.\n    fps_queue : Queue\n        The queue to put the calculated fps.\n    \"\"\"\n    root = tk.Tk()\n    root.title(\"Image Viewer\")\n    label = tk.Label(root)\n    fps_label = tk.Label(root, text=\"FPS: 0\")\n    label.grid(column=0)\n    fps_label.grid(column=1)\n\n    def on_closing():\n        print(\"window closed\")\n        root.quit()  # stop event loop\n        return\n\n    thread = threading.Thread(\n        target=_receive_images, args=(queue, fps_queue, label, fps_label), daemon=True\n    )\n    thread.start()\n\n    try:\n        root.protocol(\"WM_DELETE_WINDOW\", on_closing)\n        root.mainloop()\n    except KeyboardInterrupt:\n        return\n\n","file":"/utils/viewer.py","fileHash":"52fc96aae078917f44a38142f6c441efbc4f0a1da0fd6d812edcb0e01602eec5","hash":"52fc96aae078917f44a38142f6c441efbc4f0a1da0fd6d812edcb0e01602eec5","processedContent":"import os #undefined\nimport sys #undefined\nimport threading #undefined\nimport time #undefined\nimport tkinter as tk #undefined\nfrom multiprocessing import  Queue #undefined\nfrom typing import List\n\"\"\"\nscip-python python python-stdlib 3.11 typing/__init__:: undefined\nscip-python python python-stdlib 3.11 typing/List.: undefined\n\"\"\"\nfrom PIL import Image, ImageTk\n\"\"\"\nscip-python python Pillow 10.0.0 PIL/__init__:: undefined\nscip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\nscip-python python Pillow 10.0.0 `PIL.ImageTk`/__init__:: undefined\n\"\"\"\nfrom streamdiffusion.image_utils import postprocess_image #undefined\n\nsys.path.append(os.path.join(os.path.dirname(__file__), \"..\", \"..\"))\n\"\"\"\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 sys/__init__:: undefined\nscip-python python python-stdlib 3.11 sys/path.path.: undefined\nscip-python python python-stdlib 3.11 os/path.: undefined\nscip-python python python-stdlib 3.11 os/path.: undefined\nscip-python python python-stdlib 3.11 ntpath/join().: undefined\nscip-python python python-stdlib 3.11 ntpath/join().: undefined\n\"\"\"\n\n\ndef update_image(image_data: Image.Image, label: tk.Label) -> None:\n    \"\"\"This code updates an image displayed on a Tkinter label by creating a new PhotoImage object from the input image data and configuring the label to display it.\"\"\"\n    pass\n\ndef _receive_images(\n    \"\"\"This code continuously receives images from a queue and updates a label with the received images. It also calculates the frames per second (FPS) and updates another label with the calculated FPS.\"\"\"\n    pass\n\n\ndef receive_images(queue: Queue, fps_queue: Queue) -> None:\n    \"\"\"This code sets up a Tkinter window to display images and calculates the frames per second (FPS) of the received images. It also defines a function called `on_closing` that is triggered when the user closes the window, effectively closing the application.\"\"\"\n    pass\n\n","documentation":"The code is a Python script that creates a Tkinter window to display images and calculates the frames per second (FPS) of the received images. It also defines a function called `on_closing` that is triggered when the user closes the window, effectively closing the application."}},{"key":"scip-python python temp indexer `utils.viewer`/receive_images().","attributes":{"symbol":"scip-python python temp indexer `utils.viewer`/receive_images().","language":"python","range":[64,0,96,14],"content":"def receive_images(queue: Queue, fps_queue: Queue) -> None:\n    \"\"\"\n    Setup the Tkinter window and start the thread to receive images.\n\n    Parameters\n    ----------\n    queue : Queue\n        The queue to receive images from.\n    fps_queue : Queue\n        The queue to put the calculated fps.\n    \"\"\"\n    root = tk.Tk()\n    root.title(\"Image Viewer\")\n    label = tk.Label(root)\n    fps_label = tk.Label(root, text=\"FPS: 0\")\n    label.grid(column=0)\n    fps_label.grid(column=1)\n\n    def on_closing():\n        print(\"window closed\")\n        root.quit()  # stop event loop\n        return\n\n    thread = threading.Thread(\n        target=_receive_images, args=(queue, fps_queue, label, fps_label), daemon=True\n    )\n    thread.start()\n\n    try:\n        root.protocol(\"WM_DELETE_WINDOW\", on_closing)\n        root.mainloop()\n    except KeyboardInterrupt:\n        return","file":"/utils/viewer.py","fileHash":"52fc96aae078917f44a38142f6c441efbc4f0a1da0fd6d812edcb0e01602eec5","hash":"a4f81c489084a81d0212ba72998ebe55e32f6b92edba382866811a8507f135a5","processedContent":"def receive_images(queue: Queue, fps_queue: Queue) -> None:\n    \"\"\"\n    Setup the Tkinter window and start the thread to receive images.\n\n    Parameters\n    ----------\n    queue : Queue\n        The queue to receive images from.\n    fps_queue : Queue\n        The queue to put the calculated fps.\n    \"\"\"\n    root = tk.Tk() #undefined\n    root.title(\"Image Viewer\") #undefined\n    label = tk.Label(root) #undefined\n    fps_label = tk.Label(root, text=\"FPS: 0\") #undefined\n    label.grid(column=0) #undefined\n    fps_label.grid(column=1) #undefined\n\n    def on_closing():\n        \"\"\"This code defines a function called `on_closing` that is triggered when the user closes the window. It prints a message to the console and stops the event loop, effectively closing the application.\"\"\"\n        pass\n\n    thread = threading.Thread(\n    \"\"\"\n    scip-python python python-stdlib 3.11 threading/__init__:: undefined\n    scip-python python python-stdlib 3.11 threading/Thread#: undefined\n    \"\"\"\n        target=_receive_images, args=(queue, fps_queue, label, fps_label), daemon=True #This code continuously receives images from a queue and updates a label with the received images. It also calculates the frames per second (FPS) and updates another label with the calculated FPS.\n    )\n    thread.start() #undefined\n\n    try:\n        root.protocol(\"WM_DELETE_WINDOW\", on_closing) #undefined\n        root.mainloop() #undefined\n    except KeyboardInterrupt:\n        return","documentation":"This code sets up a Tkinter window to display images and calculates the frames per second (FPS) of the received images. It also defines a function called `on_closing` that is triggered when the user closes the window, effectively closing the application."}},{"key":"scip-python python temp indexer `examples.optimal-performance.single`/","attributes":{"symbol":"scip-python python temp indexer `examples.optimal-performance.single`/","language":"python"}},{"key":"scip-python python temp indexer `examples.screen.main`/screen().","attributes":{"range":[23,0,37,22],"symbol":"scip-python python temp indexer `examples.screen.main`/screen().","content":"def screen(\n    height: int = 512,\n    width: int = 512,\n    monitor: Dict[str, int] = {\"top\": 300, \"left\": 200, \"width\": 512, \"height\": 512},\n):\n    global inputs\n    global stop_capture\n    with mss.mss() as sct:\n        while True:\n            img = sct.grab(monitor)\n            img = PIL.Image.frombytes(\"RGB\", img.size, img.bgra, \"raw\", \"BGRX\")\n            img.resize((height, width))\n            inputs.append(pil2tensor(img))\n            if stop_capture:\n                return","file":"/examples/screen/main.py","language":"python","fileHash":"a64a104a414d96b3d8bbd68e2b29122a23a279ef0a8a2731cd32a2086841a1e2","hash":"3230c8607550c9eac88f23800c21224694c68baaf8842c19aa504beb50963626","processedContent":"def screen(\n    height: int = 512,\n    width: int = 512,\n    monitor: Dict[str, int] = {\"top\": 300, \"left\": 200, \"width\": 512, \"height\": 512}, #undefined\n):\n    global inputs #undefined\n    global stop_capture #undefined\n    with mss.mss() as sct:\n        while True:\n            img = sct.grab(monitor)\n            img = PIL.Image.frombytes(\"RGB\", img.size, img.bgra, \"raw\", \"BGRX\")\n            \"\"\"\n            scip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\n            scip-python python Pillow 10.0.0 `pil.image`/frombytes().: undefined\n            \"\"\"\n            img.resize((height, width)) #undefined\n            inputs.append(pil2tensor(img)) #undefined\n            if stop_capture: #undefined\n                return","documentation":"This code defines a function called \"screen\" that captures screenshots of a monitor and appends them to a list called \"inputs\". The function takes two arguments: \"height\" and \"width\", which specify the size of the screenshot, and \"monitor\", which is a dictionary containing information about the monitor. The function uses the \"mss\" library to capture the screenshot and the \"PIL\" library to resize it."}},{"key":"scip-python python temp indexer `examples.screen.main`/dummy_screen().","attributes":{"range":[39,0,58,71],"symbol":"scip-python python temp indexer `examples.screen.main`/dummy_screen().","content":"def dummy_screen(\n        width: int,\n        height: int,\n):\n    root = tk.Tk()\n    root.title(\"Press Enter to start\")\n    root.geometry(f\"{width}x{height}\")\n    root.resizable(False, False)\n    root.attributes(\"-alpha\", 0.8)\n    root.configure(bg=\"black\")\n    def destroy(event):\n        root.destroy()\n    root.bind(\"<Return>\", destroy)\n    def update_geometry(event):\n        global top, left\n        top = root.winfo_y()\n        left = root.winfo_x()\n    root.bind(\"<Configure>\", update_geometry)\n    root.mainloop()\n    return {\"top\": top, \"left\": left, \"width\": width, \"height\": height}","file":"/examples/screen/main.py","language":"python","fileHash":"a64a104a414d96b3d8bbd68e2b29122a23a279ef0a8a2731cd32a2086841a1e2","hash":"ed65f7122e7faec330e6c3f95202d58f84e1dff6e693c13e89af00fd7220980d","processedContent":"def dummy_screen(\n        width: int,\n        height: int,\n):\n    root = tk.Tk() #undefined\n    root.title(\"Press Enter to start\") #undefined\n    root.geometry(f\"{width}x{height}\") #undefined\n    root.resizable(False, False) #undefined\n    root.attributes(\"-alpha\", 0.8) #undefined\n    root.configure(bg=\"black\") #undefined\n    def destroy(event):\n        \"\"\"This code is a function that destroys the root window when an event occurs.\"\"\"\n        pass\n    root.bind(\"<Return>\", destroy) #undefined\n    def update_geometry(event):\n        \"\"\"This code defines a function called `update_geometry` that updates the position of a window on the screen based on the current mouse position. It does this by getting the current position of the mouse using the `winfo_y()` and `winfo_x()` methods, and then setting the top and left properties of the window to those values.\"\"\"\n        pass\n    root.bind(\"<Configure>\", update_geometry) #undefined\n    root.mainloop() #undefined\n    return {\"top\": top, \"left\": left, \"width\": width, \"height\": height}","documentation":"This code defines a function called `dummy_screen` that creates a window with a title and geometry based on the input parameters. It also binds two functions to events, one to destroy the window when the return key is pressed and another to update the window's position based on the mouse position."}},{"key":"scip-python python temp indexer `examples.screen.main`/dummy_screen().destroy().","attributes":{"range":[49,4,50,22],"symbol":"scip-python python temp indexer `examples.screen.main`/dummy_screen().destroy().","content":"def destroy(event):\n        root.destroy()","file":"/examples/screen/main.py","language":"python","fileHash":"a64a104a414d96b3d8bbd68e2b29122a23a279ef0a8a2731cd32a2086841a1e2","hash":"21073ce867c0548627992076296952425c77992da87824d104df112eee003915","processedContent":"def destroy(event):\n        root.destroy()","documentation":"This code is a function that destroys the root window when an event occurs."}},{"key":"scip-python python temp indexer `examples.screen.main`/dummy_screen().update_geometry().","attributes":{"range":[52,4,55,29],"symbol":"scip-python python temp indexer `examples.screen.main`/dummy_screen().update_geometry().","content":"def update_geometry(event):\n        global top, left\n        top = root.winfo_y()\n        left = root.winfo_x()","file":"/examples/screen/main.py","language":"python","fileHash":"a64a104a414d96b3d8bbd68e2b29122a23a279ef0a8a2731cd32a2086841a1e2","hash":"1261f25b789b1b721496eb0da1815f077617cefe9d861e25e3585b3ff96f0fc2","processedContent":"def update_geometry(event):\n        global top, left\n        \"\"\"\n        scip-python python temp indexer `examples.screen.main`/top.: undefined\n        scip-python python temp indexer `examples.screen.main`/left.: undefined\n        \"\"\"\n        top = root.winfo_y()\n        \"\"\"\n        scip-python python temp indexer `examples.screen.main`/top.: undefined\n        scip-python python python-stdlib 3.11 tkinter/Misc#winfo_y().: undefined\n        \"\"\"\n        left = root.winfo_x()","documentation":"This code defines a function called `update_geometry` that updates the position of a window on the screen based on the current mouse position. It does this by getting the current position of the mouse using the `winfo_y()` and `winfo_x()` methods, and then setting the top and left properties of the window to those values."}},{"key":"scip-python python temp indexer `examples.screen.main`/image_generation_process().","attributes":{"range":[60,0,192,18],"symbol":"scip-python python temp indexer `examples.screen.main`/image_generation_process().","content":"def image_generation_process(\n    queue: Queue,\n    fps_queue: Queue,\n    model_id_or_path: str,\n    lora_dict: Optional[Dict[str, float]],\n    prompt: str,\n    negative_prompt: str,\n    frame_buffer_size: int,\n    width: int,\n    height: int,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"],\n    use_denoising_batch: bool,\n    seed: int,\n    cfg_type: Literal[\"none\", \"full\", \"self\", \"initialize\"],\n    guidance_scale: float,\n    delta: float,\n    do_add_noise: bool,\n    enable_similar_image_filter: bool,\n    similar_image_filter_threshold: float,\n    similar_image_filter_max_skip_frame: float,\n    monitor: Dict[str, int],\n) -> None:\n    \"\"\"\n    Process for generating images based on a prompt using a specified model.\n\n    Parameters\n    ----------\n    queue : Queue\n        The queue to put the generated images in.\n    fps_queue : Queue\n        The queue to put the calculated fps.\n    model_id_or_path : str\n        The name of the model to use for image generation.\n    lora_dict : Optional[Dict[str, float]], optional\n        The lora_dict to load, by default None.\n        Keys are the LoRA names and values are the LoRA scales.\n        Example: {\"LoRA_1\" : 0.5 , \"LoRA_2\" : 0.7 ,...}\n    prompt : str\n        The prompt to generate images from.\n    negative_prompt : str, optional\n        The negative prompt to use.\n    frame_buffer_size : int, optional\n        The frame buffer size for denoising batch, by default 1.\n    width : int, optional\n        The width of the image, by default 512.\n    height : int, optional\n        The height of the image, by default 512.\n    acceleration : Literal[\"none\", \"xformers\", \"tensorrt\"], optional\n        The acceleration method, by default \"tensorrt\".\n    use_denoising_batch : bool, optional\n        Whether to use denoising batch or not, by default True.\n    seed : int, optional\n        The seed, by default 2. if -1, use random seed.\n    cfg_type : Literal[\"none\", \"full\", \"self\", \"initialize\"],\n    optional\n        The cfg_type for img2img mode, by default \"self\".\n        You cannot use anything other than \"none\" for txt2img mode.\n    guidance_scale : float, optional\n        The CFG scale, by default 1.2.\n    delta : float, optional\n        The delta multiplier of virtual residual noise,\n        by default 1.0.\n    do_add_noise : bool, optional\n        Whether to add noise for following denoising steps or not,\n        by default True.\n    enable_similar_image_filter : bool, optional\n        Whether to enable similar image filter or not,\n        by default False.\n    similar_image_filter_threshold : float, optional\n        The threshold for similar image filter, by default 0.98.\n    similar_image_filter_max_skip_frame : int, optional\n        The max skip frame for similar image filter, by default 10.\n    \"\"\"\n    \n    global inputs\n    global stop_capture\n    stream = StreamDiffusionWrapper(\n        model_id_or_path=model_id_or_path,\n        lora_dict=lora_dict,\n        t_index_list=[32, 45],\n        frame_buffer_size=frame_buffer_size,\n        width=width,\n        height=height,\n        warmup=10,\n        acceleration=acceleration,\n        do_add_noise=do_add_noise,\n        enable_similar_image_filter=enable_similar_image_filter,\n        similar_image_filter_threshold=similar_image_filter_threshold,\n        similar_image_filter_max_skip_frame=similar_image_filter_max_skip_frame,\n        mode=\"img2img\",\n        use_denoising_batch=use_denoising_batch,\n        cfg_type=cfg_type,\n        seed=seed,\n    )\n\n    stream.prepare(\n        prompt=prompt,\n        negative_prompt=negative_prompt,\n        num_inference_steps=50,\n        guidance_scale=guidance_scale,\n        delta=delta,\n    )\n\n    input_screen = threading.Thread(target=screen, args=(height, width, monitor))\n    input_screen.start()\n    time.sleep(5)\n\n    while True:\n        try:\n            if len(inputs) < frame_buffer_size:\n                time.sleep(0.005)\n                continue\n            start_time = time.time()\n            sampled_inputs = []\n            for i in range(frame_buffer_size):\n                index = (len(inputs) // frame_buffer_size) * i\n                sampled_inputs.append(inputs[len(inputs) - index - 1])\n            input_batch = torch.cat(sampled_inputs)\n            inputs.clear()\n            output_images = stream.stream(\n                input_batch.to(device=stream.device, dtype=stream.dtype)\n            ).cpu()\n            if frame_buffer_size == 1:\n                output_images = [output_images]\n            for output_image in output_images:\n                queue.put(output_image, block=False)\n\n            fps = 1 / (time.time() - start_time)\n            fps_queue.put(fps)\n        except KeyboardInterrupt:\n            stop_capture = True\n            print(f\"fps: {fps}\")\n            return","file":"/examples/screen/main.py","language":"python","fileHash":"a64a104a414d96b3d8bbd68e2b29122a23a279ef0a8a2731cd32a2086841a1e2","hash":"98ab8f7c8586d6ffc8cb11588361bb1779fde7f2220708b79a835c232b841211","processedContent":"def image_generation_process(\n    queue: Queue,\n    fps_queue: Queue,\n    model_id_or_path: str,\n    lora_dict: Optional[Dict[str, float]],\n    \"\"\"\n    scip-python python python-stdlib 3.11 typing/Optional.: undefined\n    scip-python python python-stdlib 3.11 typing/Dict.: undefined\n    \"\"\"\n    prompt: str,\n    negative_prompt: str,\n    frame_buffer_size: int,\n    width: int,\n    height: int,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"], #undefined\n    use_denoising_batch: bool,\n    seed: int,\n    cfg_type: Literal[\"none\", \"full\", \"self\", \"initialize\"], #undefined\n    guidance_scale: float,\n    delta: float,\n    do_add_noise: bool,\n    enable_similar_image_filter: bool,\n    similar_image_filter_threshold: float,\n    similar_image_filter_max_skip_frame: float,\n    monitor: Dict[str, int], #undefined\n) -> None:\n    \"\"\"\n    Process for generating images based on a prompt using a specified model.\n\n    Parameters\n    ----------\n    queue : Queue\n        The queue to put the generated images in.\n    fps_queue : Queue\n        The queue to put the calculated fps.\n    model_id_or_path : str\n        The name of the model to use for image generation.\n    lora_dict : Optional[Dict[str, float]], optional\n        The lora_dict to load, by default None.\n        Keys are the LoRA names and values are the LoRA scales.\n        Example: {\"LoRA_1\" : 0.5 , \"LoRA_2\" : 0.7 ,...}\n    prompt : str\n        The prompt to generate images from.\n    negative_prompt : str, optional\n        The negative prompt to use.\n    frame_buffer_size : int, optional\n        The frame buffer size for denoising batch, by default 1.\n    width : int, optional\n        The width of the image, by default 512.\n    height : int, optional\n        The height of the image, by default 512.\n    acceleration : Literal[\"none\", \"xformers\", \"tensorrt\"], optional\n        The acceleration method, by default \"tensorrt\".\n    use_denoising_batch : bool, optional\n        Whether to use denoising batch or not, by default True.\n    seed : int, optional\n        The seed, by default 2. if -1, use random seed.\n    cfg_type : Literal[\"none\", \"full\", \"self\", \"initialize\"],\n    optional\n        The cfg_type for img2img mode, by default \"self\".\n        You cannot use anything other than \"none\" for txt2img mode.\n    guidance_scale : float, optional\n        The CFG scale, by default 1.2.\n    delta : float, optional\n        The delta multiplier of virtual residual noise,\n        by default 1.0.\n    do_add_noise : bool, optional\n        Whether to add noise for following denoising steps or not,\n        by default True.\n    enable_similar_image_filter : bool, optional\n        Whether to enable similar image filter or not,\n        by default False.\n    similar_image_filter_threshold : float, optional\n        The threshold for similar image filter, by default 0.98.\n    similar_image_filter_max_skip_frame : int, optional\n        The max skip frame for similar image filter, by default 10.\n    \"\"\"\n    \n    global inputs #undefined\n    global stop_capture #undefined\n    stream = StreamDiffusionWrapper( #This code defines a class called `StreamDiffusionWrapper` that loads a model from a given path or ID, performs various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference. It also defines various methods for performing image generation and manipulation.\n        model_id_or_path=model_id_or_path,\n        lora_dict=lora_dict,\n        t_index_list=[32, 45],\n        frame_buffer_size=frame_buffer_size,\n        width=width,\n        height=height,\n        warmup=10,\n        acceleration=acceleration,\n        do_add_noise=do_add_noise,\n        enable_similar_image_filter=enable_similar_image_filter,\n        similar_image_filter_threshold=similar_image_filter_threshold,\n        similar_image_filter_max_skip_frame=similar_image_filter_max_skip_frame,\n        mode=\"img2img\",\n        use_denoising_batch=use_denoising_batch,\n        cfg_type=cfg_type,\n        seed=seed,\n    )\n\n    stream.prepare( #This code prepares the model for inference by setting up the prompt, number of steps, and guidance scale.\n        prompt=prompt,\n        negative_prompt=negative_prompt,\n        num_inference_steps=50,\n        guidance_scale=guidance_scale,\n        delta=delta,\n    )\n\n    input_screen = threading.Thread(target=screen, args=(height, width, monitor))\n    \"\"\"\n    scip-python python python-stdlib 3.11 threading/__init__:: undefined\n    scip-python python python-stdlib 3.11 threading/Thread#: undefined\n    scip-python python temp indexer `examples.screen.main`/screen().: This code defines a function called \"screen\" that captures screenshots of a monitor and appends them to a list called \"inputs\". The function takes two arguments: \"height\" and \"width\", which specify the size of the screenshot, and \"monitor\", which is a dictionary containing information about the monitor. The function uses the \"mss\" library to capture the screenshot and the \"PIL\" library to resize it.\n    \"\"\"\n    input_screen.start() #undefined\n    time.sleep(5)\n    \"\"\"\n    scip-python python python-stdlib 3.11 time/__init__:: undefined\n    scip-python python python-stdlib 3.11 time/sleep().: undefined\n    \"\"\"\n\n    while True:\n        try:\n            if len(inputs) < frame_buffer_size: #undefined\n                time.sleep(0.005)\n                \"\"\"\n                scip-python python python-stdlib 3.11 time/__init__:: undefined\n                scip-python python python-stdlib 3.11 time/sleep().: undefined\n                \"\"\"\n                continue\n            start_time = time.time()\n            \"\"\"\n            scip-python python python-stdlib 3.11 time/__init__:: undefined\n            scip-python python python-stdlib 3.11 time/time().: undefined\n            \"\"\"\n            sampled_inputs = []\n            for i in range(frame_buffer_size):\n                index = (len(inputs) // frame_buffer_size) * i #undefined\n                sampled_inputs.append(inputs[len(inputs) - index - 1])\n                \"\"\"\n                scip-python python temp indexer `examples.screen.main`/inputs.: undefined\n                scip-python python temp indexer `examples.screen.main`/inputs.: undefined\n                \"\"\"\n            input_batch = torch.cat(sampled_inputs)\n            inputs.clear()\n            \"\"\"\n            scip-python python temp indexer `examples.screen.main`/inputs.: undefined\n            scip-python python python-stdlib 3.11 typing/MutableSequence#clear().: undefined\n            \"\"\"\n            output_images = stream.stream( #undefined\n                input_batch.to(device=stream.device, dtype=stream.dtype)\n                \"\"\"\n                scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#device.: undefined\n                scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#dtype.: undefined\n                \"\"\"\n            ).cpu()\n            if frame_buffer_size == 1:\n                output_images = [output_images]\n            for output_image in output_images:\n                queue.put(output_image, block=False)\n\n            fps = 1 / (time.time() - start_time)\n            \"\"\"\n            scip-python python python-stdlib 3.11 time/__init__:: undefined\n            scip-python python python-stdlib 3.11 time/time().: undefined\n            \"\"\"\n            fps_queue.put(fps)\n        except KeyboardInterrupt:\n            stop_capture = True #undefined\n            print(f\"fps: {fps}\")\n            return","documentation":"This code defines a function called \"image_generation_process\" that generates images based on a prompt using a specified model. It uses a class called \"StreamDiffusionWrapper\" to load the model and perform various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference. The function also defines a function called \"screen\" that captures screenshots of a monitor and appends them to a list called \"inputs\"."}},{"key":"scip-python python temp indexer `examples.screen.main`/main().","attributes":{"range":[195,0,249,20],"symbol":"scip-python python temp indexer `examples.screen.main`/main().","content":"def main(\n    model_id_or_path: str = \"KBlueLeaf/kohaku-v2.1\",\n    lora_dict: Optional[Dict[str, float]] = None,\n    prompt: str = \"1girl with brown dog hair, thick glasses, smiling\",\n    negative_prompt: str = \"low quality, bad quality, blurry, low resolution\",\n    frame_buffer_size: int = 1,\n    width: int = 512,\n    height: int = 512,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"xformers\",\n    use_denoising_batch: bool = True,\n    seed: int = 2,\n    cfg_type: Literal[\"none\", \"full\", \"self\", \"initialize\"] = \"self\",\n    guidance_scale: float = 1.4,\n    delta: float = 0.5,\n    do_add_noise: bool = False,\n    enable_similar_image_filter: bool = True,\n    similar_image_filter_threshold: float = 0.99,\n    similar_image_filter_max_skip_frame: float = 10,\n) -> None:\n    \"\"\"\n    Main function to start the image generation and viewer processes.\n    \"\"\"\n    monitor = dummy_screen(width, height)\n\n    queue = Queue()\n    fps_queue = Queue()\n    process1 = Process(\n        target=image_generation_process,\n        args=(\n            queue,\n            fps_queue,\n            model_id_or_path,\n            lora_dict,\n            prompt,\n            negative_prompt,\n            frame_buffer_size,\n            width,\n            height,\n            acceleration,\n            use_denoising_batch,\n            seed,\n            cfg_type,\n            guidance_scale,\n            delta,\n            do_add_noise,\n            enable_similar_image_filter,\n            similar_image_filter_threshold,\n            similar_image_filter_max_skip_frame,\n            monitor\n            ),\n    )\n    process1.start()\n\n    process2 = Process(target=receive_images, args=(queue, fps_queue))\n    process2.start()","file":"/examples/screen/main.py","language":"python","fileHash":"a64a104a414d96b3d8bbd68e2b29122a23a279ef0a8a2731cd32a2086841a1e2","hash":"79bf50917d23233fb7e260375659cecd04e71a2619e19d38de4eefc88b0917cb","processedContent":"def main(\n    model_id_or_path: str = \"KBlueLeaf/kohaku-v2.1\",\n    lora_dict: Optional[Dict[str, float]] = None,\n    \"\"\"\n    scip-python python python-stdlib 3.11 typing/Optional.: undefined\n    scip-python python python-stdlib 3.11 typing/Dict.: undefined\n    \"\"\"\n    prompt: str = \"1girl with brown dog hair, thick glasses, smiling\",\n    negative_prompt: str = \"low quality, bad quality, blurry, low resolution\",\n    frame_buffer_size: int = 1,\n    width: int = 512,\n    height: int = 512,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"xformers\", #undefined\n    use_denoising_batch: bool = True,\n    seed: int = 2,\n    cfg_type: Literal[\"none\", \"full\", \"self\", \"initialize\"] = \"self\", #undefined\n    guidance_scale: float = 1.4,\n    delta: float = 0.5,\n    do_add_noise: bool = False,\n    enable_similar_image_filter: bool = True,\n    similar_image_filter_threshold: float = 0.99,\n    similar_image_filter_max_skip_frame: float = 10,\n) -> None:\n    \"\"\"\n    Main function to start the image generation and viewer processes.\n    \"\"\"\n    monitor = dummy_screen(width, height) #This code defines a function called `dummy_screen` that creates a window with a title and geometry based on the input parameters. It also binds two functions to events, one to destroy the window when the return key is pressed and another to update the window's position based on the mouse position.\n\n    queue = Queue()\n    fps_queue = Queue()\n    process1 = Process(\n        target=image_generation_process, #This code defines a function called \"image_generation_process\" that generates images based on a prompt using a specified model. It uses a class called \"StreamDiffusionWrapper\" to load the model and perform various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference. The function also defines a function called \"screen\" that captures screenshots of a monitor and appends them to a list called \"inputs\".\n        args=(\n            queue,\n            fps_queue,\n            model_id_or_path,\n            lora_dict,\n            prompt,\n            negative_prompt,\n            frame_buffer_size,\n            width,\n            height,\n            acceleration,\n            use_denoising_batch,\n            seed,\n            cfg_type,\n            guidance_scale,\n            delta,\n            do_add_noise,\n            enable_similar_image_filter,\n            similar_image_filter_threshold,\n            similar_image_filter_max_skip_frame,\n            monitor\n            ),\n    )\n    process1.start()\n\n    process2 = Process(target=receive_images, args=(queue, fps_queue)) #This code sets up a Tkinter window to display images and calculates the frames per second (FPS) of the received images. It also defines a function called `on_closing` that is triggered when the user closes the window, effectively closing the application.\n    process2.start()","documentation":"The code defines a main function that starts two processes: one for image generation and another for displaying the generated images in a Tkinter window. The first process generates images based on a prompt using a specified model, while the second process displays the images and calculates the FPS of the received images."}},{"key":"scip-python python temp indexer `examples.screen.main`/__init__:","attributes":{"symbol":"scip-python python temp indexer `examples.screen.main`/__init__:","range":[0,0,253,19],"content":"import os\nimport sys\nimport time\nimport threading\nfrom multiprocessing import Process, Queue\nfrom typing import List, Literal, Dict, Optional\nimport torch\nimport PIL.Image\nfrom streamdiffusion.image_utils import pil2tensor\nimport mss\nimport fire\nimport tkinter as tk\n\nsys.path.append(os.path.join(os.path.dirname(__file__), \"..\", \"..\"))\n\nfrom utils.viewer import receive_images\nfrom utils.wrapper import StreamDiffusionWrapper\n\ninputs = []\nstop_capture = False\ntop = 0\nleft = 0\n\ndef screen(\n    height: int = 512,\n    width: int = 512,\n    monitor: Dict[str, int] = {\"top\": 300, \"left\": 200, \"width\": 512, \"height\": 512},\n):\n    global inputs\n    global stop_capture\n    with mss.mss() as sct:\n        while True:\n            img = sct.grab(monitor)\n            img = PIL.Image.frombytes(\"RGB\", img.size, img.bgra, \"raw\", \"BGRX\")\n            img.resize((height, width))\n            inputs.append(pil2tensor(img))\n            if stop_capture:\n                return\n\ndef dummy_screen(\n        width: int,\n        height: int,\n):\n    root = tk.Tk()\n    root.title(\"Press Enter to start\")\n    root.geometry(f\"{width}x{height}\")\n    root.resizable(False, False)\n    root.attributes(\"-alpha\", 0.8)\n    root.configure(bg=\"black\")\n    def destroy(event):\n        root.destroy()\n    root.bind(\"<Return>\", destroy)\n    def update_geometry(event):\n        global top, left\n        top = root.winfo_y()\n        left = root.winfo_x()\n    root.bind(\"<Configure>\", update_geometry)\n    root.mainloop()\n    return {\"top\": top, \"left\": left, \"width\": width, \"height\": height}\n\ndef image_generation_process(\n    queue: Queue,\n    fps_queue: Queue,\n    model_id_or_path: str,\n    lora_dict: Optional[Dict[str, float]],\n    prompt: str,\n    negative_prompt: str,\n    frame_buffer_size: int,\n    width: int,\n    height: int,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"],\n    use_denoising_batch: bool,\n    seed: int,\n    cfg_type: Literal[\"none\", \"full\", \"self\", \"initialize\"],\n    guidance_scale: float,\n    delta: float,\n    do_add_noise: bool,\n    enable_similar_image_filter: bool,\n    similar_image_filter_threshold: float,\n    similar_image_filter_max_skip_frame: float,\n    monitor: Dict[str, int],\n) -> None:\n    \"\"\"\n    Process for generating images based on a prompt using a specified model.\n\n    Parameters\n    ----------\n    queue : Queue\n        The queue to put the generated images in.\n    fps_queue : Queue\n        The queue to put the calculated fps.\n    model_id_or_path : str\n        The name of the model to use for image generation.\n    lora_dict : Optional[Dict[str, float]], optional\n        The lora_dict to load, by default None.\n        Keys are the LoRA names and values are the LoRA scales.\n        Example: {\"LoRA_1\" : 0.5 , \"LoRA_2\" : 0.7 ,...}\n    prompt : str\n        The prompt to generate images from.\n    negative_prompt : str, optional\n        The negative prompt to use.\n    frame_buffer_size : int, optional\n        The frame buffer size for denoising batch, by default 1.\n    width : int, optional\n        The width of the image, by default 512.\n    height : int, optional\n        The height of the image, by default 512.\n    acceleration : Literal[\"none\", \"xformers\", \"tensorrt\"], optional\n        The acceleration method, by default \"tensorrt\".\n    use_denoising_batch : bool, optional\n        Whether to use denoising batch or not, by default True.\n    seed : int, optional\n        The seed, by default 2. if -1, use random seed.\n    cfg_type : Literal[\"none\", \"full\", \"self\", \"initialize\"],\n    optional\n        The cfg_type for img2img mode, by default \"self\".\n        You cannot use anything other than \"none\" for txt2img mode.\n    guidance_scale : float, optional\n        The CFG scale, by default 1.2.\n    delta : float, optional\n        The delta multiplier of virtual residual noise,\n        by default 1.0.\n    do_add_noise : bool, optional\n        Whether to add noise for following denoising steps or not,\n        by default True.\n    enable_similar_image_filter : bool, optional\n        Whether to enable similar image filter or not,\n        by default False.\n    similar_image_filter_threshold : float, optional\n        The threshold for similar image filter, by default 0.98.\n    similar_image_filter_max_skip_frame : int, optional\n        The max skip frame for similar image filter, by default 10.\n    \"\"\"\n    \n    global inputs\n    global stop_capture\n    stream = StreamDiffusionWrapper(\n        model_id_or_path=model_id_or_path,\n        lora_dict=lora_dict,\n        t_index_list=[32, 45],\n        frame_buffer_size=frame_buffer_size,\n        width=width,\n        height=height,\n        warmup=10,\n        acceleration=acceleration,\n        do_add_noise=do_add_noise,\n        enable_similar_image_filter=enable_similar_image_filter,\n        similar_image_filter_threshold=similar_image_filter_threshold,\n        similar_image_filter_max_skip_frame=similar_image_filter_max_skip_frame,\n        mode=\"img2img\",\n        use_denoising_batch=use_denoising_batch,\n        cfg_type=cfg_type,\n        seed=seed,\n    )\n\n    stream.prepare(\n        prompt=prompt,\n        negative_prompt=negative_prompt,\n        num_inference_steps=50,\n        guidance_scale=guidance_scale,\n        delta=delta,\n    )\n\n    input_screen = threading.Thread(target=screen, args=(height, width, monitor))\n    input_screen.start()\n    time.sleep(5)\n\n    while True:\n        try:\n            if len(inputs) < frame_buffer_size:\n                time.sleep(0.005)\n                continue\n            start_time = time.time()\n            sampled_inputs = []\n            for i in range(frame_buffer_size):\n                index = (len(inputs) // frame_buffer_size) * i\n                sampled_inputs.append(inputs[len(inputs) - index - 1])\n            input_batch = torch.cat(sampled_inputs)\n            inputs.clear()\n            output_images = stream.stream(\n                input_batch.to(device=stream.device, dtype=stream.dtype)\n            ).cpu()\n            if frame_buffer_size == 1:\n                output_images = [output_images]\n            for output_image in output_images:\n                queue.put(output_image, block=False)\n\n            fps = 1 / (time.time() - start_time)\n            fps_queue.put(fps)\n        except KeyboardInterrupt:\n            stop_capture = True\n            print(f\"fps: {fps}\")\n            return\n\n\ndef main(\n    model_id_or_path: str = \"KBlueLeaf/kohaku-v2.1\",\n    lora_dict: Optional[Dict[str, float]] = None,\n    prompt: str = \"1girl with brown dog hair, thick glasses, smiling\",\n    negative_prompt: str = \"low quality, bad quality, blurry, low resolution\",\n    frame_buffer_size: int = 1,\n    width: int = 512,\n    height: int = 512,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"xformers\",\n    use_denoising_batch: bool = True,\n    seed: int = 2,\n    cfg_type: Literal[\"none\", \"full\", \"self\", \"initialize\"] = \"self\",\n    guidance_scale: float = 1.4,\n    delta: float = 0.5,\n    do_add_noise: bool = False,\n    enable_similar_image_filter: bool = True,\n    similar_image_filter_threshold: float = 0.99,\n    similar_image_filter_max_skip_frame: float = 10,\n) -> None:\n    \"\"\"\n    Main function to start the image generation and viewer processes.\n    \"\"\"\n    monitor = dummy_screen(width, height)\n\n    queue = Queue()\n    fps_queue = Queue()\n    process1 = Process(\n        target=image_generation_process,\n        args=(\n            queue,\n            fps_queue,\n            model_id_or_path,\n            lora_dict,\n            prompt,\n            negative_prompt,\n            frame_buffer_size,\n            width,\n            height,\n            acceleration,\n            use_denoising_batch,\n            seed,\n            cfg_type,\n            guidance_scale,\n            delta,\n            do_add_noise,\n            enable_similar_image_filter,\n            similar_image_filter_threshold,\n            similar_image_filter_max_skip_frame,\n            monitor\n            ),\n    )\n    process1.start()\n\n    process2 = Process(target=receive_images, args=(queue, fps_queue))\n    process2.start()\n\n\nif __name__ == \"__main__\":\n    fire.Fire(main)","file":"/examples/screen/main.py","language":"python","fileHash":"a64a104a414d96b3d8bbd68e2b29122a23a279ef0a8a2731cd32a2086841a1e2","hash":"a64a104a414d96b3d8bbd68e2b29122a23a279ef0a8a2731cd32a2086841a1e2","processedContent":"import os #undefined\nimport sys #undefined\nimport time #undefined\nimport threading #undefined\nfrom multiprocessing import Process, Queue #undefined\nfrom typing import List, Literal, Dict, Optional\n\"\"\"\nscip-python python python-stdlib 3.11 typing/__init__:: undefined\nscip-python python python-stdlib 3.11 typing/List.: undefined\nscip-python python python-stdlib 3.11 typing/Literal.: undefined\nscip-python python python-stdlib 3.11 typing/Dict.: undefined\nscip-python python python-stdlib 3.11 typing/Optional.: undefined\n\"\"\"\nimport torch\nimport PIL.Image #undefined\nfrom streamdiffusion.image_utils import pil2tensor #undefined\nimport mss\nimport fire\nimport tkinter as tk #undefined\n\nsys.path.append(os.path.join(os.path.dirname(__file__), \"..\", \"..\"))\n\"\"\"\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 sys/__init__:: undefined\nscip-python python python-stdlib 3.11 sys/path.path.: undefined\nscip-python python python-stdlib 3.11 os/path.: undefined\nscip-python python python-stdlib 3.11 os/path.: undefined\nscip-python python python-stdlib 3.11 ntpath/join().: undefined\nscip-python python python-stdlib 3.11 ntpath/join().: undefined\n\"\"\"\n\nfrom utils.viewer import receive_images\n\"\"\"\nscip-python python temp indexer `utils.viewer`/__init__:: The code is a Python script that creates a Tkinter window to display images and calculates the frames per second (FPS) of the received images. It also defines a function called `on_closing` that is triggered when the user closes the window, effectively closing the application.\nscip-python python temp indexer `utils.viewer`/receive_images().: This code sets up a Tkinter window to display images and calculates the frames per second (FPS) of the received images. It also defines a function called `on_closing` that is triggered when the user closes the window, effectively closing the application.\n\"\"\"\nfrom utils.wrapper import StreamDiffusionWrapper\n\"\"\"\nscip-python python temp indexer `utils.wrapper`/__init__:: This code defines a class called `StreamDiffusionWrapper` that loads a model for image generation and manipulation, enabling acceleration, and preparing the model for inference.\nscip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#: This code defines a class called `StreamDiffusionWrapper` that loads a model from a given path or ID, performs various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference. It also defines various methods for performing image generation and manipulation.\n\"\"\"\n\ninputs = []\nstop_capture = False\ntop = 0\nleft = 0\n\ndef screen(\n    \"\"\"This code defines a function called \"screen\" that captures screenshots of a monitor and appends them to a list called \"inputs\". The function takes two arguments: \"height\" and \"width\", which specify the size of the screenshot, and \"monitor\", which is a dictionary containing information about the monitor. The function uses the \"mss\" library to capture the screenshot and the \"PIL\" library to resize it.\"\"\"\n    pass\n\ndef dummy_screen(\n        \"\"\"This code defines a function called `dummy_screen` that creates a window with a title and geometry based on the input parameters. It also binds two functions to events, one to destroy the window when the return key is pressed and another to update the window's position based on the mouse position.\"\"\"\n        pass\n\ndef image_generation_process(\n    \"\"\"This code defines a function called \"image_generation_process\" that generates images based on a prompt using a specified model. It uses a class called \"StreamDiffusionWrapper\" to load the model and perform various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference. The function also defines a function called \"screen\" that captures screenshots of a monitor and appends them to a list called \"inputs\".\"\"\"\n    pass\n\n\ndef main(\n    \"\"\"The code defines a main function that starts two processes: one for image generation and another for displaying the generated images in a Tkinter window. The first process generates images based on a prompt using a specified model, while the second process displays the images and calculates the FPS of the received images.\"\"\"\n    pass\n\n\nif __name__ == \"__main__\": #undefined\n    fire.Fire(main)","documentation":"The code is a Python script that uses the StreamDiffusion library to generate images based on a prompt and display them in a Tkinter window. It also calculates the frames per second (FPS) of the received images."}},{"key":"scip-python python temp indexer `examples.screen.main`/inputs.","attributes":{"symbol":"scip-python python temp indexer `examples.screen.main`/inputs.","language":"python"}},{"key":"scip-python python temp indexer `examples.screen.main`/stop_capture.","attributes":{"symbol":"scip-python python temp indexer `examples.screen.main`/stop_capture.","language":"python"}},{"key":"scip-python python Pillow 10.0.0 `pil.image`/frombytes().","attributes":{"symbol":"scip-python python Pillow 10.0.0 `pil.image`/frombytes().","language":"python"}},{"key":"scip-python python python-stdlib 3.11 tkinter/Wm#geometry.","attributes":{"symbol":"scip-python python python-stdlib 3.11 tkinter/Wm#geometry.","language":"python"}},{"key":"scip-python python python-stdlib 3.11 tkinter/Wm#resizable.","attributes":{"symbol":"scip-python python python-stdlib 3.11 tkinter/Wm#resizable.","language":"python"}},{"key":"scip-python python python-stdlib 3.11 tkinter/Wm#attributes.","attributes":{"symbol":"scip-python python python-stdlib 3.11 tkinter/Wm#attributes.","language":"python"}},{"key":"scip-python python python-stdlib 3.11 tkinter/Tk#configure().","attributes":{"symbol":"scip-python python python-stdlib 3.11 tkinter/Tk#configure().","language":"python"}},{"key":"scip-python python python-stdlib 3.11 tkinter/Tk#destroy().","attributes":{"symbol":"scip-python python python-stdlib 3.11 tkinter/Tk#destroy().","language":"python"}},{"key":"scip-python python python-stdlib 3.11 tkinter/Misc#bind().","attributes":{"symbol":"scip-python python python-stdlib 3.11 tkinter/Misc#bind().","language":"python"}},{"key":"scip-python python temp indexer `examples.screen.main`/top.","attributes":{"symbol":"scip-python python temp indexer `examples.screen.main`/top.","language":"python"}},{"key":"scip-python python temp indexer `examples.screen.main`/left.","attributes":{"symbol":"scip-python python temp indexer `examples.screen.main`/left.","language":"python"}},{"key":"scip-python python python-stdlib 3.11 tkinter/Misc#winfo_y().","attributes":{"symbol":"scip-python python python-stdlib 3.11 tkinter/Misc#winfo_y().","language":"python"}},{"key":"scip-python python python-stdlib 3.11 tkinter/Misc#winfo_x().","attributes":{"symbol":"scip-python python python-stdlib 3.11 tkinter/Misc#winfo_x().","language":"python"}},{"key":"scip-python python python-stdlib 3.11 typing/MutableSequence#clear().","attributes":{"symbol":"scip-python python python-stdlib 3.11 typing/MutableSequence#clear().","language":"python"}},{"key":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#device.","attributes":{"symbol":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#device.","language":"python"}},{"key":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#dtype.","attributes":{"symbol":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#dtype.","language":"python"}},{"key":"scip-python python temp indexer `examples.screen.main`/","attributes":{"symbol":"scip-python python temp indexer `examples.screen.main`/","language":"python"}},{"key":"scip-python python temp indexer `examples.txt2img.multi`/main().","attributes":{"range":[13,0,76,62],"symbol":"scip-python python temp indexer `examples.txt2img.multi`/main().","content":"def main(\n    output: str = os.path.join(CURRENT_DIR, \"..\", \"..\", \"images\", \"outputs\",),\n    model_id_or_path: str = \"KBlueLeaf/kohaku-v2.1\",\n    lora_dict: Optional[Dict[str, float]] = None,\n    prompt: str = \"1girl with brown dog hair, thick glasses, smiling\",\n    width: int = 512,\n    height: int = 512,\n    frame_buffer_size: int = 3,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"xformers\",\n    seed: int = 2,\n):\n    \n    \"\"\"\n    Process for generating images based on a prompt using a specified model.\n\n    Parameters\n    ----------\n    output : str, optional\n        The output image file to save images to.\n    model_id_or_path : str\n        The name of the model to use for image generation.\n    lora_dict : Optional[Dict[str, float]], optional\n        The lora_dict to load, by default None.\n        Keys are the LoRA names and values are the LoRA scales.\n        Example: {\"LoRA_1\" : 0.5 , \"LoRA_2\" : 0.7 ,...}\n    prompt : str\n        The prompt to generate images from.\n    width : int, optional\n        The width of the image, by default 512.\n    height : int, optional\n        The height of the image, by default 512.\n    acceleration : Literal[\"none\", \"xformers\", \"tensorrt\"]\n        The type of acceleration to use for image generation.\n    use_denoising_batch : bool, optional\n        Whether to use denoising batch or not, by default False.\n    seed : int, optional\n        The seed, by default 2. if -1, use random seed.\n    \"\"\"\n\n    os.makedirs(output, exist_ok=True)\n\n    stream = StreamDiffusionWrapper(\n        model_id_or_path=model_id_or_path,\n        lora_dict=lora_dict,\n        t_index_list=[0, 16, 32, 45],\n        frame_buffer_size=frame_buffer_size,\n        width=width,\n        height=height,\n        warmup=10,\n        acceleration=acceleration,\n        mode=\"txt2img\",\n        use_denoising_batch=False,\n        cfg_type=\"none\",\n        seed=seed,\n    )\n\n    stream.prepare(\n        prompt=prompt,\n        num_inference_steps=50,\n    )\n\n    output_images = stream()\n    for i, output_image in enumerate(output_images):\n        output_image.save(os.path.join(output, f\"{i:02}.png\"))","file":"/examples/txt2img/multi.py","language":"python","fileHash":"068353881012ded166f2bbbcdfbbbbbedc44650754312cfeb2a85c162c284b6d","hash":"3c4f6efadd18f3aaeb5e4cb7165e239e534239cc1408498a0d963a8272f03ece","processedContent":"def main(\n    output: str = os.path.join(CURRENT_DIR, \"..\", \"..\", \"images\", \"outputs\",),\n    \"\"\"\n    scip-python python python-stdlib 3.11 os/__init__:: undefined\n    scip-python python python-stdlib 3.11 os/path.: undefined\n    scip-python python python-stdlib 3.11 ntpath/join().: undefined\n    scip-python python temp indexer `examples.txt2img.multi`/CURRENT_DIR.: undefined\n    \"\"\"\n    model_id_or_path: str = \"KBlueLeaf/kohaku-v2.1\",\n    lora_dict: Optional[Dict[str, float]] = None,\n    \"\"\"\n    scip-python python python-stdlib 3.11 typing/Optional.: undefined\n    scip-python python python-stdlib 3.11 typing/Dict.: undefined\n    \"\"\"\n    prompt: str = \"1girl with brown dog hair, thick glasses, smiling\",\n    width: int = 512,\n    height: int = 512,\n    frame_buffer_size: int = 3,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"xformers\", #undefined\n    seed: int = 2,\n):\n    \n    \"\"\"\n    Process for generating images based on a prompt using a specified model.\n\n    Parameters\n    ----------\n    output : str, optional\n        The output image file to save images to.\n    model_id_or_path : str\n        The name of the model to use for image generation.\n    lora_dict : Optional[Dict[str, float]], optional\n        The lora_dict to load, by default None.\n        Keys are the LoRA names and values are the LoRA scales.\n        Example: {\"LoRA_1\" : 0.5 , \"LoRA_2\" : 0.7 ,...}\n    prompt : str\n        The prompt to generate images from.\n    width : int, optional\n        The width of the image, by default 512.\n    height : int, optional\n        The height of the image, by default 512.\n    acceleration : Literal[\"none\", \"xformers\", \"tensorrt\"]\n        The type of acceleration to use for image generation.\n    use_denoising_batch : bool, optional\n        Whether to use denoising batch or not, by default False.\n    seed : int, optional\n        The seed, by default 2. if -1, use random seed.\n    \"\"\"\n\n    os.makedirs(output, exist_ok=True)\n    \"\"\"\n    scip-python python python-stdlib 3.11 os/__init__:: undefined\n    scip-python python python-stdlib 3.11 os/makedirs().: undefined\n    \"\"\"\n\n    stream = StreamDiffusionWrapper( #This code defines a class called `StreamDiffusionWrapper` that loads a model from a given path or ID, performs various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference. It also defines various methods for performing image generation and manipulation.\n        model_id_or_path=model_id_or_path,\n        lora_dict=lora_dict,\n        t_index_list=[0, 16, 32, 45],\n        frame_buffer_size=frame_buffer_size,\n        width=width,\n        height=height,\n        warmup=10,\n        acceleration=acceleration,\n        mode=\"txt2img\",\n        use_denoising_batch=False,\n        cfg_type=\"none\",\n        seed=seed,\n    )\n\n    stream.prepare( #This code prepares the model for inference by setting up the prompt, number of steps, and guidance scale.\n        prompt=prompt,\n        num_inference_steps=50,\n    )\n\n    output_images = stream()\n    for i, output_image in enumerate(output_images):\n        output_image.save(os.path.join(output, f\"{i:02}.png\"))","documentation":"The code defines a function called `main` that generates images based on a prompt using a specified model. It creates a `StreamDiffusionWrapper` class instance and prepares it for inference, then generates 50 images and saves them to the output directory."}},{"key":"scip-python python temp indexer `examples.txt2img.multi`/__init__:","attributes":{"symbol":"scip-python python temp indexer `examples.txt2img.multi`/__init__:","range":[0,0,81,0],"content":"import os\nimport sys\nfrom typing import Literal, Dict, Optional\n\nimport fire\n\nsys.path.append(os.path.join(os.path.dirname(__file__), \"..\", \"..\"))\n\nfrom utils.wrapper import StreamDiffusionWrapper\n\nCURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n\n\ndef main(\n    output: str = os.path.join(CURRENT_DIR, \"..\", \"..\", \"images\", \"outputs\",),\n    model_id_or_path: str = \"KBlueLeaf/kohaku-v2.1\",\n    lora_dict: Optional[Dict[str, float]] = None,\n    prompt: str = \"1girl with brown dog hair, thick glasses, smiling\",\n    width: int = 512,\n    height: int = 512,\n    frame_buffer_size: int = 3,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"xformers\",\n    seed: int = 2,\n):\n    \n    \"\"\"\n    Process for generating images based on a prompt using a specified model.\n\n    Parameters\n    ----------\n    output : str, optional\n        The output image file to save images to.\n    model_id_or_path : str\n        The name of the model to use for image generation.\n    lora_dict : Optional[Dict[str, float]], optional\n        The lora_dict to load, by default None.\n        Keys are the LoRA names and values are the LoRA scales.\n        Example: {\"LoRA_1\" : 0.5 , \"LoRA_2\" : 0.7 ,...}\n    prompt : str\n        The prompt to generate images from.\n    width : int, optional\n        The width of the image, by default 512.\n    height : int, optional\n        The height of the image, by default 512.\n    acceleration : Literal[\"none\", \"xformers\", \"tensorrt\"]\n        The type of acceleration to use for image generation.\n    use_denoising_batch : bool, optional\n        Whether to use denoising batch or not, by default False.\n    seed : int, optional\n        The seed, by default 2. if -1, use random seed.\n    \"\"\"\n\n    os.makedirs(output, exist_ok=True)\n\n    stream = StreamDiffusionWrapper(\n        model_id_or_path=model_id_or_path,\n        lora_dict=lora_dict,\n        t_index_list=[0, 16, 32, 45],\n        frame_buffer_size=frame_buffer_size,\n        width=width,\n        height=height,\n        warmup=10,\n        acceleration=acceleration,\n        mode=\"txt2img\",\n        use_denoising_batch=False,\n        cfg_type=\"none\",\n        seed=seed,\n    )\n\n    stream.prepare(\n        prompt=prompt,\n        num_inference_steps=50,\n    )\n\n    output_images = stream()\n    for i, output_image in enumerate(output_images):\n        output_image.save(os.path.join(output, f\"{i:02}.png\"))\n\n\nif __name__ == \"__main__\":\n    fire.Fire(main)\n","file":"/examples/txt2img/multi.py","language":"python","fileHash":"068353881012ded166f2bbbcdfbbbbbedc44650754312cfeb2a85c162c284b6d","hash":"068353881012ded166f2bbbcdfbbbbbedc44650754312cfeb2a85c162c284b6d","processedContent":"import os #undefined\nimport sys #undefined\nfrom typing import Literal, Dict, Optional\n\"\"\"\nscip-python python python-stdlib 3.11 typing/__init__:: undefined\nscip-python python python-stdlib 3.11 typing/Literal.: undefined\nscip-python python python-stdlib 3.11 typing/Dict.: undefined\nscip-python python python-stdlib 3.11 typing/Optional.: undefined\n\"\"\"\n\nimport fire\n\nsys.path.append(os.path.join(os.path.dirname(__file__), \"..\", \"..\"))\n\"\"\"\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 sys/__init__:: undefined\nscip-python python python-stdlib 3.11 sys/path.path.: undefined\nscip-python python python-stdlib 3.11 os/path.: undefined\nscip-python python python-stdlib 3.11 os/path.: undefined\nscip-python python python-stdlib 3.11 ntpath/join().: undefined\nscip-python python python-stdlib 3.11 ntpath/join().: undefined\n\"\"\"\n\nfrom utils.wrapper import StreamDiffusionWrapper\n\"\"\"\nscip-python python temp indexer `utils.wrapper`/__init__:: This code defines a class called `StreamDiffusionWrapper` that loads a model for image generation and manipulation, enabling acceleration, and preparing the model for inference.\nscip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#: This code defines a class called `StreamDiffusionWrapper` that loads a model from a given path or ID, performs various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference. It also defines various methods for performing image generation and manipulation.\n\"\"\"\n\nCURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n\"\"\"\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 os/path.: undefined\nscip-python python python-stdlib 3.11 os/path.: undefined\nscip-python python python-stdlib 3.11 ntpath/join().: undefined\nscip-python python python-stdlib 3.11 ntpath/join().: undefined\nscip-python python temp indexer `examples.txt2img.multi`/: undefined\n\"\"\"\n\n\ndef main(\n    \"\"\"The code defines a function called `main` that generates images based on a prompt using a specified model. It creates a `StreamDiffusionWrapper` class instance and prepares it for inference, then generates 50 images and saves them to the output directory.\"\"\"\n    pass\n\n\nif __name__ == \"__main__\": #undefined\n    fire.Fire(main) #The code defines a function called `main` that generates images based on a prompt using a specified model. It creates a `StreamDiffusionWrapper` class instance and prepares it for inference, then generates 50 images and saves them to the output directory.\n","documentation":"The code defines a function called `main` that generates images based on a prompt using a specified model. It creates a `StreamDiffusionWrapper` class instance and prepares it for inference, then generates 50 images and saves them to the output directory."}},{"key":"scip-python python temp indexer `examples.txt2img.multi`/","attributes":{"symbol":"scip-python python temp indexer `examples.txt2img.multi`/","language":"python"}},{"key":"scip-python python temp indexer `examples.txt2img.multi`/CURRENT_DIR.","attributes":{"symbol":"scip-python python temp indexer `examples.txt2img.multi`/CURRENT_DIR.","language":"python"}},{"key":"scip-python python temp indexer `examples.txt2img.single`/main().","attributes":{"range":[14,0,77,29],"symbol":"scip-python python temp indexer `examples.txt2img.single`/main().","content":"def main(\n    output: str = os.path.join(CURRENT_DIR, \"..\", \"..\", \"images\", \"outputs\", \"output.png\"),\n    model_id_or_path: str = \"KBlueLeaf/kohaku-v2.1\",\n    lora_dict: Optional[Dict[str, float]] = None,\n    prompt: str = \"1girl with brown dog hair, thick glasses, smiling\",\n    width: int = 512,\n    height: int = 512,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"xformers\",\n    use_denoising_batch: bool = False,\n    seed: int = 2,\n):\n    \n    \"\"\"\n    Process for generating images based on a prompt using a specified model.\n\n    Parameters\n    ----------\n    output : str, optional\n        The output image file to save images to.\n    model_id_or_path : str\n        The name of the model to use for image generation.\n    lora_dict : Optional[Dict[str, float]], optional\n        The lora_dict to load, by default None.\n        Keys are the LoRA names and values are the LoRA scales.\n        Example: {\"LoRA_1\" : 0.5 , \"LoRA_2\" : 0.7 ,...}\n    prompt : str\n        The prompt to generate images from.\n    width : int, optional\n        The width of the image, by default 512.\n    height : int, optional\n        The height of the image, by default 512.\n    acceleration : Literal[\"none\", \"xformers\", \"tensorrt\"]\n        The type of acceleration to use for image generation.\n    use_denoising_batch : bool, optional\n        Whether to use denoising batch or not, by default False.\n    seed : int, optional\n        The seed, by default 2. if -1, use random seed.\n    \"\"\"\n\n    stream = StreamDiffusionWrapper(\n        model_id_or_path=model_id_or_path,\n        lora_dict=lora_dict,\n        t_index_list=[0, 16, 32, 45],\n        frame_buffer_size=1,\n        width=width,\n        height=height,\n        warmup=10,\n        acceleration=acceleration,\n        mode=\"txt2img\",\n        use_denoising_batch=use_denoising_batch,\n        cfg_type=\"none\",\n        seed=seed,\n    )\n\n    stream.prepare(\n        prompt=prompt,\n        num_inference_steps=50,\n    )\n\n    for _ in range(stream.batch_size - 1):\n        stream()\n\n    output_image = stream()\n    output_image.save(output)","file":"/examples/txt2img/single.py","language":"python","fileHash":"9380c06412e8b22b6d264d8030028407238565ca73c731362c924dc60ea9840b","hash":"9d68f5c03c7ee1ca4d9e03a2f74b8d36d93b39df5618b463f76e9aa311418ba9","processedContent":"def main(\n    output: str = os.path.join(CURRENT_DIR, \"..\", \"..\", \"images\", \"outputs\", \"output.png\"),\n    \"\"\"\n    scip-python python python-stdlib 3.11 os/__init__:: undefined\n    scip-python python python-stdlib 3.11 os/path.: undefined\n    scip-python python python-stdlib 3.11 ntpath/join().: undefined\n    scip-python python temp indexer `examples.txt2img.single`/CURRENT_DIR.: undefined\n    \"\"\"\n    model_id_or_path: str = \"KBlueLeaf/kohaku-v2.1\",\n    lora_dict: Optional[Dict[str, float]] = None,\n    \"\"\"\n    scip-python python python-stdlib 3.11 typing/Optional.: undefined\n    scip-python python python-stdlib 3.11 typing/Dict.: undefined\n    \"\"\"\n    prompt: str = \"1girl with brown dog hair, thick glasses, smiling\",\n    width: int = 512,\n    height: int = 512,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"xformers\", #undefined\n    use_denoising_batch: bool = False,\n    seed: int = 2,\n):\n    \n    \"\"\"\n    Process for generating images based on a prompt using a specified model.\n\n    Parameters\n    ----------\n    output : str, optional\n        The output image file to save images to.\n    model_id_or_path : str\n        The name of the model to use for image generation.\n    lora_dict : Optional[Dict[str, float]], optional\n        The lora_dict to load, by default None.\n        Keys are the LoRA names and values are the LoRA scales.\n        Example: {\"LoRA_1\" : 0.5 , \"LoRA_2\" : 0.7 ,...}\n    prompt : str\n        The prompt to generate images from.\n    width : int, optional\n        The width of the image, by default 512.\n    height : int, optional\n        The height of the image, by default 512.\n    acceleration : Literal[\"none\", \"xformers\", \"tensorrt\"]\n        The type of acceleration to use for image generation.\n    use_denoising_batch : bool, optional\n        Whether to use denoising batch or not, by default False.\n    seed : int, optional\n        The seed, by default 2. if -1, use random seed.\n    \"\"\"\n\n    stream = StreamDiffusionWrapper( #This code defines a class called `StreamDiffusionWrapper` that loads a model from a given path or ID, performs various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference. It also defines various methods for performing image generation and manipulation.\n        model_id_or_path=model_id_or_path,\n        lora_dict=lora_dict,\n        t_index_list=[0, 16, 32, 45],\n        frame_buffer_size=1,\n        width=width,\n        height=height,\n        warmup=10,\n        acceleration=acceleration,\n        mode=\"txt2img\",\n        use_denoising_batch=use_denoising_batch,\n        cfg_type=\"none\",\n        seed=seed,\n    )\n\n    stream.prepare( #This code prepares the model for inference by setting up the prompt, number of steps, and guidance scale.\n        prompt=prompt,\n        num_inference_steps=50,\n    )\n\n    for _ in range(stream.batch_size - 1): #undefined\n        stream()\n\n    output_image = stream()\n    output_image.save(output)","documentation":"This code defines a function called `main` that generates images based on a prompt using a specified model. It uses a class called `StreamDiffusionWrapper` to perform various tasks such as loading models, enabling acceleration, and preparing the model for inference. The function takes in various parameters such as the output image file, the model ID or path, and the prompt, and then generates the image using the `StreamDiffusionWrapper` class."}},{"key":"scip-python python temp indexer `examples.txt2img.single`/__init__:","attributes":{"symbol":"scip-python python temp indexer `examples.txt2img.single`/__init__:","range":[0,0,82,0],"content":"import os\nimport sys\nfrom typing import Literal, Dict, Optional\n\nimport fire\n\n\nsys.path.append(os.path.join(os.path.dirname(__file__), \"..\", \"..\"))\n\nfrom utils.wrapper import StreamDiffusionWrapper\n\nCURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n\n\ndef main(\n    output: str = os.path.join(CURRENT_DIR, \"..\", \"..\", \"images\", \"outputs\", \"output.png\"),\n    model_id_or_path: str = \"KBlueLeaf/kohaku-v2.1\",\n    lora_dict: Optional[Dict[str, float]] = None,\n    prompt: str = \"1girl with brown dog hair, thick glasses, smiling\",\n    width: int = 512,\n    height: int = 512,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"xformers\",\n    use_denoising_batch: bool = False,\n    seed: int = 2,\n):\n    \n    \"\"\"\n    Process for generating images based on a prompt using a specified model.\n\n    Parameters\n    ----------\n    output : str, optional\n        The output image file to save images to.\n    model_id_or_path : str\n        The name of the model to use for image generation.\n    lora_dict : Optional[Dict[str, float]], optional\n        The lora_dict to load, by default None.\n        Keys are the LoRA names and values are the LoRA scales.\n        Example: {\"LoRA_1\" : 0.5 , \"LoRA_2\" : 0.7 ,...}\n    prompt : str\n        The prompt to generate images from.\n    width : int, optional\n        The width of the image, by default 512.\n    height : int, optional\n        The height of the image, by default 512.\n    acceleration : Literal[\"none\", \"xformers\", \"tensorrt\"]\n        The type of acceleration to use for image generation.\n    use_denoising_batch : bool, optional\n        Whether to use denoising batch or not, by default False.\n    seed : int, optional\n        The seed, by default 2. if -1, use random seed.\n    \"\"\"\n\n    stream = StreamDiffusionWrapper(\n        model_id_or_path=model_id_or_path,\n        lora_dict=lora_dict,\n        t_index_list=[0, 16, 32, 45],\n        frame_buffer_size=1,\n        width=width,\n        height=height,\n        warmup=10,\n        acceleration=acceleration,\n        mode=\"txt2img\",\n        use_denoising_batch=use_denoising_batch,\n        cfg_type=\"none\",\n        seed=seed,\n    )\n\n    stream.prepare(\n        prompt=prompt,\n        num_inference_steps=50,\n    )\n\n    for _ in range(stream.batch_size - 1):\n        stream()\n\n    output_image = stream()\n    output_image.save(output)\n\n\nif __name__ == \"__main__\":\n    fire.Fire(main)\n","file":"/examples/txt2img/single.py","language":"python","fileHash":"9380c06412e8b22b6d264d8030028407238565ca73c731362c924dc60ea9840b","hash":"9380c06412e8b22b6d264d8030028407238565ca73c731362c924dc60ea9840b","processedContent":"import os #undefined\nimport sys #undefined\nfrom typing import Literal, Dict, Optional\n\"\"\"\nscip-python python python-stdlib 3.11 typing/__init__:: undefined\nscip-python python python-stdlib 3.11 typing/Literal.: undefined\nscip-python python python-stdlib 3.11 typing/Dict.: undefined\nscip-python python python-stdlib 3.11 typing/Optional.: undefined\n\"\"\"\n\nimport fire\n\n\nsys.path.append(os.path.join(os.path.dirname(__file__), \"..\", \"..\"))\n\"\"\"\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 sys/__init__:: undefined\nscip-python python python-stdlib 3.11 sys/path.path.: undefined\nscip-python python python-stdlib 3.11 os/path.: undefined\nscip-python python python-stdlib 3.11 os/path.: undefined\nscip-python python python-stdlib 3.11 ntpath/join().: undefined\nscip-python python python-stdlib 3.11 ntpath/join().: undefined\n\"\"\"\n\nfrom utils.wrapper import StreamDiffusionWrapper\n\"\"\"\nscip-python python temp indexer `utils.wrapper`/__init__:: This code defines a class called `StreamDiffusionWrapper` that loads a model for image generation and manipulation, enabling acceleration, and preparing the model for inference.\nscip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#: This code defines a class called `StreamDiffusionWrapper` that loads a model from a given path or ID, performs various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference. It also defines various methods for performing image generation and manipulation.\n\"\"\"\n\nCURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n\"\"\"\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 os/path.: undefined\nscip-python python python-stdlib 3.11 os/path.: undefined\nscip-python python python-stdlib 3.11 ntpath/join().: undefined\nscip-python python python-stdlib 3.11 ntpath/join().: undefined\nscip-python python temp indexer `examples.txt2img.single`/: undefined\n\"\"\"\n\n\ndef main(\n    \"\"\"This code defines a function called `main` that generates images based on a prompt using a specified model. It uses a class called `StreamDiffusionWrapper` to perform various tasks such as loading models, enabling acceleration, and preparing the model for inference. The function takes in various parameters such as the output image file, the model ID or path, and the prompt, and then generates the image using the `StreamDiffusionWrapper` class.\"\"\"\n    pass\n\n\nif __name__ == \"__main__\": #undefined\n    fire.Fire(main) #This code defines a function called `main` that generates images based on a prompt using a specified model. It uses a class called `StreamDiffusionWrapper` to perform various tasks such as loading models, enabling acceleration, and preparing the model for inference. The function takes in various parameters such as the output image file, the model ID or path, and the prompt, and then generates the image using the `StreamDiffusionWrapper` class.\n","documentation":"This code defines a function called `main` that generates images based on a prompt using a specified model. It uses a class called `StreamDiffusionWrapper` to perform various tasks such as loading models, enabling acceleration, and preparing the model for inference."}},{"key":"scip-python python temp indexer `examples.txt2img.single`/","attributes":{"symbol":"scip-python python temp indexer `examples.txt2img.single`/","language":"python"}},{"key":"scip-python python temp indexer `examples.txt2img.single`/CURRENT_DIR.","attributes":{"symbol":"scip-python python temp indexer `examples.txt2img.single`/CURRENT_DIR.","language":"python"}},{"key":"scip-python python temp indexer `examples.vid2vid.main`/main().","attributes":{"range":[16,0,98,50],"symbol":"scip-python python temp indexer `examples.vid2vid.main`/main().","content":"def main(\n    input: str,\n    output: str = os.path.join(CURRENT_DIR, \"..\", \"..\", \"images\", \"outputs\", \"output.mp4\"),\n    model_id: str = \"KBlueLeaf/kohaku-v2.1\",\n    lora_dict: Optional[Dict[str, float]] = None,\n    prompt: str = \"1girl with brown dog ears, thick frame glasses\",\n    scale: float = 1.0,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"xformers\",\n    use_denoising_batch: bool = True,\n    enable_similar_image_filter: bool = True,\n    seed: int = 2,\n):\n\n    \"\"\"\n    Process for generating images based on a prompt using a specified model.\n\n    Parameters\n    ----------\n    input : str, optional\n        The input video name to load images from.\n    output : str, optional\n        The output video name to save images to.\n    model_id_or_path : str\n        The name of the model to use for image generation.\n    lora_dict : Optional[Dict[str, float]], optional\n        The lora_dict to load, by default None.\n        Keys are the LoRA names and values are the LoRA scales.\n        Example: {\"LoRA_1\" : 0.5 , \"LoRA_2\" : 0.7 ,...}\n    prompt : str\n        The prompt to generate images from.\n    scale : float, optional\n        The scale of the image, by default 1.0.\n    acceleration : Literal[\"none\", \"xformers\", \"tensorrt\"]\n        The type of acceleration to use for image generation.\n    use_denoising_batch : bool, optional\n        Whether to use denoising batch or not, by default True.\n    enable_similar_image_filter : bool, optional\n        Whether to enable similar image filter or not,\n        by default True.\n    seed : int, optional\n        The seed, by default 2. if -1, use random seed.\n    \"\"\"\n\n    video_info = read_video(input)\n    video = video_info[0] / 255\n    fps = video_info[2][\"video_fps\"]\n    width = int(video.shape[1] * scale)\n    height = int(video.shape[2] * scale)\n\n    stream = StreamDiffusionWrapper(\n        model_id_or_path=model_id,\n        lora_dict=lora_dict,\n        t_index_list=[35, 45],\n        frame_buffer_size=1,\n        width=width,\n        height=height,\n        warmup=10,\n        acceleration=acceleration,\n        do_add_noise=False,\n        mode=\"img2img\",\n        output_type=\"pt\",\n        enable_similar_image_filter=enable_similar_image_filter,\n        similar_image_filter_threshold=0.98,\n        use_denoising_batch=use_denoising_batch,\n        seed=seed,\n    )\n\n    stream.prepare(\n        prompt=prompt,\n        num_inference_steps=50,\n    )\n\n    video_result = torch.zeros(video.shape[0], width, height, 3)\n\n    for _ in range(stream.batch_size):\n        stream(image=video[0].permute(2, 0, 1))\n\n    for i in tqdm(range(video.shape[0])):\n        output_image = stream(video[i].permute(2, 0, 1))\n        video_result[i] = output_image.permute(1, 2, 0)\n\n    video_result = video_result * 255\n    write_video(output, video_result[2:], fps=fps)","file":"/examples/vid2vid/main.py","language":"python","fileHash":"1996541c8ece2a894a976f3fe12f789e105848bb99a0d5fdc70b70fc8962eb7a","hash":"383784b5a13cf56c29be1c2a78478f224eddb1a84e205eaec3adc422b2c97a32","processedContent":"def main(\n    input: str,\n    output: str = os.path.join(CURRENT_DIR, \"..\", \"..\", \"images\", \"outputs\", \"output.mp4\"),\n    \"\"\"\n    scip-python python python-stdlib 3.11 os/__init__:: undefined\n    scip-python python python-stdlib 3.11 os/path.: undefined\n    scip-python python python-stdlib 3.11 ntpath/join().: undefined\n    scip-python python temp indexer `examples.vid2vid.main`/CURRENT_DIR.: undefined\n    \"\"\"\n    model_id: str = \"KBlueLeaf/kohaku-v2.1\",\n    lora_dict: Optional[Dict[str, float]] = None,\n    \"\"\"\n    scip-python python python-stdlib 3.11 typing/Optional.: undefined\n    scip-python python python-stdlib 3.11 typing/Dict.: undefined\n    \"\"\"\n    prompt: str = \"1girl with brown dog ears, thick frame glasses\",\n    scale: float = 1.0,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"xformers\", #undefined\n    use_denoising_batch: bool = True,\n    enable_similar_image_filter: bool = True,\n    seed: int = 2,\n):\n\n    \"\"\"\n    Process for generating images based on a prompt using a specified model.\n\n    Parameters\n    ----------\n    input : str, optional\n        The input video name to load images from.\n    output : str, optional\n        The output video name to save images to.\n    model_id_or_path : str\n        The name of the model to use for image generation.\n    lora_dict : Optional[Dict[str, float]], optional\n        The lora_dict to load, by default None.\n        Keys are the LoRA names and values are the LoRA scales.\n        Example: {\"LoRA_1\" : 0.5 , \"LoRA_2\" : 0.7 ,...}\n    prompt : str\n        The prompt to generate images from.\n    scale : float, optional\n        The scale of the image, by default 1.0.\n    acceleration : Literal[\"none\", \"xformers\", \"tensorrt\"]\n        The type of acceleration to use for image generation.\n    use_denoising_batch : bool, optional\n        Whether to use denoising batch or not, by default True.\n    enable_similar_image_filter : bool, optional\n        Whether to enable similar image filter or not,\n        by default True.\n    seed : int, optional\n        The seed, by default 2. if -1, use random seed.\n    \"\"\"\n\n    video_info = read_video(input)\n    video = video_info[0] / 255\n    fps = video_info[2][\"video_fps\"]\n    width = int(video.shape[1] * scale)\n    height = int(video.shape[2] * scale)\n\n    stream = StreamDiffusionWrapper( #This code defines a class called `StreamDiffusionWrapper` that loads a model from a given path or ID, performs various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference. It also defines various methods for performing image generation and manipulation.\n        model_id_or_path=model_id,\n        lora_dict=lora_dict,\n        t_index_list=[35, 45],\n        frame_buffer_size=1,\n        width=width,\n        height=height,\n        warmup=10,\n        acceleration=acceleration,\n        do_add_noise=False,\n        mode=\"img2img\",\n        output_type=\"pt\",\n        enable_similar_image_filter=enable_similar_image_filter,\n        similar_image_filter_threshold=0.98,\n        use_denoising_batch=use_denoising_batch,\n        seed=seed,\n    )\n\n    stream.prepare( #This code prepares the model for inference by setting up the prompt, number of steps, and guidance scale.\n        prompt=prompt,\n        num_inference_steps=50,\n    )\n\n    video_result = torch.zeros(video.shape[0], width, height, 3)\n\n    for _ in range(stream.batch_size): #undefined\n        stream(image=video[0].permute(2, 0, 1))\n\n    for i in tqdm(range(video.shape[0])): #undefined\n        output_image = stream(video[i].permute(2, 0, 1))\n        video_result[i] = output_image.permute(1, 2, 0)\n\n    video_result = video_result * 255\n    write_video(output, video_result[2:], fps=fps)","documentation":"This code defines a function called `main` that takes in various parameters and generates images based on a prompt using a specified model. It uses a class called `StreamDiffusionWrapper` to perform image generation and manipulation, and it prepares the model for inference by setting up the prompt and number of steps."}},{"key":"scip-python python temp indexer `examples.vid2vid.main`/__init__:","attributes":{"symbol":"scip-python python temp indexer `examples.vid2vid.main`/__init__:","range":[0,0,103,0],"content":"import os\nimport sys\nfrom typing import Literal, Dict, Optional\n\nimport fire\nimport torch\nfrom torchvision.io import read_video, write_video\nfrom tqdm import tqdm\n\nsys.path.append(os.path.join(os.path.dirname(__file__), \"..\", \"..\"))\n\nfrom utils.wrapper import StreamDiffusionWrapper\n\nCURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n\n\ndef main(\n    input: str,\n    output: str = os.path.join(CURRENT_DIR, \"..\", \"..\", \"images\", \"outputs\", \"output.mp4\"),\n    model_id: str = \"KBlueLeaf/kohaku-v2.1\",\n    lora_dict: Optional[Dict[str, float]] = None,\n    prompt: str = \"1girl with brown dog ears, thick frame glasses\",\n    scale: float = 1.0,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"xformers\",\n    use_denoising_batch: bool = True,\n    enable_similar_image_filter: bool = True,\n    seed: int = 2,\n):\n\n    \"\"\"\n    Process for generating images based on a prompt using a specified model.\n\n    Parameters\n    ----------\n    input : str, optional\n        The input video name to load images from.\n    output : str, optional\n        The output video name to save images to.\n    model_id_or_path : str\n        The name of the model to use for image generation.\n    lora_dict : Optional[Dict[str, float]], optional\n        The lora_dict to load, by default None.\n        Keys are the LoRA names and values are the LoRA scales.\n        Example: {\"LoRA_1\" : 0.5 , \"LoRA_2\" : 0.7 ,...}\n    prompt : str\n        The prompt to generate images from.\n    scale : float, optional\n        The scale of the image, by default 1.0.\n    acceleration : Literal[\"none\", \"xformers\", \"tensorrt\"]\n        The type of acceleration to use for image generation.\n    use_denoising_batch : bool, optional\n        Whether to use denoising batch or not, by default True.\n    enable_similar_image_filter : bool, optional\n        Whether to enable similar image filter or not,\n        by default True.\n    seed : int, optional\n        The seed, by default 2. if -1, use random seed.\n    \"\"\"\n\n    video_info = read_video(input)\n    video = video_info[0] / 255\n    fps = video_info[2][\"video_fps\"]\n    width = int(video.shape[1] * scale)\n    height = int(video.shape[2] * scale)\n\n    stream = StreamDiffusionWrapper(\n        model_id_or_path=model_id,\n        lora_dict=lora_dict,\n        t_index_list=[35, 45],\n        frame_buffer_size=1,\n        width=width,\n        height=height,\n        warmup=10,\n        acceleration=acceleration,\n        do_add_noise=False,\n        mode=\"img2img\",\n        output_type=\"pt\",\n        enable_similar_image_filter=enable_similar_image_filter,\n        similar_image_filter_threshold=0.98,\n        use_denoising_batch=use_denoising_batch,\n        seed=seed,\n    )\n\n    stream.prepare(\n        prompt=prompt,\n        num_inference_steps=50,\n    )\n\n    video_result = torch.zeros(video.shape[0], width, height, 3)\n\n    for _ in range(stream.batch_size):\n        stream(image=video[0].permute(2, 0, 1))\n\n    for i in tqdm(range(video.shape[0])):\n        output_image = stream(video[i].permute(2, 0, 1))\n        video_result[i] = output_image.permute(1, 2, 0)\n\n    video_result = video_result * 255\n    write_video(output, video_result[2:], fps=fps)\n\n\nif __name__ == \"__main__\":\n    fire.Fire(main)\n","file":"/examples/vid2vid/main.py","language":"python","fileHash":"1996541c8ece2a894a976f3fe12f789e105848bb99a0d5fdc70b70fc8962eb7a","hash":"1996541c8ece2a894a976f3fe12f789e105848bb99a0d5fdc70b70fc8962eb7a","processedContent":"import os #undefined\nimport sys #undefined\nfrom typing import Literal, Dict, Optional\n\"\"\"\nscip-python python python-stdlib 3.11 typing/__init__:: undefined\nscip-python python python-stdlib 3.11 typing/Literal.: undefined\nscip-python python python-stdlib 3.11 typing/Dict.: undefined\nscip-python python python-stdlib 3.11 typing/Optional.: undefined\n\"\"\"\n\nimport fire\nimport torch\nfrom torchvision.io import read_video, write_video #undefined\nfrom tqdm import tqdm\n\"\"\"\nscip-python python tqdm 4.66.1 tqdm/__init__:: undefined\nscip-python python tqdm 4.66.1 `tqdm.std`/tqdm#: undefined\n\"\"\"\n\nsys.path.append(os.path.join(os.path.dirname(__file__), \"..\", \"..\"))\n\"\"\"\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 sys/__init__:: undefined\nscip-python python python-stdlib 3.11 sys/path.path.: undefined\nscip-python python python-stdlib 3.11 os/path.: undefined\nscip-python python python-stdlib 3.11 os/path.: undefined\nscip-python python python-stdlib 3.11 ntpath/join().: undefined\nscip-python python python-stdlib 3.11 ntpath/join().: undefined\n\"\"\"\n\nfrom utils.wrapper import StreamDiffusionWrapper\n\"\"\"\nscip-python python temp indexer `utils.wrapper`/__init__:: This code defines a class called `StreamDiffusionWrapper` that loads a model for image generation and manipulation, enabling acceleration, and preparing the model for inference.\nscip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#: This code defines a class called `StreamDiffusionWrapper` that loads a model from a given path or ID, performs various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference. It also defines various methods for performing image generation and manipulation.\n\"\"\"\n\nCURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n\"\"\"\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 os/path.: undefined\nscip-python python python-stdlib 3.11 os/path.: undefined\nscip-python python python-stdlib 3.11 ntpath/join().: undefined\nscip-python python python-stdlib 3.11 ntpath/join().: undefined\nscip-python python temp indexer `examples.vid2vid.main`/: undefined\n\"\"\"\n\n\ndef main(\n    \"\"\"This code defines a function called `main` that takes in various parameters and generates images based on a prompt using a specified model. It uses a class called `StreamDiffusionWrapper` to perform image generation and manipulation, and it prepares the model for inference by setting up the prompt and number of steps.\"\"\"\n    pass\n\n\nif __name__ == \"__main__\": #undefined\n    fire.Fire(main) #This code defines a function called `main` that takes in various parameters and generates images based on a prompt using a specified model. It uses a class called `StreamDiffusionWrapper` to perform image generation and manipulation, and it prepares the model for inference by setting up the prompt and number of steps.\n","documentation":"This code defines a function called `main` that generates images based on a prompt using a specified model, using a class called `StreamDiffusionWrapper` to perform image generation and manipulation."}},{"key":"scip-python python temp indexer `torchvision.io`/__init__:","attributes":{"symbol":"scip-python python temp indexer `torchvision.io`/__init__:","language":"python"}},{"key":"scip-python python temp indexer `examples.vid2vid.main`/","attributes":{"symbol":"scip-python python temp indexer `examples.vid2vid.main`/","language":"python"}},{"key":"scip-python python temp indexer `examples.vid2vid.main`/CURRENT_DIR.","attributes":{"symbol":"scip-python python temp indexer `examples.vid2vid.main`/CURRENT_DIR.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion`/__init__:","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion`/__init__:","range":[0,0,1,0],"content":"from .pipeline import StreamDiffusion\n","file":"/src/streamdiffusion/__init__.py","language":"python","fileHash":"92d2b82559718c287ff30d693e962592affef21a24b5ee01e1972c766206df04","hash":"92d2b82559718c287ff30d693e962592affef21a24b5ee01e1972c766206df04","processedContent":"from .pipeline import StreamDiffusion\n\"\"\"\nscip-python python temp indexer `src.streamdiffusion.pipeline`/__init__:: The code defines a class called StreamDiffusion that performs image denoising using a VAE model, with functions for loading pre-trained weights, fusing the LoRA model with other models, and enabling or disabling a similar image filter.\nscip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#: The code defines a class called StreamDiffusion that performs image denoising using a VAE model. It also includes functions for loading pre-trained weights, fusing the LoRA model with other models, and enabling or disabling a similar image filter.\n\"\"\"\n","documentation":"The code defines a class called StreamDiffusion that performs image denoising using a VAE model, with functions for loading pre-trained weights, fusing the LoRA model with other models, and enabling or disabling a similar image filter."}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/__init__:","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/__init__:","language":"python","range":[0,0,496,0],"content":"import time\nfrom typing import List, Optional, Union, Any, Dict, Tuple, Literal\n\nimport numpy as np\nimport PIL.Image\nimport torch\nfrom diffusers import LCMScheduler, StableDiffusionPipeline\nfrom diffusers.image_processor import VaeImageProcessor\nfrom diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img import (\n    retrieve_latents,\n)\n\nfrom streamdiffusion.image_filter import SimilarImageFilter\n\n\nclass StreamDiffusion:\n    def __init__(\n        self,\n        pipe: StableDiffusionPipeline,\n        t_index_list: List[int],\n        torch_dtype: torch.dtype = torch.float16,\n        width: int = 512,\n        height: int = 512,\n        do_add_noise: bool = True,\n        use_denoising_batch: bool = True,\n        frame_buffer_size: int = 1,\n        cfg_type: Literal[\"none\", \"full\", \"self\", \"initialize\"] = \"self\",\n    ) -> None:\n        self.device = pipe.device\n        self.dtype = torch_dtype\n        self.generator = None\n\n        self.height = height\n        self.width = width\n\n        self.latent_height = int(height // pipe.vae_scale_factor)\n        self.latent_width = int(width // pipe.vae_scale_factor)\n\n        self.frame_bff_size = frame_buffer_size\n        self.denoising_steps_num = len(t_index_list)\n\n        self.cfg_type = cfg_type\n\n        if use_denoising_batch:\n            self.batch_size = self.denoising_steps_num * frame_buffer_size\n            if self.cfg_type == \"initialize\":\n                self.trt_unet_batch_size = (\n                    self.denoising_steps_num + 1\n                ) * self.frame_bff_size\n            elif self.cfg_type == \"full\":\n                self.trt_unet_batch_size = (\n                    2 * self.denoising_steps_num * self.frame_bff_size\n                )\n            else:\n                self.trt_unet_batch_size = self.denoising_steps_num * frame_buffer_size\n        else:\n            self.trt_unet_batch_size = self.frame_bff_size\n            self.batch_size = frame_buffer_size\n\n        self.t_list = t_index_list\n\n        self.do_add_noise = do_add_noise\n        self.use_denoising_batch = use_denoising_batch\n\n        self.similar_image_filter = False\n        self.similar_filter = SimilarImageFilter()\n        self.prev_image_result = None\n\n        self.pipe = pipe\n        self.image_processor = VaeImageProcessor(pipe.vae_scale_factor)\n\n        self.scheduler = LCMScheduler.from_config(self.pipe.scheduler.config)\n        self.text_encoder = pipe.text_encoder\n        self.unet = pipe.unet\n        self.vae = pipe.vae\n\n        self.inference_time_ema = 0\n\n    def load_lcm_lora(\n        self,\n        pretrained_model_name_or_path_or_dict: Union[\n            str, Dict[str, torch.Tensor]\n        ] = \"latent-consistency/lcm-lora-sdv1-5\",\n        adapter_name: Optional[Any] = None,\n        **kwargs,\n    ) -> None:\n        self.pipe.load_lora_weights(\n            pretrained_model_name_or_path_or_dict, adapter_name, **kwargs\n        )\n\n    def load_lora(\n        self,\n        pretrained_lora_model_name_or_path_or_dict: Union[str, Dict[str, torch.Tensor]],\n        adapter_name: Optional[Any] = None,\n        **kwargs,\n    ) -> None:\n        self.pipe.load_lora_weights(\n            pretrained_lora_model_name_or_path_or_dict, adapter_name, **kwargs\n        )\n\n    def fuse_lora(\n        self,\n        fuse_unet: bool = True,\n        fuse_text_encoder: bool = True,\n        lora_scale: float = 1.0,\n        safe_fusing: bool = False,\n    ) -> None:\n        self.pipe.fuse_lora(\n            fuse_unet=fuse_unet,\n            fuse_text_encoder=fuse_text_encoder,\n            lora_scale=lora_scale,\n            safe_fusing=safe_fusing,\n        )\n\n    def enable_similar_image_filter(self, threshold: float = 0.98, max_skip_frame: float = 10) -> None:\n        self.similar_image_filter = True\n        self.similar_filter.set_threshold(threshold)\n        self.similar_filter.set_max_skip_frame(max_skip_frame)\n\n    def disable_similar_image_filter(self) -> None:\n        self.similar_image_filter = False\n\n    @torch.no_grad()\n    def prepare(\n        self,\n        prompt: str,\n        negative_prompt: str = \"\",\n        num_inference_steps: int = 50,\n        guidance_scale: float = 1.2,\n        delta: float = 1.0,\n        generator: Optional[torch.Generator] = torch.Generator(),\n        seed: int = 2,\n    ) -> None:\n        self.generator = generator\n        self.generator.manual_seed(seed)\n        # initialize x_t_latent (it can be any random tensor)\n        if self.denoising_steps_num > 1:\n            self.x_t_latent_buffer = torch.zeros(\n                (\n                    (self.denoising_steps_num - 1) * self.frame_bff_size,\n                    4,\n                    self.latent_height,\n                    self.latent_width,\n                ),\n                dtype=self.dtype,\n                device=self.device,\n            )\n        else:\n            self.x_t_latent_buffer = None\n\n        if self.cfg_type == \"none\":\n            self.guidance_scale = 1.0\n        else:\n            self.guidance_scale = guidance_scale\n        self.delta = delta\n\n        do_classifier_free_guidance = False\n        if self.guidance_scale > 1.0:\n            do_classifier_free_guidance = True\n\n        encoder_output = self.pipe.encode_prompt(\n            prompt=prompt,\n            device=self.device,\n            num_images_per_prompt=1,\n            do_classifier_free_guidance=do_classifier_free_guidance,\n            negative_prompt=negative_prompt,\n        )\n        self.prompt_embeds = encoder_output[0].repeat(self.batch_size, 1, 1)\n\n        if self.use_denoising_batch and self.cfg_type == \"full\":\n            uncond_prompt_embeds = encoder_output[1].repeat(self.batch_size, 1, 1)\n        elif self.cfg_type == \"initialize\":\n            uncond_prompt_embeds = encoder_output[1].repeat(self.frame_bff_size, 1, 1)\n\n        if self.guidance_scale > 1.0 and (\n            self.cfg_type == \"initialize\" or self.cfg_type == \"full\"\n        ):\n            self.prompt_embeds = torch.cat(\n                [uncond_prompt_embeds, self.prompt_embeds], dim=0\n            )\n\n        self.scheduler.set_timesteps(num_inference_steps, self.device)\n        self.timesteps = self.scheduler.timesteps.to(self.device)\n\n        # make sub timesteps list based on the indices in the t_list list and the values in the timesteps list\n        self.sub_timesteps = []\n        for t in self.t_list:\n            self.sub_timesteps.append(self.timesteps[t])\n\n        sub_timesteps_tensor = torch.tensor(\n            self.sub_timesteps, dtype=torch.long, device=self.device\n        )\n        self.sub_timesteps_tensor = torch.repeat_interleave(\n            sub_timesteps_tensor,\n            repeats=self.frame_bff_size if self.use_denoising_batch else 1,\n            dim=0,\n        )\n\n        self.init_noise = torch.randn(\n            (self.batch_size, 4, self.latent_height, self.latent_width),\n            generator=generator,\n        ).to(device=self.device, dtype=self.dtype)\n\n        self.stock_noise = torch.zeros_like(self.init_noise)\n\n        c_skip_list = []\n        c_out_list = []\n        for timestep in self.sub_timesteps:\n            c_skip, c_out = self.scheduler.get_scalings_for_boundary_condition_discrete(\n                timestep\n            )\n            c_skip_list.append(c_skip)\n            c_out_list.append(c_out)\n\n        self.c_skip = (\n            torch.stack(c_skip_list)\n            .view(len(self.t_list), 1, 1, 1)\n            .to(dtype=self.dtype, device=self.device)\n        )\n        self.c_out = (\n            torch.stack(c_out_list)\n            .view(len(self.t_list), 1, 1, 1)\n            .to(dtype=self.dtype, device=self.device)\n        )\n\n        alpha_prod_t_sqrt_list = []\n        beta_prod_t_sqrt_list = []\n        for timestep in self.sub_timesteps:\n            alpha_prod_t_sqrt = self.scheduler.alphas_cumprod[timestep].sqrt()\n            beta_prod_t_sqrt = (1 - self.scheduler.alphas_cumprod[timestep]).sqrt()\n            alpha_prod_t_sqrt_list.append(alpha_prod_t_sqrt)\n            beta_prod_t_sqrt_list.append(beta_prod_t_sqrt)\n        alpha_prod_t_sqrt = (\n            torch.stack(alpha_prod_t_sqrt_list)\n            .view(len(self.t_list), 1, 1, 1)\n            .to(dtype=self.dtype, device=self.device)\n        )\n        beta_prod_t_sqrt = (\n            torch.stack(beta_prod_t_sqrt_list)\n            .view(len(self.t_list), 1, 1, 1)\n            .to(dtype=self.dtype, device=self.device)\n        )\n        self.alpha_prod_t_sqrt = torch.repeat_interleave(\n            alpha_prod_t_sqrt,\n            repeats=self.frame_bff_size if self.use_denoising_batch else 1,\n            dim=0,\n        )\n        self.beta_prod_t_sqrt = torch.repeat_interleave(\n            beta_prod_t_sqrt,\n            repeats=self.frame_bff_size if self.use_denoising_batch else 1,\n            dim=0,\n        )\n\n    @torch.no_grad()\n    def update_prompt(self, prompt: str) -> None:\n        encoder_output = self.pipe.encode_prompt(\n            prompt=prompt,\n            device=self.device,\n            num_images_per_prompt=1,\n            do_classifier_free_guidance=False,\n        )\n        self.prompt_embeds = encoder_output[0].repeat(self.batch_size, 1, 1)\n\n    def add_noise(\n        self,\n        original_samples: torch.Tensor,\n        noise: torch.Tensor,\n        t_index: int,\n    ) -> torch.Tensor:\n        noisy_samples = (\n            self.alpha_prod_t_sqrt[t_index] * original_samples\n            + self.beta_prod_t_sqrt[t_index] * noise\n        )\n        return noisy_samples\n\n    def scheduler_step_batch(\n        self,\n        model_pred_batch: torch.Tensor,\n        x_t_latent_batch: torch.Tensor,\n        idx: Optional[int] = None,\n    ) -> torch.Tensor:\n        # TODO: use t_list to select beta_prod_t_sqrt\n        if idx is None:\n            F_theta = (\n                x_t_latent_batch - self.beta_prod_t_sqrt * model_pred_batch\n            ) / self.alpha_prod_t_sqrt\n            denoised_batch = self.c_out * F_theta + self.c_skip * x_t_latent_batch\n        else:\n            F_theta = (\n                x_t_latent_batch - self.beta_prod_t_sqrt[idx] * model_pred_batch\n            ) / self.alpha_prod_t_sqrt[idx]\n            denoised_batch = (\n                self.c_out[idx] * F_theta + self.c_skip[idx] * x_t_latent_batch\n            )\n\n        return denoised_batch\n\n    def unet_step(\n        self,\n        x_t_latent: torch.Tensor,\n        t_list: Union[torch.Tensor, list[int]],\n        idx: Optional[int] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        if self.guidance_scale > 1.0 and (self.cfg_type == \"initialize\"):\n            x_t_latent_plus_uc = torch.concat([x_t_latent[0:1], x_t_latent], dim=0)\n            t_list = torch.concat([t_list[0:1], t_list], dim=0)\n        elif self.guidance_scale > 1.0 and (self.cfg_type == \"full\"):\n            x_t_latent_plus_uc = torch.concat([x_t_latent, x_t_latent], dim=0)\n            t_list = torch.concat([t_list, t_list], dim=0)\n        else:\n            x_t_latent_plus_uc = x_t_latent\n\n        model_pred = self.unet(\n            x_t_latent_plus_uc,\n            t_list,\n            encoder_hidden_states=self.prompt_embeds,\n            return_dict=False,\n        )[0]\n\n        if self.guidance_scale > 1.0 and (self.cfg_type == \"initialize\"):\n            noise_pred_text = model_pred[1:]\n            self.stock_noise = torch.concat(\n                [model_pred[0:1], self.stock_noise[1:]], dim=0\n            )  # ここコメントアウトでself out cfg\n        elif self.guidance_scale > 1.0 and (self.cfg_type == \"full\"):\n            noise_pred_uncond, noise_pred_text = model_pred.chunk(2)\n        else:\n            noise_pred_text = model_pred\n        if self.guidance_scale > 1.0 and (\n            self.cfg_type == \"self\" or self.cfg_type == \"initialize\"\n        ):\n            noise_pred_uncond = self.stock_noise * self.delta\n        if self.guidance_scale > 1.0 and self.cfg_type != \"none\":\n            model_pred = noise_pred_uncond + self.guidance_scale * (\n                noise_pred_text - noise_pred_uncond\n            )\n        else:\n            model_pred = noise_pred_text\n\n        # compute the previous noisy sample x_t -> x_t-1\n        if self.use_denoising_batch:\n            denoised_batch = self.scheduler_step_batch(model_pred, x_t_latent, idx)\n            if self.cfg_type == \"self\" or self.cfg_type == \"initialize\":\n                scaled_noise = self.beta_prod_t_sqrt * self.stock_noise\n                delta_x = self.scheduler_step_batch(model_pred, scaled_noise, idx)\n                alpha_next = torch.concat(\n                    [\n                        self.alpha_prod_t_sqrt[1:],\n                        torch.ones_like(self.alpha_prod_t_sqrt[0:1]),\n                    ],\n                    dim=0,\n                )\n                delta_x = alpha_next * delta_x\n                beta_next = torch.concat(\n                    [\n                        self.beta_prod_t_sqrt[1:],\n                        torch.ones_like(self.beta_prod_t_sqrt[0:1]),\n                    ],\n                    dim=0,\n                )\n                delta_x = delta_x / beta_next\n                init_noise = torch.concat(\n                    [self.init_noise[1:], self.init_noise[0:1]], dim=0\n                )\n                self.stock_noise = init_noise + delta_x\n\n        else:\n            # denoised_batch = self.scheduler.step(model_pred, t_list[0], x_t_latent).denoised\n            denoised_batch = self.scheduler_step_batch(model_pred, x_t_latent, idx)\n\n        return denoised_batch, model_pred\n\n    def encode_image(self, image_tensors: torch.Tensor) -> torch.Tensor:\n        image_tensors = image_tensors.to(\n            device=self.device,\n            dtype=self.vae.dtype,\n        )\n        img_latent = retrieve_latents(self.vae.encode(image_tensors), self.generator)\n        img_latent = img_latent * self.vae.config.scaling_factor\n        x_t_latent = self.add_noise(img_latent, self.init_noise[0], 0)\n        return x_t_latent\n\n    def decode_image(self, x_0_pred_out: torch.Tensor) -> torch.Tensor:\n        output_latent = self.vae.decode(\n            x_0_pred_out / self.vae.config.scaling_factor, return_dict=False\n        )[0]\n        return output_latent\n\n    def predict_x0_batch(self, x_t_latent: torch.Tensor) -> torch.Tensor:\n        prev_latent_batch = self.x_t_latent_buffer\n\n        if self.use_denoising_batch:\n            t_list = self.sub_timesteps_tensor\n            if self.denoising_steps_num > 1:\n                x_t_latent = torch.cat((x_t_latent, prev_latent_batch), dim=0)\n                self.stock_noise = torch.cat(\n                    (self.init_noise[0:1], self.stock_noise[:-1]), dim=0\n                )\n            x_0_pred_batch, model_pred = self.unet_step(x_t_latent, t_list)\n\n            if self.denoising_steps_num > 1:\n                x_0_pred_out = x_0_pred_batch[-1].unsqueeze(0)\n                if self.do_add_noise:\n                    self.x_t_latent_buffer = (\n                        self.alpha_prod_t_sqrt[1:] * x_0_pred_batch[:-1]\n                        + self.beta_prod_t_sqrt[1:] * self.init_noise[1:]\n                    )\n                else:\n                    self.x_t_latent_buffer = (\n                        self.alpha_prod_t_sqrt[1:] * x_0_pred_batch[:-1]\n                    )\n            else:\n                x_0_pred_out = x_0_pred_batch\n                self.x_t_latent_buffer = None\n        else:\n            self.init_noise = x_t_latent\n            for idx, t in enumerate(self.sub_timesteps_tensor):\n                t = t.view(\n                    1,\n                ).repeat(\n                    self.frame_bff_size,\n                )\n                x_0_pred, model_pred = self.unet_step(x_t_latent, t, idx)\n                if idx < len(self.sub_timesteps_tensor) - 1:\n                    if self.do_add_noise:\n                        x_t_latent = self.alpha_prod_t_sqrt[\n                            idx + 1\n                        ] * x_0_pred + self.beta_prod_t_sqrt[\n                            idx + 1\n                        ] * torch.randn_like(\n                            x_0_pred, device=self.device, dtype=self.dtype\n                        )\n                    else:\n                        x_t_latent = self.alpha_prod_t_sqrt[idx + 1] * x_0_pred\n            x_0_pred_out = x_0_pred\n\n        return x_0_pred_out\n\n    @torch.no_grad()\n    def __call__(\n        self, x: Union[torch.Tensor, PIL.Image.Image, np.ndarray] = None\n    ) -> torch.Tensor:\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        start.record()\n        if x is not None:\n            x = self.image_processor.preprocess(x, self.height, self.width).to(\n                device=self.device, dtype=self.dtype\n            )\n            if self.similar_image_filter:\n                x = self.similar_filter(x)\n                if x is None:\n                    time.sleep(self.inference_time_ema)\n                    return self.prev_image_result\n            x_t_latent = self.encode_image(x)\n        else:\n            # TODO: check the dimension of x_t_latent\n            x_t_latent = torch.randn((1, 4, self.latent_height, self.latent_width)).to(\n                device=self.device, dtype=self.dtype\n            )\n        x_0_pred_out = self.predict_x0_batch(x_t_latent)\n        x_output = self.decode_image(x_0_pred_out).detach().clone()\n\n        self.prev_image_result = x_output\n        end.record()\n        torch.cuda.synchronize()\n        inference_time = start.elapsed_time(end) / 1000\n        self.inference_time_ema = 0.9 * self.inference_time_ema + 0.1 * inference_time\n        return x_output\n\n    @torch.no_grad()\n    def txt2img(self, batch_size: int = 1) -> torch.Tensor:\n        x_0_pred_out = self.predict_x0_batch(\n            torch.randn((batch_size, 4, self.latent_height, self.latent_width)).to(\n                device=self.device, dtype=self.dtype\n            )\n        )\n        x_output = self.decode_image(x_0_pred_out).detach().clone()\n        return x_output\n\n    def txt2img_sd_turbo(self, batch_size: int = 1) -> torch.Tensor:\n        x_t_latent = torch.randn(\n            (batch_size, 4, self.latent_height, self.latent_width),\n            device=self.device,\n            dtype=self.dtype,\n        )\n        model_pred = self.unet(\n            x_t_latent,\n            self.sub_timesteps_tensor,\n            encoder_hidden_states=self.prompt_embeds,\n            return_dict=False,\n        )[0]\n        x_0_pred_out = (\n            x_t_latent - self.beta_prod_t_sqrt * model_pred\n        ) / self.alpha_prod_t_sqrt\n        return self.decode_image(x_0_pred_out)\n","file":"/src/streamdiffusion/pipeline.py","fileHash":"a2ae37545d7ab715dd08b4f49d75565816a53cbafc1457eb03e2c61597cd6eba","hash":"a2ae37545d7ab715dd08b4f49d75565816a53cbafc1457eb03e2c61597cd6eba","processedContent":"import time #undefined\nfrom typing import List, Optional, Union, Any, Dict, Tuple, Literal\n\"\"\"\nscip-python python python-stdlib 3.11 typing/__init__:: undefined\nscip-python python python-stdlib 3.11 typing/List.: undefined\nscip-python python python-stdlib 3.11 typing/Optional.: undefined\nscip-python python python-stdlib 3.11 typing/Union.: undefined\nscip-python python python-stdlib 3.11 typing/Any.: undefined\nscip-python python python-stdlib 3.11 typing/Dict.: undefined\nscip-python python python-stdlib 3.11 typing/Tuple.: undefined\nscip-python python python-stdlib 3.11 typing/Literal.: undefined\n\"\"\"\n\nimport numpy as np #undefined\nimport PIL.Image #undefined\nimport torch\nfrom diffusers import LCMScheduler, StableDiffusionPipeline #undefined\nfrom diffusers.image_processor import VaeImageProcessor #undefined\nfrom diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img import ( #undefined\n    retrieve_latents,\n)\n\nfrom streamdiffusion.image_filter import SimilarImageFilter\n\"\"\"\nscip-python python temp indexer `src.streamdiffusion.image_filter`/__init__:: This code defines a class called SimilarImageFilter that compares two tensors based on cosine similarity and skips frames if the similarity is below a threshold.\nscip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#: This code defines a class called SimilarImageFilter that initializes a threshold value and a maximum number of frames to skip when comparing two tensors. It also sets up a cosine similarity metric and a counter for the number of frames skipped. The class has a __call__ function that takes a tensor as input and returns a new tensor based on cosine similarity, and two additional functions to set the threshold and maximum frame skip values.\n\"\"\"\n\n\nclass StreamDiffusion:\n    \"\"\"The code defines a class called StreamDiffusion that performs image denoising using a VAE model. It also includes functions for loading pre-trained weights, fusing the LoRA model with other models, and enabling or disabling a similar image filter.\"\"\"\n    pass\n","documentation":"The code defines a class called StreamDiffusion that performs image denoising using a VAE model, with functions for loading pre-trained weights, fusing the LoRA model with other models, and enabling or disabling a similar image filter."}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#","language":"python","range":[15,0,495,46],"content":"class StreamDiffusion:\n    def __init__(\n        self,\n        pipe: StableDiffusionPipeline,\n        t_index_list: List[int],\n        torch_dtype: torch.dtype = torch.float16,\n        width: int = 512,\n        height: int = 512,\n        do_add_noise: bool = True,\n        use_denoising_batch: bool = True,\n        frame_buffer_size: int = 1,\n        cfg_type: Literal[\"none\", \"full\", \"self\", \"initialize\"] = \"self\",\n    ) -> None:\n        self.device = pipe.device\n        self.dtype = torch_dtype\n        self.generator = None\n\n        self.height = height\n        self.width = width\n\n        self.latent_height = int(height // pipe.vae_scale_factor)\n        self.latent_width = int(width // pipe.vae_scale_factor)\n\n        self.frame_bff_size = frame_buffer_size\n        self.denoising_steps_num = len(t_index_list)\n\n        self.cfg_type = cfg_type\n\n        if use_denoising_batch:\n            self.batch_size = self.denoising_steps_num * frame_buffer_size\n            if self.cfg_type == \"initialize\":\n                self.trt_unet_batch_size = (\n                    self.denoising_steps_num + 1\n                ) * self.frame_bff_size\n            elif self.cfg_type == \"full\":\n                self.trt_unet_batch_size = (\n                    2 * self.denoising_steps_num * self.frame_bff_size\n                )\n            else:\n                self.trt_unet_batch_size = self.denoising_steps_num * frame_buffer_size\n        else:\n            self.trt_unet_batch_size = self.frame_bff_size\n            self.batch_size = frame_buffer_size\n\n        self.t_list = t_index_list\n\n        self.do_add_noise = do_add_noise\n        self.use_denoising_batch = use_denoising_batch\n\n        self.similar_image_filter = False\n        self.similar_filter = SimilarImageFilter()\n        self.prev_image_result = None\n\n        self.pipe = pipe\n        self.image_processor = VaeImageProcessor(pipe.vae_scale_factor)\n\n        self.scheduler = LCMScheduler.from_config(self.pipe.scheduler.config)\n        self.text_encoder = pipe.text_encoder\n        self.unet = pipe.unet\n        self.vae = pipe.vae\n\n        self.inference_time_ema = 0\n\n    def load_lcm_lora(\n        self,\n        pretrained_model_name_or_path_or_dict: Union[\n            str, Dict[str, torch.Tensor]\n        ] = \"latent-consistency/lcm-lora-sdv1-5\",\n        adapter_name: Optional[Any] = None,\n        **kwargs,\n    ) -> None:\n        self.pipe.load_lora_weights(\n            pretrained_model_name_or_path_or_dict, adapter_name, **kwargs\n        )\n\n    def load_lora(\n        self,\n        pretrained_lora_model_name_or_path_or_dict: Union[str, Dict[str, torch.Tensor]],\n        adapter_name: Optional[Any] = None,\n        **kwargs,\n    ) -> None:\n        self.pipe.load_lora_weights(\n            pretrained_lora_model_name_or_path_or_dict, adapter_name, **kwargs\n        )\n\n    def fuse_lora(\n        self,\n        fuse_unet: bool = True,\n        fuse_text_encoder: bool = True,\n        lora_scale: float = 1.0,\n        safe_fusing: bool = False,\n    ) -> None:\n        self.pipe.fuse_lora(\n            fuse_unet=fuse_unet,\n            fuse_text_encoder=fuse_text_encoder,\n            lora_scale=lora_scale,\n            safe_fusing=safe_fusing,\n        )\n\n    def enable_similar_image_filter(self, threshold: float = 0.98, max_skip_frame: float = 10) -> None:\n        self.similar_image_filter = True\n        self.similar_filter.set_threshold(threshold)\n        self.similar_filter.set_max_skip_frame(max_skip_frame)\n\n    def disable_similar_image_filter(self) -> None:\n        self.similar_image_filter = False\n\n    @torch.no_grad()\n    def prepare(\n        self,\n        prompt: str,\n        negative_prompt: str = \"\",\n        num_inference_steps: int = 50,\n        guidance_scale: float = 1.2,\n        delta: float = 1.0,\n        generator: Optional[torch.Generator] = torch.Generator(),\n        seed: int = 2,\n    ) -> None:\n        self.generator = generator\n        self.generator.manual_seed(seed)\n        # initialize x_t_latent (it can be any random tensor)\n        if self.denoising_steps_num > 1:\n            self.x_t_latent_buffer = torch.zeros(\n                (\n                    (self.denoising_steps_num - 1) * self.frame_bff_size,\n                    4,\n                    self.latent_height,\n                    self.latent_width,\n                ),\n                dtype=self.dtype,\n                device=self.device,\n            )\n        else:\n            self.x_t_latent_buffer = None\n\n        if self.cfg_type == \"none\":\n            self.guidance_scale = 1.0\n        else:\n            self.guidance_scale = guidance_scale\n        self.delta = delta\n\n        do_classifier_free_guidance = False\n        if self.guidance_scale > 1.0:\n            do_classifier_free_guidance = True\n\n        encoder_output = self.pipe.encode_prompt(\n            prompt=prompt,\n            device=self.device,\n            num_images_per_prompt=1,\n            do_classifier_free_guidance=do_classifier_free_guidance,\n            negative_prompt=negative_prompt,\n        )\n        self.prompt_embeds = encoder_output[0].repeat(self.batch_size, 1, 1)\n\n        if self.use_denoising_batch and self.cfg_type == \"full\":\n            uncond_prompt_embeds = encoder_output[1].repeat(self.batch_size, 1, 1)\n        elif self.cfg_type == \"initialize\":\n            uncond_prompt_embeds = encoder_output[1].repeat(self.frame_bff_size, 1, 1)\n\n        if self.guidance_scale > 1.0 and (\n            self.cfg_type == \"initialize\" or self.cfg_type == \"full\"\n        ):\n            self.prompt_embeds = torch.cat(\n                [uncond_prompt_embeds, self.prompt_embeds], dim=0\n            )\n\n        self.scheduler.set_timesteps(num_inference_steps, self.device)\n        self.timesteps = self.scheduler.timesteps.to(self.device)\n\n        # make sub timesteps list based on the indices in the t_list list and the values in the timesteps list\n        self.sub_timesteps = []\n        for t in self.t_list:\n            self.sub_timesteps.append(self.timesteps[t])\n\n        sub_timesteps_tensor = torch.tensor(\n            self.sub_timesteps, dtype=torch.long, device=self.device\n        )\n        self.sub_timesteps_tensor = torch.repeat_interleave(\n            sub_timesteps_tensor,\n            repeats=self.frame_bff_size if self.use_denoising_batch else 1,\n            dim=0,\n        )\n\n        self.init_noise = torch.randn(\n            (self.batch_size, 4, self.latent_height, self.latent_width),\n            generator=generator,\n        ).to(device=self.device, dtype=self.dtype)\n\n        self.stock_noise = torch.zeros_like(self.init_noise)\n\n        c_skip_list = []\n        c_out_list = []\n        for timestep in self.sub_timesteps:\n            c_skip, c_out = self.scheduler.get_scalings_for_boundary_condition_discrete(\n                timestep\n            )\n            c_skip_list.append(c_skip)\n            c_out_list.append(c_out)\n\n        self.c_skip = (\n            torch.stack(c_skip_list)\n            .view(len(self.t_list), 1, 1, 1)\n            .to(dtype=self.dtype, device=self.device)\n        )\n        self.c_out = (\n            torch.stack(c_out_list)\n            .view(len(self.t_list), 1, 1, 1)\n            .to(dtype=self.dtype, device=self.device)\n        )\n\n        alpha_prod_t_sqrt_list = []\n        beta_prod_t_sqrt_list = []\n        for timestep in self.sub_timesteps:\n            alpha_prod_t_sqrt = self.scheduler.alphas_cumprod[timestep].sqrt()\n            beta_prod_t_sqrt = (1 - self.scheduler.alphas_cumprod[timestep]).sqrt()\n            alpha_prod_t_sqrt_list.append(alpha_prod_t_sqrt)\n            beta_prod_t_sqrt_list.append(beta_prod_t_sqrt)\n        alpha_prod_t_sqrt = (\n            torch.stack(alpha_prod_t_sqrt_list)\n            .view(len(self.t_list), 1, 1, 1)\n            .to(dtype=self.dtype, device=self.device)\n        )\n        beta_prod_t_sqrt = (\n            torch.stack(beta_prod_t_sqrt_list)\n            .view(len(self.t_list), 1, 1, 1)\n            .to(dtype=self.dtype, device=self.device)\n        )\n        self.alpha_prod_t_sqrt = torch.repeat_interleave(\n            alpha_prod_t_sqrt,\n            repeats=self.frame_bff_size if self.use_denoising_batch else 1,\n            dim=0,\n        )\n        self.beta_prod_t_sqrt = torch.repeat_interleave(\n            beta_prod_t_sqrt,\n            repeats=self.frame_bff_size if self.use_denoising_batch else 1,\n            dim=0,\n        )\n\n    @torch.no_grad()\n    def update_prompt(self, prompt: str) -> None:\n        encoder_output = self.pipe.encode_prompt(\n            prompt=prompt,\n            device=self.device,\n            num_images_per_prompt=1,\n            do_classifier_free_guidance=False,\n        )\n        self.prompt_embeds = encoder_output[0].repeat(self.batch_size, 1, 1)\n\n    def add_noise(\n        self,\n        original_samples: torch.Tensor,\n        noise: torch.Tensor,\n        t_index: int,\n    ) -> torch.Tensor:\n        noisy_samples = (\n            self.alpha_prod_t_sqrt[t_index] * original_samples\n            + self.beta_prod_t_sqrt[t_index] * noise\n        )\n        return noisy_samples\n\n    def scheduler_step_batch(\n        self,\n        model_pred_batch: torch.Tensor,\n        x_t_latent_batch: torch.Tensor,\n        idx: Optional[int] = None,\n    ) -> torch.Tensor:\n        # TODO: use t_list to select beta_prod_t_sqrt\n        if idx is None:\n            F_theta = (\n                x_t_latent_batch - self.beta_prod_t_sqrt * model_pred_batch\n            ) / self.alpha_prod_t_sqrt\n            denoised_batch = self.c_out * F_theta + self.c_skip * x_t_latent_batch\n        else:\n            F_theta = (\n                x_t_latent_batch - self.beta_prod_t_sqrt[idx] * model_pred_batch\n            ) / self.alpha_prod_t_sqrt[idx]\n            denoised_batch = (\n                self.c_out[idx] * F_theta + self.c_skip[idx] * x_t_latent_batch\n            )\n\n        return denoised_batch\n\n    def unet_step(\n        self,\n        x_t_latent: torch.Tensor,\n        t_list: Union[torch.Tensor, list[int]],\n        idx: Optional[int] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        if self.guidance_scale > 1.0 and (self.cfg_type == \"initialize\"):\n            x_t_latent_plus_uc = torch.concat([x_t_latent[0:1], x_t_latent], dim=0)\n            t_list = torch.concat([t_list[0:1], t_list], dim=0)\n        elif self.guidance_scale > 1.0 and (self.cfg_type == \"full\"):\n            x_t_latent_plus_uc = torch.concat([x_t_latent, x_t_latent], dim=0)\n            t_list = torch.concat([t_list, t_list], dim=0)\n        else:\n            x_t_latent_plus_uc = x_t_latent\n\n        model_pred = self.unet(\n            x_t_latent_plus_uc,\n            t_list,\n            encoder_hidden_states=self.prompt_embeds,\n            return_dict=False,\n        )[0]\n\n        if self.guidance_scale > 1.0 and (self.cfg_type == \"initialize\"):\n            noise_pred_text = model_pred[1:]\n            self.stock_noise = torch.concat(\n                [model_pred[0:1], self.stock_noise[1:]], dim=0\n            )  # ここコメントアウトでself out cfg\n        elif self.guidance_scale > 1.0 and (self.cfg_type == \"full\"):\n            noise_pred_uncond, noise_pred_text = model_pred.chunk(2)\n        else:\n            noise_pred_text = model_pred\n        if self.guidance_scale > 1.0 and (\n            self.cfg_type == \"self\" or self.cfg_type == \"initialize\"\n        ):\n            noise_pred_uncond = self.stock_noise * self.delta\n        if self.guidance_scale > 1.0 and self.cfg_type != \"none\":\n            model_pred = noise_pred_uncond + self.guidance_scale * (\n                noise_pred_text - noise_pred_uncond\n            )\n        else:\n            model_pred = noise_pred_text\n\n        # compute the previous noisy sample x_t -> x_t-1\n        if self.use_denoising_batch:\n            denoised_batch = self.scheduler_step_batch(model_pred, x_t_latent, idx)\n            if self.cfg_type == \"self\" or self.cfg_type == \"initialize\":\n                scaled_noise = self.beta_prod_t_sqrt * self.stock_noise\n                delta_x = self.scheduler_step_batch(model_pred, scaled_noise, idx)\n                alpha_next = torch.concat(\n                    [\n                        self.alpha_prod_t_sqrt[1:],\n                        torch.ones_like(self.alpha_prod_t_sqrt[0:1]),\n                    ],\n                    dim=0,\n                )\n                delta_x = alpha_next * delta_x\n                beta_next = torch.concat(\n                    [\n                        self.beta_prod_t_sqrt[1:],\n                        torch.ones_like(self.beta_prod_t_sqrt[0:1]),\n                    ],\n                    dim=0,\n                )\n                delta_x = delta_x / beta_next\n                init_noise = torch.concat(\n                    [self.init_noise[1:], self.init_noise[0:1]], dim=0\n                )\n                self.stock_noise = init_noise + delta_x\n\n        else:\n            # denoised_batch = self.scheduler.step(model_pred, t_list[0], x_t_latent).denoised\n            denoised_batch = self.scheduler_step_batch(model_pred, x_t_latent, idx)\n\n        return denoised_batch, model_pred\n\n    def encode_image(self, image_tensors: torch.Tensor) -> torch.Tensor:\n        image_tensors = image_tensors.to(\n            device=self.device,\n            dtype=self.vae.dtype,\n        )\n        img_latent = retrieve_latents(self.vae.encode(image_tensors), self.generator)\n        img_latent = img_latent * self.vae.config.scaling_factor\n        x_t_latent = self.add_noise(img_latent, self.init_noise[0], 0)\n        return x_t_latent\n\n    def decode_image(self, x_0_pred_out: torch.Tensor) -> torch.Tensor:\n        output_latent = self.vae.decode(\n            x_0_pred_out / self.vae.config.scaling_factor, return_dict=False\n        )[0]\n        return output_latent\n\n    def predict_x0_batch(self, x_t_latent: torch.Tensor) -> torch.Tensor:\n        prev_latent_batch = self.x_t_latent_buffer\n\n        if self.use_denoising_batch:\n            t_list = self.sub_timesteps_tensor\n            if self.denoising_steps_num > 1:\n                x_t_latent = torch.cat((x_t_latent, prev_latent_batch), dim=0)\n                self.stock_noise = torch.cat(\n                    (self.init_noise[0:1], self.stock_noise[:-1]), dim=0\n                )\n            x_0_pred_batch, model_pred = self.unet_step(x_t_latent, t_list)\n\n            if self.denoising_steps_num > 1:\n                x_0_pred_out = x_0_pred_batch[-1].unsqueeze(0)\n                if self.do_add_noise:\n                    self.x_t_latent_buffer = (\n                        self.alpha_prod_t_sqrt[1:] * x_0_pred_batch[:-1]\n                        + self.beta_prod_t_sqrt[1:] * self.init_noise[1:]\n                    )\n                else:\n                    self.x_t_latent_buffer = (\n                        self.alpha_prod_t_sqrt[1:] * x_0_pred_batch[:-1]\n                    )\n            else:\n                x_0_pred_out = x_0_pred_batch\n                self.x_t_latent_buffer = None\n        else:\n            self.init_noise = x_t_latent\n            for idx, t in enumerate(self.sub_timesteps_tensor):\n                t = t.view(\n                    1,\n                ).repeat(\n                    self.frame_bff_size,\n                )\n                x_0_pred, model_pred = self.unet_step(x_t_latent, t, idx)\n                if idx < len(self.sub_timesteps_tensor) - 1:\n                    if self.do_add_noise:\n                        x_t_latent = self.alpha_prod_t_sqrt[\n                            idx + 1\n                        ] * x_0_pred + self.beta_prod_t_sqrt[\n                            idx + 1\n                        ] * torch.randn_like(\n                            x_0_pred, device=self.device, dtype=self.dtype\n                        )\n                    else:\n                        x_t_latent = self.alpha_prod_t_sqrt[idx + 1] * x_0_pred\n            x_0_pred_out = x_0_pred\n\n        return x_0_pred_out\n\n    @torch.no_grad()\n    def __call__(\n        self, x: Union[torch.Tensor, PIL.Image.Image, np.ndarray] = None\n    ) -> torch.Tensor:\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        start.record()\n        if x is not None:\n            x = self.image_processor.preprocess(x, self.height, self.width).to(\n                device=self.device, dtype=self.dtype\n            )\n            if self.similar_image_filter:\n                x = self.similar_filter(x)\n                if x is None:\n                    time.sleep(self.inference_time_ema)\n                    return self.prev_image_result\n            x_t_latent = self.encode_image(x)\n        else:\n            # TODO: check the dimension of x_t_latent\n            x_t_latent = torch.randn((1, 4, self.latent_height, self.latent_width)).to(\n                device=self.device, dtype=self.dtype\n            )\n        x_0_pred_out = self.predict_x0_batch(x_t_latent)\n        x_output = self.decode_image(x_0_pred_out).detach().clone()\n\n        self.prev_image_result = x_output\n        end.record()\n        torch.cuda.synchronize()\n        inference_time = start.elapsed_time(end) / 1000\n        self.inference_time_ema = 0.9 * self.inference_time_ema + 0.1 * inference_time\n        return x_output\n\n    @torch.no_grad()\n    def txt2img(self, batch_size: int = 1) -> torch.Tensor:\n        x_0_pred_out = self.predict_x0_batch(\n            torch.randn((batch_size, 4, self.latent_height, self.latent_width)).to(\n                device=self.device, dtype=self.dtype\n            )\n        )\n        x_output = self.decode_image(x_0_pred_out).detach().clone()\n        return x_output\n\n    def txt2img_sd_turbo(self, batch_size: int = 1) -> torch.Tensor:\n        x_t_latent = torch.randn(\n            (batch_size, 4, self.latent_height, self.latent_width),\n            device=self.device,\n            dtype=self.dtype,\n        )\n        model_pred = self.unet(\n            x_t_latent,\n            self.sub_timesteps_tensor,\n            encoder_hidden_states=self.prompt_embeds,\n            return_dict=False,\n        )[0]\n        x_0_pred_out = (\n            x_t_latent - self.beta_prod_t_sqrt * model_pred\n        ) / self.alpha_prod_t_sqrt\n        return self.decode_image(x_0_pred_out)","file":"/src/streamdiffusion/pipeline.py","fileHash":"a2ae37545d7ab715dd08b4f49d75565816a53cbafc1457eb03e2c61597cd6eba","hash":"7bd93fef7c0085ec178d91716199fec7f198cc37a6f632e614825d1cf1a80cfb","processedContent":"class StreamDiffusion:\n    def __init__(\n        \"\"\"This code defines a class called StreamDiffusion that initializes various parameters and objects, including a pipeline, an image processor, a scheduler, a text encoder, and a U-Net. It also defines a SimilarImageFilter class to compare two tensors based on cosine similarity.\"\"\"\n        pass\n\n    def load_lcm_lora(\n        \"\"\"This code defines a function called `load_lcm_lora` that loads pre-trained weights for a machine learning model. The function takes in several parameters, including the name or path of the pre-trained model, an adapter name, and keyword arguments. It then calls another function called `load_lora_weights` to load the weights into the model.\"\"\"\n        pass\n\n    def load_lora(\n        \"\"\"This code defines a function called `load_lora` that loads pre-trained weights for a language model. It takes in a few parameters, including the name or path of the pre-trained model, an adapter name, and some keyword arguments. The function then calls another function called `load_lora_weights` to load the weights into the model.\"\"\"\n        pass\n\n    def fuse_lora(\n        \"\"\"This code defines a function called `fuse_lora` that fuses the LoRA model with other models in the pipeline. It takes several parameters, including `fuse_unet`, `fuse_text_encoder`, `lora_scale`, and `safe_fusing`. The function modifies the pipeline by adding or removing certain components based on the input parameters.\"\"\"\n        pass\n\n    def enable_similar_image_filter(self, threshold: float = 0.98, max_skip_frame: float = 10) -> None:\n        \"\"\"This code enables a similar image filter for the StreamDiffusion class. It sets a threshold value and maximum number of frames to skip when comparing images.\"\"\"\n        pass\n\n    def disable_similar_image_filter(self) -> None:\n        \"\"\"This code disables the similar image filter in the application.\"\"\"\n        pass\n\n    @torch.no_grad()\n    def prepare(\n        \"\"\"This code is a part of a larger program that generates images using a diffusion process. It prepares the necessary variables and structures for the diffusion process by encoding the prompt, setting up the timesteps, and initializing the noise.\"\"\"\n        pass\n\n    @torch.no_grad()\n    def update_prompt(self, prompt: str) -> None:\n        \"\"\"This code updates the prompt for a model by encoding it and repeating it to match the batch size.\"\"\"\n        pass\n\n    def add_noise(\n        \"\"\"This code defines a function called `add_noise` that takes in three parameters: `original_samples`, `noise`, and `t_index`. It returns a new tensor that is the sum of the product of `alpha_prod_t_sqrt[t_index]` and `original_samples` and the product of `beta_prod_t_sqrt[t_index]` and `noise`.\"\"\"\n        pass\n\n    def scheduler_step_batch(\n        \"\"\"This code defines a function called `scheduler_step_batch` that takes in a batch of data and performs some operations on it. It then returns the denoised batch.\"\"\"\n        pass\n\n    def unet_step(\n        \"\"\"This code defines a function called `unet_step` that takes in a tensor and performs some operations on it. It then returns the denoised batch.\"\"\"\n        pass\n\n    def encode_image(self, image_tensors: torch.Tensor) -> torch.Tensor:\n        \"\"\"This code defines a function called `encode_image` that takes in an image tensor and returns a new tensor that is the sum of the product of `alpha_prod_t_sqrt[t_index]` and `original_samples` and the product of `beta_prod_t_sqrt[t_index]` and `noise`.\"\"\"\n        pass\n\n    def decode_image(self, x_0_pred_out: torch.Tensor) -> torch.Tensor:\n        \"\"\"This code is a function that takes in a tensor and uses it to decode an image using a VAE model. It returns the decoded image as a tensor.\"\"\"\n        pass\n\n    def predict_x0_batch(self, x_t_latent: torch.Tensor) -> torch.Tensor:\n        \"\"\"This code defines a function called `predict_x0_batch` that takes in a tensor and performs some operations on it. It then returns the denoised batch.\"\"\"\n        pass\n\n    @torch.no_grad()\n    def __call__(\n        \"\"\"This code defines a function called `StreamDiffusion` that takes in an image tensor and returns a denoised image tensor using a VAE model. It also maintains a moving average of the inference time to improve performance.\"\"\"\n        pass\n\n    @torch.no_grad()\n    def txt2img(self, batch_size: int = 1) -> torch.Tensor:\n        \"\"\"This code defines a function called `txt2img` that takes in a batch size and returns a denoised image tensor using a VAE model.\"\"\"\n        pass\n\n    def txt2img_sd_turbo(self, batch_size: int = 1) -> torch.Tensor:\n        x_t_latent = torch.randn(\n            (batch_size, 4, self.latent_height, self.latent_width),\n            device=self.device,\n            dtype=self.dtype,\n        )\n        model_pred = self.unet(\n            x_t_latent,\n            self.sub_timesteps_tensor,\n            encoder_hidden_states=self.prompt_embeds,\n            return_dict=False,\n        )[0]\n        x_0_pred_out = (\n            x_t_latent - self.beta_prod_t_sqrt * model_pred\n        ) / self.alpha_prod_t_sqrt\n        return self.decode_image(x_0_pred_out)","documentation":"The code defines a class called StreamDiffusion that performs image denoising using a VAE model. It also includes functions for loading pre-trained weights, fusing the LoRA model with other models, and enabling or disabling a similar image filter."}},{"key":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#","attributes":{"range":[6,0,44,44],"symbol":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#","content":"class SimilarImageFilter:\n    def __init__(self, threshold: float = 0.98, max_skip_frame: float = 10) -> None:\n        self.threshold = threshold\n        self.prev_tensor = None\n        self.cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n        self.max_skip_frame = max_skip_frame\n        self.skip_count = 0\n\n    def __call__(self, x: torch.Tensor) -> Optional[torch.Tensor]:\n        if self.prev_tensor is None:\n            self.prev_tensor = x.detach().clone()\n            return x\n        else:\n            cos_sim = self.cos(self.prev_tensor.reshape(-1), x.reshape(-1)).item()\n            sample = random.uniform(0, 1)\n            if self.threshold >= 1:\n                skip_prob = 0\n            else:\n                skip_prob = max(0, 1 - (1 - cos_sim) / (1 - self.threshold))\n\n            # not skip frame\n            if skip_prob < sample:\n                self.prev_tensor = x.detach().clone()\n                return x\n            # skip frame\n            else:\n                if self.skip_count > self.max_skip_frame:\n                    self.skip_count = 0\n                    self.prev_tensor = x.detach().clone()\n                    return x\n                else:\n                    self.skip_count += 1\n                    return None\n\n    def set_threshold(self, threshold: float) -> None:\n        self.threshold = threshold\n    \n    def set_max_skip_frame(self, max_skip_frame: float) -> None:\n        self.max_skip_frame = max_skip_frame","file":"/src/streamdiffusion/image_filter.py","language":"python","fileHash":"a53b52b86d6ecbad2638fa4d705d28ea7d71ce34e7292d3340d9457ef579b20f","hash":"7a87f3e0f7d02eee38cffd301775bd6143d4f2dcbdf36b3f56a618fe7c12dc5c","processedContent":"class SimilarImageFilter:\n    def __init__(self, threshold: float = 0.98, max_skip_frame: float = 10) -> None:\n        \"\"\"This code defines a class that initializes a threshold value and a maximum number of frames to skip when comparing two tensors. It also sets up a cosine similarity metric and a counter for the number of frames skipped.\"\"\"\n        pass\n\n    def __call__(self, x: torch.Tensor) -> Optional[torch.Tensor]:\n        \"\"\"This code defines a function that takes a tensor as input and returns a new tensor. The function uses cosine similarity to determine whether to skip a frame or not, based on the similarity between the current frame and the previous frame. If the similarity is below a certain threshold, the function will return None, indicating that the frame should be skipped.\"\"\"\n        pass\n\n    def set_threshold(self, threshold: float) -> None:\n        \"\"\"This code sets a threshold value for the class instance. It takes a float argument and assigns it to the instance's threshold attribute.\"\"\"\n        pass\n    \n    def set_max_skip_frame(self, max_skip_frame: float) -> None:\n        self.max_skip_frame = max_skip_frame","documentation":"This code defines a class called SimilarImageFilter that initializes a threshold value and a maximum number of frames to skip when comparing two tensors. It also sets up a cosine similarity metric and a counter for the number of frames skipped. The class has a __call__ function that takes a tensor as input and returns a new tensor based on cosine similarity, and two additional functions to set the threshold and maximum frame skip values."}},{"key":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#__init__().","attributes":{"range":[7,4,12,27],"symbol":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#__init__().","content":"def __init__(self, threshold: float = 0.98, max_skip_frame: float = 10) -> None:\n        self.threshold = threshold\n        self.prev_tensor = None\n        self.cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n        self.max_skip_frame = max_skip_frame\n        self.skip_count = 0","file":"/src/streamdiffusion/image_filter.py","language":"python","fileHash":"a53b52b86d6ecbad2638fa4d705d28ea7d71ce34e7292d3340d9457ef579b20f","hash":"1d469d1f9ebc76dc4de07b32dfa2709f9ba1ec0ad5e59f1b5014fb6bd7eb48bc","processedContent":"def __init__(self, threshold: float = 0.98, max_skip_frame: float = 10) -> None:\n        self.threshold = threshold\n        self.prev_tensor = None\n        self.cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n        self.max_skip_frame = max_skip_frame\n        self.skip_count = 0","documentation":"This code defines a class that initializes a threshold value and a maximum number of frames to skip when comparing two tensors. It also sets up a cosine similarity metric and a counter for the number of frames skipped."}},{"key":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#__call__().","attributes":{"range":[14,4,38,31],"symbol":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#__call__().","content":"def __call__(self, x: torch.Tensor) -> Optional[torch.Tensor]:\n        if self.prev_tensor is None:\n            self.prev_tensor = x.detach().clone()\n            return x\n        else:\n            cos_sim = self.cos(self.prev_tensor.reshape(-1), x.reshape(-1)).item()\n            sample = random.uniform(0, 1)\n            if self.threshold >= 1:\n                skip_prob = 0\n            else:\n                skip_prob = max(0, 1 - (1 - cos_sim) / (1 - self.threshold))\n\n            # not skip frame\n            if skip_prob < sample:\n                self.prev_tensor = x.detach().clone()\n                return x\n            # skip frame\n            else:\n                if self.skip_count > self.max_skip_frame:\n                    self.skip_count = 0\n                    self.prev_tensor = x.detach().clone()\n                    return x\n                else:\n                    self.skip_count += 1\n                    return None","file":"/src/streamdiffusion/image_filter.py","language":"python","fileHash":"a53b52b86d6ecbad2638fa4d705d28ea7d71ce34e7292d3340d9457ef579b20f","hash":"7162c55790b0b14be8e5bae69a5038ecd91dc9fbcb76716a9ae5c2077540722d","processedContent":"def __call__(self, x: torch.Tensor) -> Optional[torch.Tensor]:\n        if self.prev_tensor is None: #undefined\n            self.prev_tensor = x.detach().clone() #undefined\n            return x\n        else:\n            cos_sim = self.cos(self.prev_tensor.reshape(-1), x.reshape(-1)).item()\n            \"\"\"\n            scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#prev_tensor.: undefined\n            scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#cos.: undefined\n            \"\"\"\n            sample = random.uniform(0, 1)\n            \"\"\"\n            scip-python python python-stdlib 3.11 random/__init__:: undefined\n            scip-python python python-stdlib 3.11 random/uniform.: undefined\n            \"\"\"\n            if self.threshold >= 1: #undefined\n                skip_prob = 0\n            else:\n                skip_prob = max(0, 1 - (1 - cos_sim) / (1 - self.threshold)) #undefined\n\n            # not skip frame\n            if skip_prob < sample:\n                self.prev_tensor = x.detach().clone() #undefined\n                return x\n            # skip frame\n            else:\n                if self.skip_count > self.max_skip_frame:\n                \"\"\"\n                scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#skip_count.: undefined\n                scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#max_skip_frame.: undefined\n                \"\"\"\n                    self.skip_count = 0 #undefined\n                    self.prev_tensor = x.detach().clone() #undefined\n                    return x\n                else:\n                    self.skip_count += 1 #undefined\n                    return None","documentation":"This code defines a function that takes a tensor as input and returns a new tensor. The function uses cosine similarity to determine whether to skip a frame or not, based on the similarity between the current frame and the previous frame. If the similarity is below a certain threshold, the function will return None, indicating that the frame should be skipped."}},{"key":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#set_threshold().","attributes":{"range":[40,4,41,34],"symbol":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#set_threshold().","content":"def set_threshold(self, threshold: float) -> None:\n        self.threshold = threshold","file":"/src/streamdiffusion/image_filter.py","language":"python","fileHash":"a53b52b86d6ecbad2638fa4d705d28ea7d71ce34e7292d3340d9457ef579b20f","hash":"fdcdd51c12dab65b47b19c0e3627b3e607893fbbe94843b4d65ac391b8ce710f","processedContent":"def set_threshold(self, threshold: float) -> None:\n        self.threshold = threshold","documentation":"This code sets a threshold value for the class instance. It takes a float argument and assigns it to the instance's threshold attribute."}},{"key":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#set_max_skip_frame().","attributes":{"range":[43,4,44,44],"symbol":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#set_max_skip_frame().","content":"def set_max_skip_frame(self, max_skip_frame: float) -> None:\n        self.max_skip_frame = max_skip_frame","file":"/src/streamdiffusion/image_filter.py","language":"python","fileHash":"a53b52b86d6ecbad2638fa4d705d28ea7d71ce34e7292d3340d9457ef579b20f","hash":"2ff5d7d25c2a00539902d605610aeae0d4dca7b2caa4fcdd8d73439a2e958186","processedContent":"def set_max_skip_frame(self, max_skip_frame: float) -> None:\n        self.max_skip_frame = max_skip_frame","documentation":"This code sets the maximum number of frames that can be skipped when playing a video."}},{"key":"scip-python python temp indexer `src.streamdiffusion.image_filter`/__init__:","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.image_filter`/__init__:","range":[0,0,45,0],"content":"from typing import Optional\nimport random\n\nimport torch\n\n\nclass SimilarImageFilter:\n    def __init__(self, threshold: float = 0.98, max_skip_frame: float = 10) -> None:\n        self.threshold = threshold\n        self.prev_tensor = None\n        self.cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n        self.max_skip_frame = max_skip_frame\n        self.skip_count = 0\n\n    def __call__(self, x: torch.Tensor) -> Optional[torch.Tensor]:\n        if self.prev_tensor is None:\n            self.prev_tensor = x.detach().clone()\n            return x\n        else:\n            cos_sim = self.cos(self.prev_tensor.reshape(-1), x.reshape(-1)).item()\n            sample = random.uniform(0, 1)\n            if self.threshold >= 1:\n                skip_prob = 0\n            else:\n                skip_prob = max(0, 1 - (1 - cos_sim) / (1 - self.threshold))\n\n            # not skip frame\n            if skip_prob < sample:\n                self.prev_tensor = x.detach().clone()\n                return x\n            # skip frame\n            else:\n                if self.skip_count > self.max_skip_frame:\n                    self.skip_count = 0\n                    self.prev_tensor = x.detach().clone()\n                    return x\n                else:\n                    self.skip_count += 1\n                    return None\n\n    def set_threshold(self, threshold: float) -> None:\n        self.threshold = threshold\n    \n    def set_max_skip_frame(self, max_skip_frame: float) -> None:\n        self.max_skip_frame = max_skip_frame\n","file":"/src/streamdiffusion/image_filter.py","language":"python","fileHash":"a53b52b86d6ecbad2638fa4d705d28ea7d71ce34e7292d3340d9457ef579b20f","hash":"a53b52b86d6ecbad2638fa4d705d28ea7d71ce34e7292d3340d9457ef579b20f","processedContent":"from typing import Optional\n\"\"\"\nscip-python python python-stdlib 3.11 typing/__init__:: undefined\nscip-python python python-stdlib 3.11 typing/Optional.: undefined\n\"\"\"\nimport random #undefined\n\nimport torch\n\n\nclass SimilarImageFilter:\n    \"\"\"This code defines a class called SimilarImageFilter that initializes a threshold value and a maximum number of frames to skip when comparing two tensors. It also sets up a cosine similarity metric and a counter for the number of frames skipped. The class has a __call__ function that takes a tensor as input and returns a new tensor based on cosine similarity, and two additional functions to set the threshold and maximum frame skip values.\"\"\"\n    pass\n","documentation":"This code defines a class called SimilarImageFilter that compares two tensors based on cosine similarity and skips frames if the similarity is below a threshold."}},{"key":"scip-python python python-stdlib 3.11 random/__init__:","attributes":{"symbol":"scip-python python python-stdlib 3.11 random/__init__:","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#prev_tensor.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#prev_tensor.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#cos.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#cos.","language":"python"}},{"key":"scip-python python python-stdlib 3.11 random/uniform.","attributes":{"symbol":"scip-python python python-stdlib 3.11 random/uniform.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#threshold.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#threshold.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#skip_count.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#skip_count.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#max_skip_frame.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#max_skip_frame.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.image_utils`/denormalize().","attributes":{"range":[8,0,12,41],"symbol":"scip-python python temp indexer `src.streamdiffusion.image_utils`/denormalize().","content":"def denormalize(images: Union[torch.Tensor, np.ndarray]) -> torch.Tensor:\n    \"\"\"\n    Denormalize an image array to [0,1].\n    \"\"\"\n    return (images / 2 + 0.5).clamp(0, 1)","file":"/src/streamdiffusion/image_utils.py","language":"python","fileHash":"a58beff119790024a1b319b08d727e0ec526e5d7037fa16c9fc9918aa819fb2a","hash":"a8f092e55e1e5389f22038c68b7611f54409a3ccc1198302a70575334b1d9206","processedContent":"def denormalize(images: Union[torch.Tensor, np.ndarray]) -> torch.Tensor:\n\"\"\"\nscip-python python python-stdlib 3.11 typing/Union.: undefined\nscip-python python numpy 1.25.2 numpy/ndarray#: undefined\n\"\"\"\n    \"\"\"\n    Denormalize an image array to [0,1].\n    \"\"\"\n    return (images / 2 + 0.5).clamp(0, 1)","documentation":"This code denormalizes an image array by dividing it by 2 and adding 0.5, then clamping the result between 0 and 1 to ensure that the values are within the range of [0,1]."}},{"key":"scip-python python temp indexer `src.streamdiffusion.image_utils`/pt_to_numpy().","attributes":{"range":[15,0,20,17],"symbol":"scip-python python temp indexer `src.streamdiffusion.image_utils`/pt_to_numpy().","content":"def pt_to_numpy(images: torch.Tensor) -> np.ndarray:\n    \"\"\"\n    Convert a PyTorch tensor to a NumPy image.\n    \"\"\"\n    images = images.cpu().permute(0, 2, 3, 1).float().numpy()\n    return images","file":"/src/streamdiffusion/image_utils.py","language":"python","fileHash":"a58beff119790024a1b319b08d727e0ec526e5d7037fa16c9fc9918aa819fb2a","hash":"7d4abb2e4b46b6e9d3217ee5fd6eca0d36096663eadd2493f8fcfed08d2e1323","processedContent":"def pt_to_numpy(images: torch.Tensor) -> np.ndarray: #undefined\n    \"\"\"\n    Convert a PyTorch tensor to a NumPy image.\n    \"\"\"\n    images = images.cpu().permute(0, 2, 3, 1).float().numpy()\n    return images","documentation":"This code converts a PyTorch tensor to a NumPy image."}},{"key":"scip-python python temp indexer `src.streamdiffusion.image_utils`/numpy_to_pil().","attributes":{"range":[23,0,38,21],"symbol":"scip-python python temp indexer `src.streamdiffusion.image_utils`/numpy_to_pil().","content":"def numpy_to_pil(images: np.ndarray) -> PIL.Image.Image:\n    \"\"\"\n    Convert a NumPy image or a batch of images to a PIL image.\n    \"\"\"\n    if images.ndim == 3:\n        images = images[None, ...]\n    images = (images * 255).round().astype(\"uint8\")\n    if images.shape[-1] == 1:\n        # special case for grayscale (single channel) images\n        pil_images = [\n            PIL.Image.fromarray(image.squeeze(), mode=\"L\") for image in images\n        ]\n    else:\n        pil_images = [PIL.Image.fromarray(image) for image in images]\n\n    return pil_images","file":"/src/streamdiffusion/image_utils.py","language":"python","fileHash":"a58beff119790024a1b319b08d727e0ec526e5d7037fa16c9fc9918aa819fb2a","hash":"e9f41144ca039a39188cfd02bc723f03af96571277e9eece7af98947643834b9","processedContent":"def numpy_to_pil(images: np.ndarray) -> PIL.Image.Image:\n\"\"\"\nscip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\nscip-python python Pillow 10.0.0 `pil.image`/Image#: undefined\nscip-python python numpy 1.25.2 numpy/ndarray#: undefined\n\"\"\"\n    \"\"\"\n    Convert a NumPy image or a batch of images to a PIL image.\n    \"\"\"\n    if images.ndim == 3: #undefined\n        images = images[None, ...]\n    images = (images * 255).round().astype(\"uint8\")\n    \"\"\"\n    scip-python python numpy 1.25.2 numpy/_ArrayOrScalarCommon#round().: undefined\n    scip-python python numpy 1.25.2 numpy/ndarray#astype().: undefined\n    \"\"\"\n    if images.shape[-1] == 1: #undefined\n        # special case for grayscale (single channel) images\n        pil_images = [\n            PIL.Image.fromarray(image.squeeze(), mode=\"L\") for image in images\n            \"\"\"\n            scip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\n            scip-python python Pillow 10.0.0 `pil.image`/fromarray().: undefined\n            \"\"\"\n        ]\n    else:\n        pil_images = [PIL.Image.fromarray(image) for image in images]\n        \"\"\"\n        scip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\n        scip-python python Pillow 10.0.0 `pil.image`/fromarray().: undefined\n        \"\"\"\n\n    return pil_images","documentation":"This code takes a NumPy array of images and converts it to a list of PIL images. It first checks if the input is a single image or a batch of images, and then converts each image to a PIL image using the `fromarray()` method."}},{"key":"scip-python python temp indexer `src.streamdiffusion.image_utils`/postprocess_image().","attributes":{"range":[41,0,74,34],"symbol":"scip-python python temp indexer `src.streamdiffusion.image_utils`/postprocess_image().","content":"def postprocess_image(\n    image: torch.Tensor,\n    output_type: str = \"pil\",\n    do_denormalize: Optional[List[bool]] = None,\n) -> Union[torch.Tensor, np.ndarray, PIL.Image.Image]:\n    if not isinstance(image, torch.Tensor):\n        raise ValueError(\n            f\"Input for postprocessing is in incorrect format: {type(image)}. We only support pytorch tensor\"\n        )\n\n    if output_type == \"latent\":\n        return image\n\n    do_normalize_flg = True\n    if do_denormalize is None:\n        do_denormalize = [do_normalize_flg] * image.shape[0]\n\n    image = torch.stack(\n        [\n            denormalize(image[i]) if do_denormalize[i] else image[i]\n            for i in range(image.shape[0])\n        ]\n    )\n\n    if output_type == \"pt\":\n        return image\n\n    image = pt_to_numpy(image)\n\n    if output_type == \"np\":\n        return image\n\n    if output_type == \"pil\":\n        return numpy_to_pil(image)","file":"/src/streamdiffusion/image_utils.py","language":"python","fileHash":"a58beff119790024a1b319b08d727e0ec526e5d7037fa16c9fc9918aa819fb2a","hash":"0fba6b5660296a9b0c0585863457f95f0d229aa3a944e59a315e7e24b3cfdaaa","processedContent":"def postprocess_image(\n    image: torch.Tensor,\n    output_type: str = \"pil\",\n    do_denormalize: Optional[List[bool]] = None,\n    \"\"\"\n    scip-python python python-stdlib 3.11 typing/Optional.: undefined\n    scip-python python python-stdlib 3.11 typing/List.: undefined\n    \"\"\"\n) -> Union[torch.Tensor, np.ndarray, PIL.Image.Image]:\n\"\"\"\nscip-python python python-stdlib 3.11 typing/Union.: undefined\nscip-python python numpy 1.25.2 numpy/ndarray#: undefined\nscip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\nscip-python python Pillow 10.0.0 `pil.image`/Image#: undefined\n\"\"\"\n    if not isinstance(image, torch.Tensor):\n        raise ValueError(\n            f\"Input for postprocessing is in incorrect format: {type(image)}. We only support pytorch tensor\"\n        )\n\n    if output_type == \"latent\":\n        return image\n\n    do_normalize_flg = True\n    if do_denormalize is None:\n        do_denormalize = [do_normalize_flg] * image.shape[0]\n\n    image = torch.stack(\n        [\n            denormalize(image[i]) if do_denormalize[i] else image[i] #This code denormalizes an image array by dividing it by 2 and adding 0.5, then clamping the result between 0 and 1 to ensure that the values are within the range of [0,1].\n            for i in range(image.shape[0])\n        ]\n    )\n\n    if output_type == \"pt\":\n        return image\n\n    image = pt_to_numpy(image) #This code converts a PyTorch tensor to a NumPy image.\n\n    if output_type == \"np\":\n        return image\n\n    if output_type == \"pil\":\n        return numpy_to_pil(image)","documentation":"This code is a post-processing function for images. It takes an image tensor as input, denormalizes it if necessary, converts it to a NumPy array, and then converts it to a PIL image if requested."}},{"key":"scip-python python temp indexer `src.streamdiffusion.image_utils`/process_image().","attributes":{"range":[77,0,83,38],"symbol":"scip-python python temp indexer `src.streamdiffusion.image_utils`/process_image().","content":"def process_image(\n    image_pil: PIL.Image.Image, range: Tuple[int, int] = (-1, 1)\n) -> Tuple[torch.Tensor, PIL.Image.Image]:\n    image = torchvision.transforms.ToTensor()(image_pil)\n    r_min, r_max = range[0], range[1]\n    image = image * (r_max - r_min) + r_min\n    return image[None, ...], image_pil","file":"/src/streamdiffusion/image_utils.py","language":"python","fileHash":"a58beff119790024a1b319b08d727e0ec526e5d7037fa16c9fc9918aa819fb2a","hash":"05bf96cf10056266655e405bb7260a779612f8f6ed043a0f64bb0ac5e8938393","processedContent":"def process_image(\n    image_pil: PIL.Image.Image, range: Tuple[int, int] = (-1, 1)\n    \"\"\"\n    scip-python python python-stdlib 3.11 typing/Tuple.: undefined\n    scip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\n    scip-python python Pillow 10.0.0 `pil.image`/Image#: undefined\n    \"\"\"\n) -> Tuple[torch.Tensor, PIL.Image.Image]:\n\"\"\"\nscip-python python python-stdlib 3.11 typing/Tuple.: undefined\nscip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\nscip-python python Pillow 10.0.0 `pil.image`/Image#: undefined\n\"\"\"\n    image = torchvision.transforms.ToTensor()(image_pil)\n    r_min, r_max = range[0], range[1]\n    image = image * (r_max - r_min) + r_min\n    return image[None, ...], image_pil","documentation":"This code processes an image by converting it to a tensor, scaling its values within a specified range, and then returning the scaled tensor and the original image."}},{"key":"scip-python python temp indexer `src.streamdiffusion.image_utils`/pil2tensor().","attributes":{"range":[86,0,97,24],"symbol":"scip-python python temp indexer `src.streamdiffusion.image_utils`/pil2tensor().","content":"def pil2tensor(image_pil: PIL.Image.Image) -> torch.Tensor:\n    height = image_pil.height\n    width = image_pil.width\n    imgs = []\n    img, _ = process_image(image_pil)\n    imgs.append(img)\n    imgs = torch.vstack(imgs)\n    images = torch.nn.functional.interpolate(\n        imgs, size=(height, width), mode=\"bilinear\"\n    )\n    image_tensors = images.to(torch.float16)\n    return image_tensors","file":"/src/streamdiffusion/image_utils.py","language":"python","fileHash":"a58beff119790024a1b319b08d727e0ec526e5d7037fa16c9fc9918aa819fb2a","hash":"9b16bc80ed2b83495d8ec715b3733776a096ef105241405114402f233cece8f1","processedContent":"def pil2tensor(image_pil: PIL.Image.Image) -> torch.Tensor:\n\"\"\"\nscip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\nscip-python python Pillow 10.0.0 `pil.image`/Image#: undefined\n\"\"\"\n    height = image_pil.height #undefined\n    width = image_pil.width #undefined\n    imgs = []\n    img, _ = process_image(image_pil) #This code processes an image by converting it to a tensor, scaling its values within a specified range, and then returning the scaled tensor and the original image.\n    imgs.append(img)\n    imgs = torch.vstack(imgs)\n    images = torch.nn.functional.interpolate(\n        imgs, size=(height, width), mode=\"bilinear\"\n    )\n    image_tensors = images.to(torch.float16)\n    return image_tensors","documentation":"This code takes a PIL image and converts it to a PyTorch tensor, resizing it to the original height and width, and then normalizing its values within a specified range."}},{"key":"scip-python python temp indexer `src.streamdiffusion.image_utils`/__init__:","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.image_utils`/__init__:","range":[0,0,98,0],"content":"from typing import List, Optional, Tuple, Union\n\nimport numpy as np\nimport PIL.Image\nimport torch\nimport torchvision\n\n\ndef denormalize(images: Union[torch.Tensor, np.ndarray]) -> torch.Tensor:\n    \"\"\"\n    Denormalize an image array to [0,1].\n    \"\"\"\n    return (images / 2 + 0.5).clamp(0, 1)\n\n\ndef pt_to_numpy(images: torch.Tensor) -> np.ndarray:\n    \"\"\"\n    Convert a PyTorch tensor to a NumPy image.\n    \"\"\"\n    images = images.cpu().permute(0, 2, 3, 1).float().numpy()\n    return images\n\n\ndef numpy_to_pil(images: np.ndarray) -> PIL.Image.Image:\n    \"\"\"\n    Convert a NumPy image or a batch of images to a PIL image.\n    \"\"\"\n    if images.ndim == 3:\n        images = images[None, ...]\n    images = (images * 255).round().astype(\"uint8\")\n    if images.shape[-1] == 1:\n        # special case for grayscale (single channel) images\n        pil_images = [\n            PIL.Image.fromarray(image.squeeze(), mode=\"L\") for image in images\n        ]\n    else:\n        pil_images = [PIL.Image.fromarray(image) for image in images]\n\n    return pil_images\n\n\ndef postprocess_image(\n    image: torch.Tensor,\n    output_type: str = \"pil\",\n    do_denormalize: Optional[List[bool]] = None,\n) -> Union[torch.Tensor, np.ndarray, PIL.Image.Image]:\n    if not isinstance(image, torch.Tensor):\n        raise ValueError(\n            f\"Input for postprocessing is in incorrect format: {type(image)}. We only support pytorch tensor\"\n        )\n\n    if output_type == \"latent\":\n        return image\n\n    do_normalize_flg = True\n    if do_denormalize is None:\n        do_denormalize = [do_normalize_flg] * image.shape[0]\n\n    image = torch.stack(\n        [\n            denormalize(image[i]) if do_denormalize[i] else image[i]\n            for i in range(image.shape[0])\n        ]\n    )\n\n    if output_type == \"pt\":\n        return image\n\n    image = pt_to_numpy(image)\n\n    if output_type == \"np\":\n        return image\n\n    if output_type == \"pil\":\n        return numpy_to_pil(image)\n\n\ndef process_image(\n    image_pil: PIL.Image.Image, range: Tuple[int, int] = (-1, 1)\n) -> Tuple[torch.Tensor, PIL.Image.Image]:\n    image = torchvision.transforms.ToTensor()(image_pil)\n    r_min, r_max = range[0], range[1]\n    image = image * (r_max - r_min) + r_min\n    return image[None, ...], image_pil\n\n\ndef pil2tensor(image_pil: PIL.Image.Image) -> torch.Tensor:\n    height = image_pil.height\n    width = image_pil.width\n    imgs = []\n    img, _ = process_image(image_pil)\n    imgs.append(img)\n    imgs = torch.vstack(imgs)\n    images = torch.nn.functional.interpolate(\n        imgs, size=(height, width), mode=\"bilinear\"\n    )\n    image_tensors = images.to(torch.float16)\n    return image_tensors\n","file":"/src/streamdiffusion/image_utils.py","language":"python","fileHash":"a58beff119790024a1b319b08d727e0ec526e5d7037fa16c9fc9918aa819fb2a","hash":"a58beff119790024a1b319b08d727e0ec526e5d7037fa16c9fc9918aa819fb2a","processedContent":"from typing import List, Optional, Tuple, Union\n\"\"\"\nscip-python python python-stdlib 3.11 typing/__init__:: undefined\nscip-python python python-stdlib 3.11 typing/List.: undefined\nscip-python python python-stdlib 3.11 typing/Optional.: undefined\nscip-python python python-stdlib 3.11 typing/Tuple.: undefined\nscip-python python python-stdlib 3.11 typing/Union.: undefined\n\"\"\"\n\nimport numpy as np #undefined\nimport PIL.Image #undefined\nimport torch\nimport torchvision\n\n\ndef denormalize(images: Union[torch.Tensor, np.ndarray]) -> torch.Tensor:\n    \"\"\"This code denormalizes an image array by dividing it by 2 and adding 0.5, then clamping the result between 0 and 1 to ensure that the values are within the range of [0,1].\"\"\"\n    pass\n\n\ndef pt_to_numpy(images: torch.Tensor) -> np.ndarray:\n    \"\"\"This code converts a PyTorch tensor to a NumPy image.\"\"\"\n    pass\n\n\ndef numpy_to_pil(images: np.ndarray) -> PIL.Image.Image:\n    \"\"\"This code takes a NumPy array of images and converts it to a list of PIL images. It first checks if the input is a single image or a batch of images, and then converts each image to a PIL image using the `fromarray()` method.\"\"\"\n    pass\n\n\ndef postprocess_image(\n    \"\"\"This code is a post-processing function for images. It takes an image tensor as input, denormalizes it if necessary, converts it to a NumPy array, and then converts it to a PIL image if requested.\"\"\"\n    pass\n\n\ndef process_image(\n    \"\"\"This code processes an image by converting it to a tensor, scaling its values within a specified range, and then returning the scaled tensor and the original image.\"\"\"\n    pass\n\n\ndef pil2tensor(image_pil: PIL.Image.Image) -> torch.Tensor:\n    \"\"\"This code takes a PIL image and converts it to a PyTorch tensor, resizing it to the original height and width, and then normalizing its values within a specified range.\"\"\"\n    pass\n","documentation":"The code is a collection of functions for processing images in PyTorch, including denormalizing, converting to and from NumPy and PIL, and scaling the values within a specified range."}},{"key":"scip-python python python-stdlib 3.11 typing/Tuple.","attributes":{"symbol":"scip-python python python-stdlib 3.11 typing/Tuple.","language":"python"}},{"key":"scip-python python python-stdlib 3.11 typing/Union.","attributes":{"symbol":"scip-python python python-stdlib 3.11 typing/Union.","language":"python"}},{"key":"scip-python python numpy 1.25.2 numpy/ndarray#","attributes":{"symbol":"scip-python python numpy 1.25.2 numpy/ndarray#","language":"python"}},{"key":"scip-python python numpy 1.25.2 numpy/ndarray#ndim().","attributes":{"symbol":"scip-python python numpy 1.25.2 numpy/ndarray#ndim().","language":"python"}},{"key":"scip-python python numpy 1.25.2 numpy/_ArrayOrScalarCommon#round().","attributes":{"symbol":"scip-python python numpy 1.25.2 numpy/_ArrayOrScalarCommon#round().","language":"python"}},{"key":"scip-python python numpy 1.25.2 numpy/ndarray#astype().","attributes":{"symbol":"scip-python python numpy 1.25.2 numpy/ndarray#astype().","language":"python"}},{"key":"scip-python python numpy 1.25.2 numpy/ndarray#shape().","attributes":{"symbol":"scip-python python numpy 1.25.2 numpy/ndarray#shape().","language":"python"}},{"key":"scip-python python Pillow 10.0.0 `pil.image`/fromarray().","attributes":{"symbol":"scip-python python Pillow 10.0.0 `pil.image`/fromarray().","language":"python"}},{"key":"scip-python python Pillow 10.0.0 `pil.image`/Image#height().","attributes":{"symbol":"scip-python python Pillow 10.0.0 `pil.image`/Image#height().","language":"python"}},{"key":"scip-python python Pillow 10.0.0 `pil.image`/Image#width().","attributes":{"symbol":"scip-python python Pillow 10.0.0 `pil.image`/Image#width().","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/version().","attributes":{"range":[14,0,18,19],"symbol":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/version().","content":"def version(package: str) -> Optional[Version]:\n    try:\n        return Version(importlib.import_module(package).__version__)\n    except ModuleNotFoundError:\n        return None","file":"/src/streamdiffusion/pip_utils.py","language":"python","fileHash":"ccb6d55479085996365e1ab12a07c61a175ca8c06cc98d0d35826d2a8aff1fe0","hash":"7fce1fcaae89f5dff580312a5c7d4599a78184dcab3bebf7f79673e85303ad9c","processedContent":"def version(package: str) -> Optional[Version]:\n\"\"\"\nscip-python python python-stdlib 3.11 typing/Optional.: undefined\nscip-python python packaging 23.1 `packaging.version`/Version#: undefined\n\"\"\"\n    try:\n        return Version(importlib.import_module(package).__version__)\n        \"\"\"\n        scip-python python packaging 23.1 `packaging.version`/Version#: undefined\n        scip-python python python-stdlib 3.11 importlib/__init__:: undefined\n        scip-python python python-stdlib 3.11 importlib/import_module().: undefined\n        \"\"\"\n    except ModuleNotFoundError:\n        return None","documentation":"This code defines a function called `version` that takes a string argument representing a package name and returns the version of that package if it is installed, or None if it is not."}},{"key":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/is_installed().","attributes":{"range":[21,0,27,27],"symbol":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/is_installed().","content":"def is_installed(package: str) -> bool:\n    try:\n        spec = importlib.util.find_spec(package)\n    except ModuleNotFoundError:\n        return False\n\n    return spec is not None","file":"/src/streamdiffusion/pip_utils.py","language":"python","fileHash":"ccb6d55479085996365e1ab12a07c61a175ca8c06cc98d0d35826d2a8aff1fe0","hash":"78ec9167abdadcbfa2f5475df9d9499a3588184e01a99bde809f6a27e2b288e7","processedContent":"def is_installed(package: str) -> bool:\n    try:\n        spec = importlib.util.find_spec(package) #undefined\n    except ModuleNotFoundError:\n        return False\n\n    return spec is not None","documentation":"This code checks if a package is installed by attempting to import it and returning True if the import succeeds, False otherwise."}},{"key":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/run_python().","attributes":{"range":[30,0,47,30],"symbol":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/run_python().","content":"def run_python(command: str, env: Dict[str, str] = None) -> str:\n    run_kwargs = {\n        \"args\": f\"{python} {command}\",\n        \"shell\": True,\n        \"env\": os.environ if env is None else env,\n        \"encoding\": \"utf8\",\n        \"errors\": \"ignore\",\n    }\n\n    print(run_kwargs[\"args\"])\n\n    result = subprocess.run(**run_kwargs)\n\n    if result.returncode != 0:\n        print(f\"Error running command: {command}\", file=sys.stderr)\n        raise RuntimeError(f\"Error running command: {command}\")\n\n    return result.stdout or \"\"","file":"/src/streamdiffusion/pip_utils.py","language":"python","fileHash":"ccb6d55479085996365e1ab12a07c61a175ca8c06cc98d0d35826d2a8aff1fe0","hash":"a06f259b2bfb32c01402ec6644a67a8d69a9044d8b09bd12a6963a9f2c96eaf0","processedContent":"def run_python(command: str, env: Dict[str, str] = None) -> str: #undefined\n    run_kwargs = {\n        \"args\": f\"{python} {command}\", #undefined\n        \"shell\": True,\n        \"env\": os.environ if env is None else env,\n        \"\"\"\n        scip-python python python-stdlib 3.11 os/__init__:: undefined\n        scip-python python python-stdlib 3.11 os/environ.environ.: undefined\n        \"\"\"\n        \"encoding\": \"utf8\",\n        \"errors\": \"ignore\",\n    }\n\n    print(run_kwargs[\"args\"])\n\n    result = subprocess.run(**run_kwargs)\n    \"\"\"\n    scip-python python python-stdlib 3.11 subprocess/__init__:: undefined\n    scip-python python python-stdlib 3.11 subprocess/run().: undefined\n    \"\"\"\n\n    if result.returncode != 0:\n        print(f\"Error running command: {command}\", file=sys.stderr)\n        \"\"\"\n        scip-python python python-stdlib 3.11 sys/__init__:: undefined\n        scip-python python python-stdlib 3.11 sys/stderr.stderr.: undefined\n        \"\"\"\n        raise RuntimeError(f\"Error running command: {command}\")\n\n    return result.stdout or \"\"","documentation":"This code defines a function called `run_python` that takes a command and an environment as input, runs the command in a subprocess, and returns the output of the command. The function uses the `subprocess` module to run the command and captures the output. If the command fails, the function raises a `RuntimeError`."}},{"key":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/run_pip().","attributes":{"range":[50,0,51,47],"symbol":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/run_pip().","content":"def run_pip(command: str, env: Dict[str, str] = None) -> str:\n    return run_python(f\"-m pip {command}\", env)","file":"/src/streamdiffusion/pip_utils.py","language":"python","fileHash":"ccb6d55479085996365e1ab12a07c61a175ca8c06cc98d0d35826d2a8aff1fe0","hash":"6640c06bc2d573be5abd8a89aec18e10611daa6558afc01cf9869c8045ae1ec2","processedContent":"def run_pip(command: str, env: Dict[str, str] = None) -> str: #undefined\n    return run_python(f\"-m pip {command}\", env)","documentation":"This code defines a function called `run_pip` that runs the Python package manager (pip) with a given command and environment variables."}},{"key":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/__init__:","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/__init__:","range":[0,0,52,0],"content":"import importlib\nimport importlib.util\nimport os\nimport subprocess\nimport sys\nfrom typing import Dict, Optional\n\nfrom packaging.version import Version\n\n\npython = sys.executable\nindex_url = os.environ.get(\"INDEX_URL\", \"\")\n\n\ndef version(package: str) -> Optional[Version]:\n    try:\n        return Version(importlib.import_module(package).__version__)\n    except ModuleNotFoundError:\n        return None\n\n\ndef is_installed(package: str) -> bool:\n    try:\n        spec = importlib.util.find_spec(package)\n    except ModuleNotFoundError:\n        return False\n\n    return spec is not None\n\n\ndef run_python(command: str, env: Dict[str, str] = None) -> str:\n    run_kwargs = {\n        \"args\": f\"{python} {command}\",\n        \"shell\": True,\n        \"env\": os.environ if env is None else env,\n        \"encoding\": \"utf8\",\n        \"errors\": \"ignore\",\n    }\n\n    print(run_kwargs[\"args\"])\n\n    result = subprocess.run(**run_kwargs)\n\n    if result.returncode != 0:\n        print(f\"Error running command: {command}\", file=sys.stderr)\n        raise RuntimeError(f\"Error running command: {command}\")\n\n    return result.stdout or \"\"\n\n\ndef run_pip(command: str, env: Dict[str, str] = None) -> str:\n    return run_python(f\"-m pip {command}\", env)\n","file":"/src/streamdiffusion/pip_utils.py","language":"python","fileHash":"ccb6d55479085996365e1ab12a07c61a175ca8c06cc98d0d35826d2a8aff1fe0","hash":"ccb6d55479085996365e1ab12a07c61a175ca8c06cc98d0d35826d2a8aff1fe0","processedContent":"import importlib #undefined\nimport importlib.util #undefined\nimport os #undefined\nimport subprocess #undefined\nimport sys #undefined\nfrom typing import Dict, Optional\n\"\"\"\nscip-python python python-stdlib 3.11 typing/__init__:: undefined\nscip-python python python-stdlib 3.11 typing/Dict.: undefined\nscip-python python python-stdlib 3.11 typing/Optional.: undefined\n\"\"\"\n\nfrom packaging.version import Version\n\"\"\"\nscip-python python packaging 23.1 `packaging.version`/__init__:: undefined\nscip-python python packaging 23.1 `packaging.version`/Version#: undefined\n\"\"\"\n\n\npython = sys.executable\n\"\"\"\nscip-python python python-stdlib 3.11 sys/__init__:: undefined\nscip-python python python-stdlib 3.11 sys/executable.executable.: undefined\n\"\"\"\nindex_url = os.environ.get(\"INDEX_URL\", \"\")\n\"\"\"\nscip-python python python-stdlib 3.11 os/__init__:: undefined\nscip-python python python-stdlib 3.11 os/environ.environ.: undefined\nscip-python python python-stdlib 3.11 typing/Mapping#get().: undefined\n\"\"\"\n\n\ndef version(package: str) -> Optional[Version]:\n    \"\"\"This code defines a function called `version` that takes a string argument representing a package name and returns the version of that package if it is installed, or None if it is not.\"\"\"\n    pass\n\n\ndef is_installed(package: str) -> bool:\n    \"\"\"This code checks if a package is installed by attempting to import it and returning True if the import succeeds, False otherwise.\"\"\"\n    pass\n\n\ndef run_python(command: str, env: Dict[str, str] = None) -> str:\n    \"\"\"This code defines a function called `run_python` that takes a command and an environment as input, runs the command in a subprocess, and returns the output of the command. The function uses the `subprocess` module to run the command and captures the output. If the command fails, the function raises a `RuntimeError`.\"\"\"\n    pass\n\n\ndef run_pip(command: str, env: Dict[str, str] = None) -> str:\n    \"\"\"This code defines a function called `run_pip` that runs the Python package manager (pip) with a given command and environment variables.\"\"\"\n    pass\n","documentation":"The code defines a set of functions for working with Python packages, including `version`, `is_installed`, and `run_python` and `run_pip`. These functions use the `subprocess` module to run commands in a subprocess and capture their output."}},{"key":"scip-python python python-stdlib 3.11 importlib/__init__:","attributes":{"symbol":"scip-python python python-stdlib 3.11 importlib/__init__:","language":"python"}},{"key":"scip-python python python-stdlib 3.11 `importlib.util`/__init__:","attributes":{"symbol":"scip-python python python-stdlib 3.11 `importlib.util`/__init__:","language":"python"}},{"key":"scip-python python python-stdlib 3.11 subprocess/__init__:","attributes":{"symbol":"scip-python python python-stdlib 3.11 subprocess/__init__:","language":"python"}},{"key":"scip-python python packaging 23.1 `packaging.version`/__init__:","attributes":{"symbol":"scip-python python packaging 23.1 `packaging.version`/__init__:","language":"python"}},{"key":"scip-python python packaging 23.1 `packaging.version`/Version#","attributes":{"symbol":"scip-python python packaging 23.1 `packaging.version`/Version#","language":"python"}},{"key":"scip-python python python-stdlib 3.11 sys/executable.executable.","attributes":{"symbol":"scip-python python python-stdlib 3.11 sys/executable.executable.","language":"python"}},{"key":"scip-python python python-stdlib 3.11 importlib/import_module().","attributes":{"symbol":"scip-python python python-stdlib 3.11 importlib/import_module().","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/python.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/python.","language":"python"}},{"key":"scip-python python python-stdlib 3.11 subprocess/run().","attributes":{"symbol":"scip-python python python-stdlib 3.11 subprocess/run().","language":"python"}},{"key":"scip-python python python-stdlib 3.11 sys/stderr.stderr.","attributes":{"symbol":"scip-python python python-stdlib 3.11 sys/stderr.stderr.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__init__().","attributes":{"range":[16,4,76,35],"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__init__().","content":"def __init__(\n        self,\n        pipe: StableDiffusionPipeline,\n        t_index_list: List[int],\n        torch_dtype: torch.dtype = torch.float16,\n        width: int = 512,\n        height: int = 512,\n        do_add_noise: bool = True,\n        use_denoising_batch: bool = True,\n        frame_buffer_size: int = 1,\n        cfg_type: Literal[\"none\", \"full\", \"self\", \"initialize\"] = \"self\",\n    ) -> None:\n        self.device = pipe.device\n        self.dtype = torch_dtype\n        self.generator = None\n\n        self.height = height\n        self.width = width\n\n        self.latent_height = int(height // pipe.vae_scale_factor)\n        self.latent_width = int(width // pipe.vae_scale_factor)\n\n        self.frame_bff_size = frame_buffer_size\n        self.denoising_steps_num = len(t_index_list)\n\n        self.cfg_type = cfg_type\n\n        if use_denoising_batch:\n            self.batch_size = self.denoising_steps_num * frame_buffer_size\n            if self.cfg_type == \"initialize\":\n                self.trt_unet_batch_size = (\n                    self.denoising_steps_num + 1\n                ) * self.frame_bff_size\n            elif self.cfg_type == \"full\":\n                self.trt_unet_batch_size = (\n                    2 * self.denoising_steps_num * self.frame_bff_size\n                )\n            else:\n                self.trt_unet_batch_size = self.denoising_steps_num * frame_buffer_size\n        else:\n            self.trt_unet_batch_size = self.frame_bff_size\n            self.batch_size = frame_buffer_size\n\n        self.t_list = t_index_list\n\n        self.do_add_noise = do_add_noise\n        self.use_denoising_batch = use_denoising_batch\n\n        self.similar_image_filter = False\n        self.similar_filter = SimilarImageFilter()\n        self.prev_image_result = None\n\n        self.pipe = pipe\n        self.image_processor = VaeImageProcessor(pipe.vae_scale_factor)\n\n        self.scheduler = LCMScheduler.from_config(self.pipe.scheduler.config)\n        self.text_encoder = pipe.text_encoder\n        self.unet = pipe.unet\n        self.vae = pipe.vae\n\n        self.inference_time_ema = 0","file":"/src/streamdiffusion/pipeline.py","language":"python","fileHash":"a2ae37545d7ab715dd08b4f49d75565816a53cbafc1457eb03e2c61597cd6eba","hash":"cf0bef40f69209a94b5193da7b267f2cbdfc8558ba87bcdcda94e236a1e965e3","processedContent":"def __init__(\n        self,\n        pipe: StableDiffusionPipeline,\n        t_index_list: List[int], #undefined\n        torch_dtype: torch.dtype = torch.float16,\n        width: int = 512,\n        height: int = 512,\n        do_add_noise: bool = True,\n        use_denoising_batch: bool = True,\n        frame_buffer_size: int = 1,\n        cfg_type: Literal[\"none\", \"full\", \"self\", \"initialize\"] = \"self\", #undefined\n    ) -> None:\n        self.device = pipe.device\n        self.dtype = torch_dtype\n        self.generator = None\n\n        self.height = height\n        self.width = width\n\n        self.latent_height = int(height // pipe.vae_scale_factor)\n        self.latent_width = int(width // pipe.vae_scale_factor)\n\n        self.frame_bff_size = frame_buffer_size\n        self.denoising_steps_num = len(t_index_list)\n\n        self.cfg_type = cfg_type\n\n        if use_denoising_batch:\n            self.batch_size = self.denoising_steps_num * frame_buffer_size #undefined\n            if self.cfg_type == \"initialize\": #undefined\n                self.trt_unet_batch_size = ( #undefined\n                    self.denoising_steps_num + 1 #undefined\n                ) * self.frame_bff_size #undefined\n            elif self.cfg_type == \"full\": #undefined\n                self.trt_unet_batch_size = (\n                    2 * self.denoising_steps_num * self.frame_bff_size\n                    \"\"\"\n                    scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#denoising_steps_num.: undefined\n                    scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#frame_bff_size.: undefined\n                    \"\"\"\n                )\n            else:\n                self.trt_unet_batch_size = self.denoising_steps_num * frame_buffer_size\n                \"\"\"\n                scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#denoising_steps_num.: undefined\n                scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#trt_unet_batch_size.: undefined\n                \"\"\"\n        else:\n            self.trt_unet_batch_size = self.frame_bff_size\n            \"\"\"\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#frame_bff_size.: undefined\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#trt_unet_batch_size.: undefined\n            \"\"\"\n            self.batch_size = frame_buffer_size #undefined\n\n        self.t_list = t_index_list\n\n        self.do_add_noise = do_add_noise\n        self.use_denoising_batch = use_denoising_batch\n\n        self.similar_image_filter = False\n        self.similar_filter = SimilarImageFilter() #This code defines a class called SimilarImageFilter that initializes a threshold value and a maximum number of frames to skip when comparing two tensors. It also sets up a cosine similarity metric and a counter for the number of frames skipped. The class has a __call__ function that takes a tensor as input and returns a new tensor based on cosine similarity, and two additional functions to set the threshold and maximum frame skip values.\n        self.prev_image_result = None\n\n        self.pipe = pipe\n        self.image_processor = VaeImageProcessor(pipe.vae_scale_factor)\n\n        self.scheduler = LCMScheduler.from_config(self.pipe.scheduler.config) #undefined\n        self.text_encoder = pipe.text_encoder\n        self.unet = pipe.unet\n        self.vae = pipe.vae\n\n        self.inference_time_ema = 0","documentation":"This code defines a class called StreamDiffusion that initializes various parameters and objects, including a pipeline, an image processor, a scheduler, a text encoder, and a U-Net. It also defines a SimilarImageFilter class to compare two tensors based on cosine similarity."}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#load_lcm_lora().","attributes":{"range":[78,4,88,9],"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#load_lcm_lora().","content":"def load_lcm_lora(\n        self,\n        pretrained_model_name_or_path_or_dict: Union[\n            str, Dict[str, torch.Tensor]\n        ] = \"latent-consistency/lcm-lora-sdv1-5\",\n        adapter_name: Optional[Any] = None,\n        **kwargs,\n    ) -> None:\n        self.pipe.load_lora_weights(\n            pretrained_model_name_or_path_or_dict, adapter_name, **kwargs\n        )","file":"/src/streamdiffusion/pipeline.py","language":"python","fileHash":"a2ae37545d7ab715dd08b4f49d75565816a53cbafc1457eb03e2c61597cd6eba","hash":"856f6e40c1d287c6911868700a4b733770037195d4ee00fbdaff145d8331efa7","processedContent":"def load_lcm_lora(\n        self,\n        pretrained_model_name_or_path_or_dict: Union[ #undefined\n            str, Dict[str, torch.Tensor] #undefined\n        ] = \"latent-consistency/lcm-lora-sdv1-5\",\n        adapter_name: Optional[Any] = None,\n        \"\"\"\n        scip-python python python-stdlib 3.11 typing/Optional.: undefined\n        scip-python python python-stdlib 3.11 typing/Any.: undefined\n        \"\"\"\n        **kwargs,\n    ) -> None:\n        self.pipe.load_lora_weights( #undefined\n            pretrained_model_name_or_path_or_dict, adapter_name, **kwargs\n        )","documentation":"This code defines a function called `load_lcm_lora` that loads pre-trained weights for a machine learning model. The function takes in several parameters, including the name or path of the pre-trained model, an adapter name, and keyword arguments. It then calls another function called `load_lora_weights` to load the weights into the model."}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#load_lora().","attributes":{"range":[90,4,98,9],"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#load_lora().","content":"def load_lora(\n        self,\n        pretrained_lora_model_name_or_path_or_dict: Union[str, Dict[str, torch.Tensor]],\n        adapter_name: Optional[Any] = None,\n        **kwargs,\n    ) -> None:\n        self.pipe.load_lora_weights(\n            pretrained_lora_model_name_or_path_or_dict, adapter_name, **kwargs\n        )","file":"/src/streamdiffusion/pipeline.py","language":"python","fileHash":"a2ae37545d7ab715dd08b4f49d75565816a53cbafc1457eb03e2c61597cd6eba","hash":"3b554fb6ee28de878769caedea2c2fca8176ac2c1a0ce01c999dcb569f7536eb","processedContent":"def load_lora(\n        self,\n        pretrained_lora_model_name_or_path_or_dict: Union[str, Dict[str, torch.Tensor]],\n        \"\"\"\n        scip-python python python-stdlib 3.11 typing/Union.: undefined\n        scip-python python python-stdlib 3.11 typing/Dict.: undefined\n        \"\"\"\n        adapter_name: Optional[Any] = None,\n        \"\"\"\n        scip-python python python-stdlib 3.11 typing/Optional.: undefined\n        scip-python python python-stdlib 3.11 typing/Any.: undefined\n        \"\"\"\n        **kwargs,\n    ) -> None:\n        self.pipe.load_lora_weights( #undefined\n            pretrained_lora_model_name_or_path_or_dict, adapter_name, **kwargs\n        )","documentation":"This code defines a function called `load_lora` that loads pre-trained weights for a language model. It takes in a few parameters, including the name or path of the pre-trained model, an adapter name, and some keyword arguments. The function then calls another function called `load_lora_weights` to load the weights into the model."}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#fuse_lora().","attributes":{"range":[100,4,112,9],"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#fuse_lora().","content":"def fuse_lora(\n        self,\n        fuse_unet: bool = True,\n        fuse_text_encoder: bool = True,\n        lora_scale: float = 1.0,\n        safe_fusing: bool = False,\n    ) -> None:\n        self.pipe.fuse_lora(\n            fuse_unet=fuse_unet,\n            fuse_text_encoder=fuse_text_encoder,\n            lora_scale=lora_scale,\n            safe_fusing=safe_fusing,\n        )","file":"/src/streamdiffusion/pipeline.py","language":"python","fileHash":"a2ae37545d7ab715dd08b4f49d75565816a53cbafc1457eb03e2c61597cd6eba","hash":"f3c03e125674f276dd128cae71d0d90657cb3509ae2c8a2918e05e2d721c860d","processedContent":"def fuse_lora(\n        self,\n        fuse_unet: bool = True,\n        fuse_text_encoder: bool = True,\n        lora_scale: float = 1.0,\n        safe_fusing: bool = False,\n    ) -> None:\n        self.pipe.fuse_lora( #undefined\n            fuse_unet=fuse_unet,\n            fuse_text_encoder=fuse_text_encoder,\n            lora_scale=lora_scale,\n            safe_fusing=safe_fusing,\n        )","documentation":"This code defines a function called `fuse_lora` that fuses the LoRA model with other models in the pipeline. It takes several parameters, including `fuse_unet`, `fuse_text_encoder`, `lora_scale`, and `safe_fusing`. The function modifies the pipeline by adding or removing certain components based on the input parameters."}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#enable_similar_image_filter().","attributes":{"range":[114,4,117,62],"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#enable_similar_image_filter().","content":"def enable_similar_image_filter(self, threshold: float = 0.98, max_skip_frame: float = 10) -> None:\n        self.similar_image_filter = True\n        self.similar_filter.set_threshold(threshold)\n        self.similar_filter.set_max_skip_frame(max_skip_frame)","file":"/src/streamdiffusion/pipeline.py","language":"python","fileHash":"a2ae37545d7ab715dd08b4f49d75565816a53cbafc1457eb03e2c61597cd6eba","hash":"0d3354158835da3d39661c56601ad950dcedc741685e2cffd90e39bfc168dc4f","processedContent":"def enable_similar_image_filter(self, threshold: float = 0.98, max_skip_frame: float = 10) -> None:\n        self.similar_image_filter = True #undefined\n        self.similar_filter.set_threshold(threshold)\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#similar_filter.: undefined\n        scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#set_threshold().: This code sets a threshold value for the class instance. It takes a float argument and assigns it to the instance's threshold attribute.\n        \"\"\"\n        self.similar_filter.set_max_skip_frame(max_skip_frame)","documentation":"This code enables a similar image filter for the StreamDiffusion class. It sets a threshold value and maximum number of frames to skip when comparing images."}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#disable_similar_image_filter().","attributes":{"range":[119,4,120,41],"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#disable_similar_image_filter().","content":"def disable_similar_image_filter(self) -> None:\n        self.similar_image_filter = False","file":"/src/streamdiffusion/pipeline.py","language":"python","fileHash":"a2ae37545d7ab715dd08b4f49d75565816a53cbafc1457eb03e2c61597cd6eba","hash":"80e5cbbc85426530a0d749159d0611ae07e0f7b4f8bb34139dc57ba450a7cc88","processedContent":"def disable_similar_image_filter(self) -> None:\n        self.similar_image_filter = False","documentation":"This code disables the similar image filter in the application."}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","attributes":{"range":[122,4,251,9],"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","content":"@torch.no_grad()\n    def prepare(\n        self,\n        prompt: str,\n        negative_prompt: str = \"\",\n        num_inference_steps: int = 50,\n        guidance_scale: float = 1.2,\n        delta: float = 1.0,\n        generator: Optional[torch.Generator] = torch.Generator(),\n        seed: int = 2,\n    ) -> None:\n        self.generator = generator\n        self.generator.manual_seed(seed)\n        # initialize x_t_latent (it can be any random tensor)\n        if self.denoising_steps_num > 1:\n            self.x_t_latent_buffer = torch.zeros(\n                (\n                    (self.denoising_steps_num - 1) * self.frame_bff_size,\n                    4,\n                    self.latent_height,\n                    self.latent_width,\n                ),\n                dtype=self.dtype,\n                device=self.device,\n            )\n        else:\n            self.x_t_latent_buffer = None\n\n        if self.cfg_type == \"none\":\n            self.guidance_scale = 1.0\n        else:\n            self.guidance_scale = guidance_scale\n        self.delta = delta\n\n        do_classifier_free_guidance = False\n        if self.guidance_scale > 1.0:\n            do_classifier_free_guidance = True\n\n        encoder_output = self.pipe.encode_prompt(\n            prompt=prompt,\n            device=self.device,\n            num_images_per_prompt=1,\n            do_classifier_free_guidance=do_classifier_free_guidance,\n            negative_prompt=negative_prompt,\n        )\n        self.prompt_embeds = encoder_output[0].repeat(self.batch_size, 1, 1)\n\n        if self.use_denoising_batch and self.cfg_type == \"full\":\n            uncond_prompt_embeds = encoder_output[1].repeat(self.batch_size, 1, 1)\n        elif self.cfg_type == \"initialize\":\n            uncond_prompt_embeds = encoder_output[1].repeat(self.frame_bff_size, 1, 1)\n\n        if self.guidance_scale > 1.0 and (\n            self.cfg_type == \"initialize\" or self.cfg_type == \"full\"\n        ):\n            self.prompt_embeds = torch.cat(\n                [uncond_prompt_embeds, self.prompt_embeds], dim=0\n            )\n\n        self.scheduler.set_timesteps(num_inference_steps, self.device)\n        self.timesteps = self.scheduler.timesteps.to(self.device)\n\n        # make sub timesteps list based on the indices in the t_list list and the values in the timesteps list\n        self.sub_timesteps = []\n        for t in self.t_list:\n            self.sub_timesteps.append(self.timesteps[t])\n\n        sub_timesteps_tensor = torch.tensor(\n            self.sub_timesteps, dtype=torch.long, device=self.device\n        )\n        self.sub_timesteps_tensor = torch.repeat_interleave(\n            sub_timesteps_tensor,\n            repeats=self.frame_bff_size if self.use_denoising_batch else 1,\n            dim=0,\n        )\n\n        self.init_noise = torch.randn(\n            (self.batch_size, 4, self.latent_height, self.latent_width),\n            generator=generator,\n        ).to(device=self.device, dtype=self.dtype)\n\n        self.stock_noise = torch.zeros_like(self.init_noise)\n\n        c_skip_list = []\n        c_out_list = []\n        for timestep in self.sub_timesteps:\n            c_skip, c_out = self.scheduler.get_scalings_for_boundary_condition_discrete(\n                timestep\n            )\n            c_skip_list.append(c_skip)\n            c_out_list.append(c_out)\n\n        self.c_skip = (\n            torch.stack(c_skip_list)\n            .view(len(self.t_list), 1, 1, 1)\n            .to(dtype=self.dtype, device=self.device)\n        )\n        self.c_out = (\n            torch.stack(c_out_list)\n            .view(len(self.t_list), 1, 1, 1)\n            .to(dtype=self.dtype, device=self.device)\n        )\n\n        alpha_prod_t_sqrt_list = []\n        beta_prod_t_sqrt_list = []\n        for timestep in self.sub_timesteps:\n            alpha_prod_t_sqrt = self.scheduler.alphas_cumprod[timestep].sqrt()\n            beta_prod_t_sqrt = (1 - self.scheduler.alphas_cumprod[timestep]).sqrt()\n            alpha_prod_t_sqrt_list.append(alpha_prod_t_sqrt)\n            beta_prod_t_sqrt_list.append(beta_prod_t_sqrt)\n        alpha_prod_t_sqrt = (\n            torch.stack(alpha_prod_t_sqrt_list)\n            .view(len(self.t_list), 1, 1, 1)\n            .to(dtype=self.dtype, device=self.device)\n        )\n        beta_prod_t_sqrt = (\n            torch.stack(beta_prod_t_sqrt_list)\n            .view(len(self.t_list), 1, 1, 1)\n            .to(dtype=self.dtype, device=self.device)\n        )\n        self.alpha_prod_t_sqrt = torch.repeat_interleave(\n            alpha_prod_t_sqrt,\n            repeats=self.frame_bff_size if self.use_denoising_batch else 1,\n            dim=0,\n        )\n        self.beta_prod_t_sqrt = torch.repeat_interleave(\n            beta_prod_t_sqrt,\n            repeats=self.frame_bff_size if self.use_denoising_batch else 1,\n            dim=0,\n        )","file":"/src/streamdiffusion/pipeline.py","language":"python","fileHash":"a2ae37545d7ab715dd08b4f49d75565816a53cbafc1457eb03e2c61597cd6eba","hash":"705f5f2abf9f0b353c70b675923d434d9d2ba9fc43edbd69c76e338d18fcb247","processedContent":"@torch.no_grad()\n    def prepare(\n        self,\n        prompt: str,\n        negative_prompt: str = \"\",\n        num_inference_steps: int = 50,\n        guidance_scale: float = 1.2,\n        delta: float = 1.0,\n        generator: Optional[torch.Generator] = torch.Generator(), #undefined\n        seed: int = 2,\n    ) -> None:\n        self.generator = generator #undefined\n        self.generator.manual_seed(seed) #undefined\n        # initialize x_t_latent (it can be any random tensor)\n        if self.denoising_steps_num > 1: #undefined\n            self.x_t_latent_buffer = torch.zeros(\n                (\n                    (self.denoising_steps_num - 1) * self.frame_bff_size,\n                    \"\"\"\n                    scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#denoising_steps_num.: undefined\n                    scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#frame_bff_size.: undefined\n                    \"\"\"\n                    4,\n                    self.latent_height, #undefined\n                    self.latent_width, #undefined\n                ),\n                dtype=self.dtype, #undefined\n                device=self.device, #undefined\n            )\n        else:\n            self.x_t_latent_buffer = None #undefined\n\n        if self.cfg_type == \"none\": #undefined\n            self.guidance_scale = 1.0\n        else:\n            self.guidance_scale = guidance_scale #undefined\n        self.delta = delta\n\n        do_classifier_free_guidance = False\n        if self.guidance_scale > 1.0: #undefined\n            do_classifier_free_guidance = True\n\n        encoder_output = self.pipe.encode_prompt( #undefined\n            prompt=prompt,\n            device=self.device,\n            num_images_per_prompt=1,\n            do_classifier_free_guidance=do_classifier_free_guidance,\n            negative_prompt=negative_prompt,\n        )\n        self.prompt_embeds = encoder_output[0].repeat(self.batch_size, 1, 1) #undefined\n\n        if self.use_denoising_batch and self.cfg_type == \"full\":\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#cfg_type.: undefined\n        scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#use_denoising_batch.: undefined\n        \"\"\"\n            uncond_prompt_embeds = encoder_output[1].repeat(self.batch_size, 1, 1) #undefined\n        elif self.cfg_type == \"initialize\": #undefined\n            uncond_prompt_embeds = encoder_output[1].repeat(self.frame_bff_size, 1, 1) #undefined\n\n        if self.guidance_scale > 1.0 and ( #undefined\n            self.cfg_type == \"initialize\" or self.cfg_type == \"full\"\n            \"\"\"\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#cfg_type.: undefined\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#cfg_type.: undefined\n            \"\"\"\n        ):\n            self.prompt_embeds = torch.cat( #undefined\n                [uncond_prompt_embeds, self.prompt_embeds], dim=0 #undefined\n            )\n\n        self.scheduler.set_timesteps(num_inference_steps, self.device)\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#device.: undefined\n        scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#scheduler.: undefined\n        \"\"\"\n        self.timesteps = self.scheduler.timesteps.to(self.device)\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#device.: undefined\n        scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#scheduler.: undefined\n        \"\"\"\n\n        # make sub timesteps list based on the indices in the t_list list and the values in the timesteps list\n        self.sub_timesteps = []\n        for t in self.t_list: #undefined\n            self.sub_timesteps.append(self.timesteps[t])\n            \"\"\"\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#sub_timesteps.: undefined\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#timesteps.: undefined\n            \"\"\"\n\n        sub_timesteps_tensor = torch.tensor(\n            self.sub_timesteps, dtype=torch.long, device=self.device\n            \"\"\"\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#device.: undefined\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#sub_timesteps.: undefined\n            \"\"\"\n        )\n        self.sub_timesteps_tensor = torch.repeat_interleave(\n            sub_timesteps_tensor,\n            repeats=self.frame_bff_size if self.use_denoising_batch else 1,\n            \"\"\"\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#frame_bff_size.: undefined\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#use_denoising_batch.: undefined\n            \"\"\"\n            \"\"\"\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#frame_bff_size.: undefined\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#use_denoising_batch.: undefined\n            \"\"\"\n            \"\"\"\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#frame_bff_size.: undefined\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#use_denoising_batch.: undefined\n            \"\"\"\n            dim=0,\n        )\n\n        self.init_noise = torch.randn(\n            (self.batch_size, 4, self.latent_height, self.latent_width),\n            \"\"\"\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#latent_height.: undefined\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#latent_width.: undefined\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#batch_size.: undefined\n            \"\"\"\n            generator=generator,\n        ).to(device=self.device, dtype=self.dtype)\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#dtype.: undefined\n        scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#device.: undefined\n        \"\"\"\n\n        self.stock_noise = torch.zeros_like(self.init_noise) #undefined\n\n        c_skip_list = []\n        c_out_list = []\n        for timestep in self.sub_timesteps: #undefined\n            c_skip, c_out = self.scheduler.get_scalings_for_boundary_condition_discrete( #undefined\n                timestep\n            )\n            c_skip_list.append(c_skip)\n            c_out_list.append(c_out)\n\n        self.c_skip = (\n            torch.stack(c_skip_list)\n            .view(len(self.t_list), 1, 1, 1) #undefined\n            .to(dtype=self.dtype, device=self.device)\n            \"\"\"\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#dtype.: undefined\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#device.: undefined\n            \"\"\"\n            \"\"\"\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#dtype.: undefined\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#device.: undefined\n            \"\"\"\n            \"\"\"\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#dtype.: undefined\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#device.: undefined\n            \"\"\"\n            \"\"\"\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#dtype.: undefined\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#device.: undefined\n            \"\"\"\n        )\n        self.c_out = (\n            torch.stack(c_out_list)\n            .view(len(self.t_list), 1, 1, 1) #undefined\n            .to(dtype=self.dtype, device=self.device)\n        )\n\n        alpha_prod_t_sqrt_list = []\n        beta_prod_t_sqrt_list = []\n        for timestep in self.sub_timesteps: #undefined\n            alpha_prod_t_sqrt = self.scheduler.alphas_cumprod[timestep].sqrt() #undefined\n            beta_prod_t_sqrt = (1 - self.scheduler.alphas_cumprod[timestep]).sqrt() #undefined\n            alpha_prod_t_sqrt_list.append(alpha_prod_t_sqrt)\n            beta_prod_t_sqrt_list.append(beta_prod_t_sqrt)\n        alpha_prod_t_sqrt = (\n            torch.stack(alpha_prod_t_sqrt_list)\n            .view(len(self.t_list), 1, 1, 1) #undefined\n            .to(dtype=self.dtype, device=self.device)\n        )\n        beta_prod_t_sqrt = (\n            torch.stack(beta_prod_t_sqrt_list)\n            .view(len(self.t_list), 1, 1, 1) #undefined\n            .to(dtype=self.dtype, device=self.device)\n        )\n        self.alpha_prod_t_sqrt = torch.repeat_interleave(\n            alpha_prod_t_sqrt,\n            repeats=self.frame_bff_size if self.use_denoising_batch else 1,\n            dim=0,\n        )\n        self.beta_prod_t_sqrt = torch.repeat_interleave(\n            beta_prod_t_sqrt,\n            repeats=self.frame_bff_size if self.use_denoising_batch else 1,\n            dim=0,\n        )","documentation":"This code is a part of a larger program that generates images using a diffusion process. It prepares the necessary variables and structures for the diffusion process by encoding the prompt, setting up the timesteps, and initializing the noise."}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#update_prompt().","attributes":{"range":[253,4,261,76],"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#update_prompt().","content":"@torch.no_grad()\n    def update_prompt(self, prompt: str) -> None:\n        encoder_output = self.pipe.encode_prompt(\n            prompt=prompt,\n            device=self.device,\n            num_images_per_prompt=1,\n            do_classifier_free_guidance=False,\n        )\n        self.prompt_embeds = encoder_output[0].repeat(self.batch_size, 1, 1)","file":"/src/streamdiffusion/pipeline.py","language":"python","fileHash":"a2ae37545d7ab715dd08b4f49d75565816a53cbafc1457eb03e2c61597cd6eba","hash":"6eff465d10af44528cf10c74e064937c6c9c02749ba76d42d1475dc0b90ba3a5","processedContent":"@torch.no_grad()\n    def update_prompt(self, prompt: str) -> None:\n        encoder_output = self.pipe.encode_prompt( #undefined\n            prompt=prompt,\n            device=self.device, #undefined\n            num_images_per_prompt=1,\n            do_classifier_free_guidance=False,\n        )\n        self.prompt_embeds = encoder_output[0].repeat(self.batch_size, 1, 1)","documentation":"This code updates the prompt for a model by encoding it and repeating it to match the batch size."}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#add_noise().","attributes":{"range":[263,4,273,28],"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#add_noise().","content":"def add_noise(\n        self,\n        original_samples: torch.Tensor,\n        noise: torch.Tensor,\n        t_index: int,\n    ) -> torch.Tensor:\n        noisy_samples = (\n            self.alpha_prod_t_sqrt[t_index] * original_samples\n            + self.beta_prod_t_sqrt[t_index] * noise\n        )\n        return noisy_samples","file":"/src/streamdiffusion/pipeline.py","language":"python","fileHash":"a2ae37545d7ab715dd08b4f49d75565816a53cbafc1457eb03e2c61597cd6eba","hash":"b8b539289f90626a1c8a25f4e20699ed03b9713c1c7e223f7096986422618464","processedContent":"def add_noise(\n        self,\n        original_samples: torch.Tensor,\n        noise: torch.Tensor,\n        t_index: int,\n    ) -> torch.Tensor:\n        noisy_samples = (\n            self.alpha_prod_t_sqrt[t_index] * original_samples #undefined\n            + self.beta_prod_t_sqrt[t_index] * noise #undefined\n        )\n        return noisy_samples","documentation":"This code defines a function called `add_noise` that takes in three parameters: `original_samples`, `noise`, and `t_index`. It returns a new tensor that is the sum of the product of `alpha_prod_t_sqrt[t_index]` and `original_samples` and the product of `beta_prod_t_sqrt[t_index]` and `noise`."}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#scheduler_step_batch().","attributes":{"range":[275,4,295,29],"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#scheduler_step_batch().","content":"def scheduler_step_batch(\n        self,\n        model_pred_batch: torch.Tensor,\n        x_t_latent_batch: torch.Tensor,\n        idx: Optional[int] = None,\n    ) -> torch.Tensor:\n        # TODO: use t_list to select beta_prod_t_sqrt\n        if idx is None:\n            F_theta = (\n                x_t_latent_batch - self.beta_prod_t_sqrt * model_pred_batch\n            ) / self.alpha_prod_t_sqrt\n            denoised_batch = self.c_out * F_theta + self.c_skip * x_t_latent_batch\n        else:\n            F_theta = (\n                x_t_latent_batch - self.beta_prod_t_sqrt[idx] * model_pred_batch\n            ) / self.alpha_prod_t_sqrt[idx]\n            denoised_batch = (\n                self.c_out[idx] * F_theta + self.c_skip[idx] * x_t_latent_batch\n            )\n\n        return denoised_batch","file":"/src/streamdiffusion/pipeline.py","language":"python","fileHash":"a2ae37545d7ab715dd08b4f49d75565816a53cbafc1457eb03e2c61597cd6eba","hash":"2929477dfee94c5050f25d7fd7cc3938bad273a493ff5ec3e03cc88eceed1789","processedContent":"def scheduler_step_batch(\n        self,\n        model_pred_batch: torch.Tensor,\n        x_t_latent_batch: torch.Tensor,\n        idx: Optional[int] = None, #undefined\n    ) -> torch.Tensor:\n        # TODO: use t_list to select beta_prod_t_sqrt\n        if idx is None:\n            F_theta = (\n                x_t_latent_batch - self.beta_prod_t_sqrt * model_pred_batch #undefined\n            ) / self.alpha_prod_t_sqrt #undefined\n            denoised_batch = self.c_out * F_theta + self.c_skip * x_t_latent_batch\n            \"\"\"\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#c_out.: undefined\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#c_skip.: undefined\n            \"\"\"\n        else:\n            F_theta = (\n                x_t_latent_batch - self.beta_prod_t_sqrt[idx] * model_pred_batch #undefined\n            ) / self.alpha_prod_t_sqrt[idx] #undefined\n            denoised_batch = (\n                self.c_out[idx] * F_theta + self.c_skip[idx] * x_t_latent_batch\n                \"\"\"\n                scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#c_out.: undefined\n                scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#c_skip.: undefined\n                \"\"\"\n            )\n\n        return denoised_batch","documentation":"This code defines a function called `scheduler_step_batch` that takes in a batch of data and performs some operations on it. It then returns the denoised batch."}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","attributes":{"range":[297,4,370,41],"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","content":"def unet_step(\n        self,\n        x_t_latent: torch.Tensor,\n        t_list: Union[torch.Tensor, list[int]],\n        idx: Optional[int] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        if self.guidance_scale > 1.0 and (self.cfg_type == \"initialize\"):\n            x_t_latent_plus_uc = torch.concat([x_t_latent[0:1], x_t_latent], dim=0)\n            t_list = torch.concat([t_list[0:1], t_list], dim=0)\n        elif self.guidance_scale > 1.0 and (self.cfg_type == \"full\"):\n            x_t_latent_plus_uc = torch.concat([x_t_latent, x_t_latent], dim=0)\n            t_list = torch.concat([t_list, t_list], dim=0)\n        else:\n            x_t_latent_plus_uc = x_t_latent\n\n        model_pred = self.unet(\n            x_t_latent_plus_uc,\n            t_list,\n            encoder_hidden_states=self.prompt_embeds,\n            return_dict=False,\n        )[0]\n\n        if self.guidance_scale > 1.0 and (self.cfg_type == \"initialize\"):\n            noise_pred_text = model_pred[1:]\n            self.stock_noise = torch.concat(\n                [model_pred[0:1], self.stock_noise[1:]], dim=0\n            )  # ここコメントアウトでself out cfg\n        elif self.guidance_scale > 1.0 and (self.cfg_type == \"full\"):\n            noise_pred_uncond, noise_pred_text = model_pred.chunk(2)\n        else:\n            noise_pred_text = model_pred\n        if self.guidance_scale > 1.0 and (\n            self.cfg_type == \"self\" or self.cfg_type == \"initialize\"\n        ):\n            noise_pred_uncond = self.stock_noise * self.delta\n        if self.guidance_scale > 1.0 and self.cfg_type != \"none\":\n            model_pred = noise_pred_uncond + self.guidance_scale * (\n                noise_pred_text - noise_pred_uncond\n            )\n        else:\n            model_pred = noise_pred_text\n\n        # compute the previous noisy sample x_t -> x_t-1\n        if self.use_denoising_batch:\n            denoised_batch = self.scheduler_step_batch(model_pred, x_t_latent, idx)\n            if self.cfg_type == \"self\" or self.cfg_type == \"initialize\":\n                scaled_noise = self.beta_prod_t_sqrt * self.stock_noise\n                delta_x = self.scheduler_step_batch(model_pred, scaled_noise, idx)\n                alpha_next = torch.concat(\n                    [\n                        self.alpha_prod_t_sqrt[1:],\n                        torch.ones_like(self.alpha_prod_t_sqrt[0:1]),\n                    ],\n                    dim=0,\n                )\n                delta_x = alpha_next * delta_x\n                beta_next = torch.concat(\n                    [\n                        self.beta_prod_t_sqrt[1:],\n                        torch.ones_like(self.beta_prod_t_sqrt[0:1]),\n                    ],\n                    dim=0,\n                )\n                delta_x = delta_x / beta_next\n                init_noise = torch.concat(\n                    [self.init_noise[1:], self.init_noise[0:1]], dim=0\n                )\n                self.stock_noise = init_noise + delta_x\n\n        else:\n            # denoised_batch = self.scheduler.step(model_pred, t_list[0], x_t_latent).denoised\n            denoised_batch = self.scheduler_step_batch(model_pred, x_t_latent, idx)\n\n        return denoised_batch, model_pred","file":"/src/streamdiffusion/pipeline.py","language":"python","fileHash":"a2ae37545d7ab715dd08b4f49d75565816a53cbafc1457eb03e2c61597cd6eba","hash":"e0952e875259c7e0844fab5e3290df5e8ccdadeacf66437534c19ba38eb74265","processedContent":"def unet_step(\n        self,\n        x_t_latent: torch.Tensor,\n        t_list: Union[torch.Tensor, list[int]], #undefined\n        idx: Optional[int] = None, #undefined\n    ) -> Tuple[torch.Tensor, torch.Tensor]: #undefined\n        if self.guidance_scale > 1.0 and (self.cfg_type == \"initialize\"):\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#guidance_scale.: undefined\n        scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#cfg_type.: undefined\n        \"\"\"\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#guidance_scale.: undefined\n        scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#cfg_type.: undefined\n        \"\"\"\n            x_t_latent_plus_uc = torch.concat([x_t_latent[0:1], x_t_latent], dim=0)\n            t_list = torch.concat([t_list[0:1], t_list], dim=0)\n        elif self.guidance_scale > 1.0 and (self.cfg_type == \"full\"):\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#guidance_scale.: undefined\n        scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#cfg_type.: undefined\n        \"\"\"\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#guidance_scale.: undefined\n        scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#cfg_type.: undefined\n        \"\"\"\n            x_t_latent_plus_uc = torch.concat([x_t_latent, x_t_latent], dim=0)\n            t_list = torch.concat([t_list, t_list], dim=0)\n        else:\n            x_t_latent_plus_uc = x_t_latent\n\n        model_pred = self.unet( #undefined\n            x_t_latent_plus_uc,\n            t_list,\n            encoder_hidden_states=self.prompt_embeds, #undefined\n            return_dict=False,\n        )[0]\n\n        if self.guidance_scale > 1.0 and (self.cfg_type == \"initialize\"):\n            noise_pred_text = model_pred[1:]\n            self.stock_noise = torch.concat( #undefined\n                [model_pred[0:1], self.stock_noise[1:]], dim=0 #undefined\n            )  # ここコメントアウトでself out cfg\n        elif self.guidance_scale > 1.0 and (self.cfg_type == \"full\"):\n            noise_pred_uncond, noise_pred_text = model_pred.chunk(2)\n        else:\n            noise_pred_text = model_pred\n        if self.guidance_scale > 1.0 and ( #undefined\n            self.cfg_type == \"self\" or self.cfg_type == \"initialize\"\n            \"\"\"\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#cfg_type.: undefined\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#cfg_type.: undefined\n            \"\"\"\n        ):\n            noise_pred_uncond = self.stock_noise * self.delta\n            \"\"\"\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#stock_noise.: undefined\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#delta.: undefined\n            \"\"\"\n        if self.guidance_scale > 1.0 and self.cfg_type != \"none\":\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#guidance_scale.: undefined\n        scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#cfg_type.: undefined\n        \"\"\"\n            model_pred = noise_pred_uncond + self.guidance_scale * ( #undefined\n                noise_pred_text - noise_pred_uncond\n            )\n        else:\n            model_pred = noise_pred_text\n\n        # compute the previous noisy sample x_t -> x_t-1\n        if self.use_denoising_batch: #undefined\n            denoised_batch = self.scheduler_step_batch(model_pred, x_t_latent, idx) #This code defines a function called `scheduler_step_batch` that takes in a batch of data and performs some operations on it. It then returns the denoised batch.\n            if self.cfg_type == \"self\" or self.cfg_type == \"initialize\":\n            \"\"\"\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#cfg_type.: undefined\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#cfg_type.: undefined\n            \"\"\"\n                scaled_noise = self.beta_prod_t_sqrt * self.stock_noise\n                \"\"\"\n                scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#stock_noise.: undefined\n                scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#beta_prod_t_sqrt.: undefined\n                \"\"\"\n                delta_x = self.scheduler_step_batch(model_pred, scaled_noise, idx) #This code defines a function called `scheduler_step_batch` that takes in a batch of data and performs some operations on it. It then returns the denoised batch.\n                alpha_next = torch.concat(\n                    [\n                        self.alpha_prod_t_sqrt[1:], #undefined\n                        torch.ones_like(self.alpha_prod_t_sqrt[0:1]), #undefined\n                    ],\n                    dim=0,\n                )\n                delta_x = alpha_next * delta_x\n                beta_next = torch.concat(\n                    [\n                        self.beta_prod_t_sqrt[1:], #undefined\n                        torch.ones_like(self.beta_prod_t_sqrt[0:1]), #undefined\n                    ],\n                    dim=0,\n                )\n                delta_x = delta_x / beta_next\n                init_noise = torch.concat(\n                    [self.init_noise[1:], self.init_noise[0:1]], dim=0\n                    \"\"\"\n                    scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#init_noise.: undefined\n                    scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#init_noise.: undefined\n                    \"\"\"\n                )\n                self.stock_noise = init_noise + delta_x #undefined\n\n        else:\n            # denoised_batch = self.scheduler.step(model_pred, t_list[0], x_t_latent).denoised\n            denoised_batch = self.scheduler_step_batch(model_pred, x_t_latent, idx) #This code defines a function called `scheduler_step_batch` that takes in a batch of data and performs some operations on it. It then returns the denoised batch.\n\n        return denoised_batch, model_pred","documentation":"This code defines a function called `unet_step` that takes in a tensor and performs some operations on it. It then returns the denoised batch."}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#encode_image().","attributes":{"range":[372,4,380,25],"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#encode_image().","content":"def encode_image(self, image_tensors: torch.Tensor) -> torch.Tensor:\n        image_tensors = image_tensors.to(\n            device=self.device,\n            dtype=self.vae.dtype,\n        )\n        img_latent = retrieve_latents(self.vae.encode(image_tensors), self.generator)\n        img_latent = img_latent * self.vae.config.scaling_factor\n        x_t_latent = self.add_noise(img_latent, self.init_noise[0], 0)\n        return x_t_latent","file":"/src/streamdiffusion/pipeline.py","language":"python","fileHash":"a2ae37545d7ab715dd08b4f49d75565816a53cbafc1457eb03e2c61597cd6eba","hash":"dc704b252cadc0bc2325d551f45a9a610d92ff062c2d68181450670b006c056c","processedContent":"def encode_image(self, image_tensors: torch.Tensor) -> torch.Tensor:\n        image_tensors = image_tensors.to(\n            device=self.device, #undefined\n            dtype=self.vae.dtype, #undefined\n        )\n        img_latent = retrieve_latents(self.vae.encode(image_tensors), self.generator)\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#vae.: undefined\n        scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#generator.: undefined\n        \"\"\"\n        img_latent = img_latent * self.vae.config.scaling_factor #undefined\n        x_t_latent = self.add_noise(img_latent, self.init_noise[0], 0)\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#add_noise().: This code defines a function called `add_noise` that takes in three parameters: `original_samples`, `noise`, and `t_index`. It returns a new tensor that is the sum of the product of `alpha_prod_t_sqrt[t_index]` and `original_samples` and the product of `beta_prod_t_sqrt[t_index]` and `noise`.\n        scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#init_noise.: undefined\n        \"\"\"\n        return x_t_latent","documentation":"This code defines a function called `encode_image` that takes in an image tensor and returns a new tensor that is the sum of the product of `alpha_prod_t_sqrt[t_index]` and `original_samples` and the product of `beta_prod_t_sqrt[t_index]` and `noise`."}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#decode_image().","attributes":{"range":[382,4,386,28],"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#decode_image().","content":"def decode_image(self, x_0_pred_out: torch.Tensor) -> torch.Tensor:\n        output_latent = self.vae.decode(\n            x_0_pred_out / self.vae.config.scaling_factor, return_dict=False\n        )[0]\n        return output_latent","file":"/src/streamdiffusion/pipeline.py","language":"python","fileHash":"a2ae37545d7ab715dd08b4f49d75565816a53cbafc1457eb03e2c61597cd6eba","hash":"bd854a86e76b71b63e79b0fc20a328343c70a6ea78fc35600a7b0f90930507bf","processedContent":"def decode_image(self, x_0_pred_out: torch.Tensor) -> torch.Tensor:\n        output_latent = self.vae.decode( #undefined\n            x_0_pred_out / self.vae.config.scaling_factor, return_dict=False #undefined\n        )[0]\n        return output_latent","documentation":"This code is a function that takes in a tensor and uses it to decode an image using a VAE model. It returns the decoded image as a tensor."}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#predict_x0_batch().","attributes":{"range":[388,4,436,27],"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#predict_x0_batch().","content":"def predict_x0_batch(self, x_t_latent: torch.Tensor) -> torch.Tensor:\n        prev_latent_batch = self.x_t_latent_buffer\n\n        if self.use_denoising_batch:\n            t_list = self.sub_timesteps_tensor\n            if self.denoising_steps_num > 1:\n                x_t_latent = torch.cat((x_t_latent, prev_latent_batch), dim=0)\n                self.stock_noise = torch.cat(\n                    (self.init_noise[0:1], self.stock_noise[:-1]), dim=0\n                )\n            x_0_pred_batch, model_pred = self.unet_step(x_t_latent, t_list)\n\n            if self.denoising_steps_num > 1:\n                x_0_pred_out = x_0_pred_batch[-1].unsqueeze(0)\n                if self.do_add_noise:\n                    self.x_t_latent_buffer = (\n                        self.alpha_prod_t_sqrt[1:] * x_0_pred_batch[:-1]\n                        + self.beta_prod_t_sqrt[1:] * self.init_noise[1:]\n                    )\n                else:\n                    self.x_t_latent_buffer = (\n                        self.alpha_prod_t_sqrt[1:] * x_0_pred_batch[:-1]\n                    )\n            else:\n                x_0_pred_out = x_0_pred_batch\n                self.x_t_latent_buffer = None\n        else:\n            self.init_noise = x_t_latent\n            for idx, t in enumerate(self.sub_timesteps_tensor):\n                t = t.view(\n                    1,\n                ).repeat(\n                    self.frame_bff_size,\n                )\n                x_0_pred, model_pred = self.unet_step(x_t_latent, t, idx)\n                if idx < len(self.sub_timesteps_tensor) - 1:\n                    if self.do_add_noise:\n                        x_t_latent = self.alpha_prod_t_sqrt[\n                            idx + 1\n                        ] * x_0_pred + self.beta_prod_t_sqrt[\n                            idx + 1\n                        ] * torch.randn_like(\n                            x_0_pred, device=self.device, dtype=self.dtype\n                        )\n                    else:\n                        x_t_latent = self.alpha_prod_t_sqrt[idx + 1] * x_0_pred\n            x_0_pred_out = x_0_pred\n\n        return x_0_pred_out","file":"/src/streamdiffusion/pipeline.py","language":"python","fileHash":"a2ae37545d7ab715dd08b4f49d75565816a53cbafc1457eb03e2c61597cd6eba","hash":"129922959c67166e01a3622b1e0f37d3201f931c2d0b18bf686ddcf21d1a145b","processedContent":"def predict_x0_batch(self, x_t_latent: torch.Tensor) -> torch.Tensor:\n        prev_latent_batch = self.x_t_latent_buffer #undefined\n\n        if self.use_denoising_batch: #undefined\n            t_list = self.sub_timesteps_tensor #undefined\n            if self.denoising_steps_num > 1: #undefined\n                x_t_latent = torch.cat((x_t_latent, prev_latent_batch), dim=0)\n                self.stock_noise = torch.cat( #undefined\n                    (self.init_noise[0:1], self.stock_noise[:-1]), dim=0\n                    \"\"\"\n                    scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#stock_noise.: undefined\n                    scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#init_noise.: undefined\n                    \"\"\"\n                )\n            x_0_pred_batch, model_pred = self.unet_step(x_t_latent, t_list) #This code defines a function called `unet_step` that takes in a tensor and performs some operations on it. It then returns the denoised batch.\n\n            if self.denoising_steps_num > 1: #undefined\n                x_0_pred_out = x_0_pred_batch[-1].unsqueeze(0)\n                if self.do_add_noise: #undefined\n                    self.x_t_latent_buffer = ( #undefined\n                        self.alpha_prod_t_sqrt[1:] * x_0_pred_batch[:-1] #undefined\n                        + self.beta_prod_t_sqrt[1:] * self.init_noise[1:]\n                        \"\"\"\n                        scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#init_noise.: undefined\n                        scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#beta_prod_t_sqrt.: undefined\n                        \"\"\"\n                    )\n                else:\n                    self.x_t_latent_buffer = ( #undefined\n                        self.alpha_prod_t_sqrt[1:] * x_0_pred_batch[:-1] #undefined\n                    )\n            else:\n                x_0_pred_out = x_0_pred_batch\n                self.x_t_latent_buffer = None #undefined\n        else:\n            self.init_noise = x_t_latent #undefined\n            for idx, t in enumerate(self.sub_timesteps_tensor): #undefined\n                t = t.view(\n                    1,\n                ).repeat(\n                    self.frame_bff_size, #undefined\n                )\n                x_0_pred, model_pred = self.unet_step(x_t_latent, t, idx) #This code defines a function called `unet_step` that takes in a tensor and performs some operations on it. It then returns the denoised batch.\n                if idx < len(self.sub_timesteps_tensor) - 1: #undefined\n                    if self.do_add_noise: #undefined\n                        x_t_latent = self.alpha_prod_t_sqrt[ #undefined\n                            idx + 1\n                        ] * x_0_pred + self.beta_prod_t_sqrt[ #undefined\n                            idx + 1\n                        ] * torch.randn_like(\n                            x_0_pred, device=self.device, dtype=self.dtype\n                            \"\"\"\n                            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#device.: undefined\n                            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#dtype.: undefined\n                            \"\"\"\n                        )\n                    else:\n                        x_t_latent = self.alpha_prod_t_sqrt[idx + 1] * x_0_pred #undefined\n            x_0_pred_out = x_0_pred\n\n        return x_0_pred_out","documentation":"This code defines a function called `predict_x0_batch` that takes in a tensor and performs some operations on it. It then returns the denoised batch."}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__call__().","attributes":{"range":[438,4,468,23],"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__call__().","content":"@torch.no_grad()\n    def __call__(\n        self, x: Union[torch.Tensor, PIL.Image.Image, np.ndarray] = None\n    ) -> torch.Tensor:\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        start.record()\n        if x is not None:\n            x = self.image_processor.preprocess(x, self.height, self.width).to(\n                device=self.device, dtype=self.dtype\n            )\n            if self.similar_image_filter:\n                x = self.similar_filter(x)\n                if x is None:\n                    time.sleep(self.inference_time_ema)\n                    return self.prev_image_result\n            x_t_latent = self.encode_image(x)\n        else:\n            # TODO: check the dimension of x_t_latent\n            x_t_latent = torch.randn((1, 4, self.latent_height, self.latent_width)).to(\n                device=self.device, dtype=self.dtype\n            )\n        x_0_pred_out = self.predict_x0_batch(x_t_latent)\n        x_output = self.decode_image(x_0_pred_out).detach().clone()\n\n        self.prev_image_result = x_output\n        end.record()\n        torch.cuda.synchronize()\n        inference_time = start.elapsed_time(end) / 1000\n        self.inference_time_ema = 0.9 * self.inference_time_ema + 0.1 * inference_time\n        return x_output","file":"/src/streamdiffusion/pipeline.py","language":"python","fileHash":"a2ae37545d7ab715dd08b4f49d75565816a53cbafc1457eb03e2c61597cd6eba","hash":"dad6a0c57f9333c07c43a6da04922d27423431a48d0b667b6f284e4960870aba","processedContent":"@torch.no_grad()\n    def __call__(\n        self, x: Union[torch.Tensor, PIL.Image.Image, np.ndarray] = None\n        \"\"\"\n        scip-python python python-stdlib 3.11 typing/Union.: undefined\n        scip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\n        scip-python python Pillow 10.0.0 `pil.image`/Image#: undefined\n        scip-python python numpy 1.25.2 numpy/ndarray#: undefined\n        \"\"\"\n    ) -> torch.Tensor:\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        start.record()\n        if x is not None:\n            x = self.image_processor.preprocess(x, self.height, self.width).to(\n            \"\"\"\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#image_processor.: undefined\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#height.: undefined\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#width.: undefined\n            \"\"\"\n                device=self.device, dtype=self.dtype\n                \"\"\"\n                scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#device.: undefined\n                scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#dtype.: undefined\n                \"\"\"\n                \"\"\"\n                scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#device.: undefined\n                scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#dtype.: undefined\n                \"\"\"\n            )\n            if self.similar_image_filter: #undefined\n                x = self.similar_filter(x) #undefined\n                if x is None:\n                    time.sleep(self.inference_time_ema)\n                    \"\"\"\n                    scip-python python python-stdlib 3.11 time/__init__:: undefined\n                    scip-python python python-stdlib 3.11 time/sleep().: undefined\n                    scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#inference_time_ema.: undefined\n                    \"\"\"\n                    return self.prev_image_result #undefined\n            x_t_latent = self.encode_image(x) #This code defines a function called `encode_image` that takes in an image tensor and returns a new tensor that is the sum of the product of `alpha_prod_t_sqrt[t_index]` and `original_samples` and the product of `beta_prod_t_sqrt[t_index]` and `noise`.\n        else:\n            # TODO: check the dimension of x_t_latent\n            x_t_latent = torch.randn((1, 4, self.latent_height, self.latent_width)).to(\n            \"\"\"\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#latent_height.: undefined\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#latent_width.: undefined\n            \"\"\"\n                device=self.device, dtype=self.dtype\n            )\n        x_0_pred_out = self.predict_x0_batch(x_t_latent) #This code defines a function called `predict_x0_batch` that takes in a tensor and performs some operations on it. It then returns the denoised batch.\n        x_output = self.decode_image(x_0_pred_out).detach().clone() #This code is a function that takes in a tensor and uses it to decode an image using a VAE model. It returns the decoded image as a tensor.\n\n        self.prev_image_result = x_output #undefined\n        end.record()\n        torch.cuda.synchronize()\n        inference_time = start.elapsed_time(end) / 1000\n        self.inference_time_ema = 0.9 * self.inference_time_ema + 0.1 * inference_time\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#inference_time_ema.: undefined\n        scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#inference_time_ema.: undefined\n        \"\"\"\n        return x_output","documentation":"This code defines a function called `StreamDiffusion` that takes in an image tensor and returns a denoised image tensor using a VAE model. It also maintains a moving average of the inference time to improve performance."}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#txt2img().","attributes":{"range":[470,4,478,23],"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#txt2img().","content":"@torch.no_grad()\n    def txt2img(self, batch_size: int = 1) -> torch.Tensor:\n        x_0_pred_out = self.predict_x0_batch(\n            torch.randn((batch_size, 4, self.latent_height, self.latent_width)).to(\n                device=self.device, dtype=self.dtype\n            )\n        )\n        x_output = self.decode_image(x_0_pred_out).detach().clone()\n        return x_output","file":"/src/streamdiffusion/pipeline.py","language":"python","fileHash":"a2ae37545d7ab715dd08b4f49d75565816a53cbafc1457eb03e2c61597cd6eba","hash":"10ec523351d1912689fb5a30904fd1211de9b97dd1a8ca8db8ad52c7e10de539","processedContent":"@torch.no_grad()\n    def txt2img(self, batch_size: int = 1) -> torch.Tensor:\n        x_0_pred_out = self.predict_x0_batch( #This code defines a function called `predict_x0_batch` that takes in a tensor and performs some operations on it. It then returns the denoised batch.\n            torch.randn((batch_size, 4, self.latent_height, self.latent_width)).to(\n            \"\"\"\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#latent_height.: undefined\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#latent_width.: undefined\n            \"\"\"\n                device=self.device, dtype=self.dtype\n                \"\"\"\n                scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#device.: undefined\n                scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#dtype.: undefined\n                \"\"\"\n            )\n        )\n        x_output = self.decode_image(x_0_pred_out).detach().clone() #This code is a function that takes in a tensor and uses it to decode an image using a VAE model. It returns the decoded image as a tensor.\n        return x_output","documentation":"This code defines a function called `txt2img` that takes in a batch size and returns a denoised image tensor using a VAE model."}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#txt2img_sd_turbo().","attributes":{"range":[480,4,495,46],"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#txt2img_sd_turbo().","content":"def txt2img_sd_turbo(self, batch_size: int = 1) -> torch.Tensor:\n        x_t_latent = torch.randn(\n            (batch_size, 4, self.latent_height, self.latent_width),\n            device=self.device,\n            dtype=self.dtype,\n        )\n        model_pred = self.unet(\n            x_t_latent,\n            self.sub_timesteps_tensor,\n            encoder_hidden_states=self.prompt_embeds,\n            return_dict=False,\n        )[0]\n        x_0_pred_out = (\n            x_t_latent - self.beta_prod_t_sqrt * model_pred\n        ) / self.alpha_prod_t_sqrt\n        return self.decode_image(x_0_pred_out)","file":"/src/streamdiffusion/pipeline.py","language":"python","fileHash":"a2ae37545d7ab715dd08b4f49d75565816a53cbafc1457eb03e2c61597cd6eba","hash":"4a89b4469077859217fda948d2ae8580801ceb9a6f0ce119e3fb64564b886d72","processedContent":"def txt2img_sd_turbo(self, batch_size: int = 1) -> torch.Tensor:\n        x_t_latent = torch.randn(\n            (batch_size, 4, self.latent_height, self.latent_width),\n            \"\"\"\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#latent_height.: undefined\n            scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#latent_width.: undefined\n            \"\"\"\n            device=self.device, #undefined\n            dtype=self.dtype, #undefined\n        )\n        model_pred = self.unet( #undefined\n            x_t_latent,\n            self.sub_timesteps_tensor, #undefined\n            encoder_hidden_states=self.prompt_embeds, #undefined\n            return_dict=False,\n        )[0]\n        x_0_pred_out = (\n            x_t_latent - self.beta_prod_t_sqrt * model_pred #undefined\n        ) / self.alpha_prod_t_sqrt #undefined\n        return self.decode_image(x_0_pred_out)","documentation":"This code defines a function called `txt2img_sd_turbo` that takes in a batch size and returns a tensor representing an image. The function first generates random noise, then passes it through a neural network to generate an output image."}},{"key":"scip-python python python-stdlib 3.11 typing/Any.","attributes":{"symbol":"scip-python python python-stdlib 3.11 typing/Any.","language":"python"}},{"key":"scip-python python temp indexer diffusers/__init__:","attributes":{"symbol":"scip-python python temp indexer diffusers/__init__:","language":"python"}},{"key":"scip-python python temp indexer `diffusers.image_processor`/__init__:","attributes":{"symbol":"scip-python python temp indexer `diffusers.image_processor`/__init__:","language":"python"}},{"key":"scip-python python temp indexer `diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img`/__init__:","attributes":{"symbol":"scip-python python temp indexer `diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img`/__init__:","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#denoising_steps_num.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#denoising_steps_num.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#cfg_type.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#cfg_type.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#frame_bff_size.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#frame_bff_size.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#trt_unet_batch_size.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#trt_unet_batch_size.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#batch_size.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#batch_size.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#pipe.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#pipe.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#similar_image_filter.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#similar_image_filter.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#similar_filter.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#similar_filter.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#generator.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#generator.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#latent_height.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#latent_height.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#latent_width.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#latent_width.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#dtype.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#dtype.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#device.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#device.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#x_t_latent_buffer.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#x_t_latent_buffer.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#guidance_scale.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#guidance_scale.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#use_denoising_batch.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#use_denoising_batch.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prompt_embeds.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prompt_embeds.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#scheduler.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#scheduler.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#t_list.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#t_list.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#sub_timesteps.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#sub_timesteps.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#timesteps.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#timesteps.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#init_noise.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#init_noise.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#alpha_prod_t_sqrt.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#alpha_prod_t_sqrt.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#beta_prod_t_sqrt.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#beta_prod_t_sqrt.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#c_out.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#c_out.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#c_skip.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#c_skip.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#stock_noise.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#stock_noise.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#delta.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#delta.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#vae.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#vae.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#sub_timesteps_tensor.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#sub_timesteps_tensor.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#do_add_noise.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#do_add_noise.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#image_processor.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#image_processor.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#height.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#height.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#width.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#width.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#inference_time_ema.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#inference_time_ema.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prev_image_result.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prev_image_result.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration`/__init__:","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration`/__init__:","range":[0,0,0,0],"content":"","file":"/src/streamdiffusion/acceleration/__init__.py","language":"python","fileHash":"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855","hash":"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855","processedContent":""}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.sfast`/accelerate_with_stable_fast().","attributes":{"range":[7,0,32,17],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.sfast`/accelerate_with_stable_fast().","content":"def accelerate_with_stable_fast(\n    stream: StreamDiffusion,\n    config: Optional[CompilationConfig] = None,\n):\n    if config is None:\n        config = CompilationConfig.Default()\n        # xformers and Triton are suggested for achieving best performance.\n        try:\n            import xformers\n\n            config.enable_xformers = True\n        except ImportError:\n            print(\"xformers not installed, skip\")\n        try:\n            import triton\n\n            config.enable_triton = True\n        except ImportError:\n            print(\"Triton not installed, skip\")\n        # CUDA Graph is suggested for small batch sizes and small resolutions to reduce CPU overhead.\n        config.enable_cuda_graph = True\n    stream.pipe = compile(stream.pipe, config)\n    stream.unet = stream.pipe.unet\n    stream.vae = stream.pipe.vae\n    stream.text_encoder = stream.pipe.text_encoder\n    return stream","file":"/src/streamdiffusion/acceleration/sfast/__init__.py","language":"python","fileHash":"e076ae61a6877ba6bb376b71070099613eaef4d14ccd6abc76e37fc691f81a91","hash":"dafe0a8e3de33bddc138c723b48fc56fde8a2be2d8986498c280fdd22963ca1c","processedContent":"def accelerate_with_stable_fast(\n    stream: StreamDiffusion, #The code defines a class called StreamDiffusion that performs image denoising using a VAE model. It also includes functions for loading pre-trained weights, fusing the LoRA model with other models, and enabling or disabling a similar image filter.\n    config: Optional[CompilationConfig] = None, #undefined\n):\n    if config is None:\n        config = CompilationConfig.Default()\n        # xformers and Triton are suggested for achieving best performance.\n        try:\n            import xformers\n\n            config.enable_xformers = True\n        except ImportError:\n            print(\"xformers not installed, skip\")\n        try:\n            import triton\n\n            config.enable_triton = True\n        except ImportError:\n            print(\"Triton not installed, skip\")\n        # CUDA Graph is suggested for small batch sizes and small resolutions to reduce CPU overhead.\n        config.enable_cuda_graph = True\n    stream.pipe = compile(stream.pipe, config)\n    \"\"\"\n    scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#pipe.: undefined\n    scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#pipe.: undefined\n    \"\"\"\n    stream.unet = stream.pipe.unet\n    \"\"\"\n    scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#pipe.: undefined\n    scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet.: undefined\n    \"\"\"\n    stream.vae = stream.pipe.vae\n    \"\"\"\n    scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#pipe.: undefined\n    scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#vae.: undefined\n    \"\"\"\n    stream.text_encoder = stream.pipe.text_encoder\n    \"\"\"\n    scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#pipe.: undefined\n    scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#text_encoder.: undefined\n    \"\"\"\n    return stream","documentation":"This code defines a function called `accelerate_with_stable_fast` that takes in a `StreamDiffusion` object and a `CompilationConfig` object as arguments. The function modifies the `StreamDiffusion` object by compiling its pipeline using the `compile` function from the `scip-python` library, and then sets the `unet`, `vae`, and `text_encoder` attributes of the `StreamDiffusion` object to the corresponding components of the compiled pipeline."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.sfast`/__init__:","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.sfast`/__init__:","range":[0,0,33,0],"content":"from typing import Optional\n\nfrom sfast.compilers.stable_diffusion_pipeline_compiler import CompilationConfig, compile\n\nfrom ...pipeline import StreamDiffusion\n\n\ndef accelerate_with_stable_fast(\n    stream: StreamDiffusion,\n    config: Optional[CompilationConfig] = None,\n):\n    if config is None:\n        config = CompilationConfig.Default()\n        # xformers and Triton are suggested for achieving best performance.\n        try:\n            import xformers\n\n            config.enable_xformers = True\n        except ImportError:\n            print(\"xformers not installed, skip\")\n        try:\n            import triton\n\n            config.enable_triton = True\n        except ImportError:\n            print(\"Triton not installed, skip\")\n        # CUDA Graph is suggested for small batch sizes and small resolutions to reduce CPU overhead.\n        config.enable_cuda_graph = True\n    stream.pipe = compile(stream.pipe, config)\n    stream.unet = stream.pipe.unet\n    stream.vae = stream.pipe.vae\n    stream.text_encoder = stream.pipe.text_encoder\n    return stream\n","file":"/src/streamdiffusion/acceleration/sfast/__init__.py","language":"python","fileHash":"e076ae61a6877ba6bb376b71070099613eaef4d14ccd6abc76e37fc691f81a91","hash":"e076ae61a6877ba6bb376b71070099613eaef4d14ccd6abc76e37fc691f81a91","processedContent":"from typing import Optional\n\"\"\"\nscip-python python python-stdlib 3.11 typing/__init__:: undefined\nscip-python python python-stdlib 3.11 typing/Optional.: undefined\n\"\"\"\n\nfrom sfast.compilers.stable_diffusion_pipeline_compiler import CompilationConfig, compile #undefined\n\nfrom ...pipeline import StreamDiffusion\n\"\"\"\nscip-python python temp indexer `src.streamdiffusion.pipeline`/__init__:: The code defines a class called StreamDiffusion that performs image denoising using a VAE model, with functions for loading pre-trained weights, fusing the LoRA model with other models, and enabling or disabling a similar image filter.\nscip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#: The code defines a class called StreamDiffusion that performs image denoising using a VAE model. It also includes functions for loading pre-trained weights, fusing the LoRA model with other models, and enabling or disabling a similar image filter.\n\"\"\"\n\n\ndef accelerate_with_stable_fast(\n    \"\"\"This code defines a function called `accelerate_with_stable_fast` that takes in a `StreamDiffusion` object and a `CompilationConfig` object as arguments. The function modifies the `StreamDiffusion` object by compiling its pipeline using the `compile` function from the `scip-python` library, and then sets the `unet`, `vae`, and `text_encoder` attributes of the `StreamDiffusion` object to the corresponding components of the compiled pipeline.\"\"\"\n    pass\n","documentation":"This code defines a function called `accelerate_with_stable_fast` that modifies a `StreamDiffusion` object by compiling its pipeline using the `compile` function from the `scip-python` library and setting the `unet`, `vae`, and `text_encoder` attributes of the `StreamDiffusion` object to the corresponding components of the compiled pipeline."}},{"key":"scip-python python temp indexer `sfast.compilers.stable_diffusion_pipeline_compiler`/__init__:","attributes":{"symbol":"scip-python python temp indexer `sfast.compilers.stable_diffusion_pipeline_compiler`/__init__:","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#text_encoder.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#text_encoder.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/TorchVAEEncoder#","attributes":{"range":[16,0,22,51],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/TorchVAEEncoder#","content":"class TorchVAEEncoder(torch.nn.Module):\n    def __init__(self, vae: AutoencoderKL):\n        super().__init__()\n        self.vae = vae\n\n    def forward(self, x: torch.Tensor):\n        return retrieve_latents(self.vae.encode(x))","file":"/src/streamdiffusion/acceleration/tensorrt/__init__.py","language":"python","fileHash":"439d68d7a93c08798da56cb4f569803613287b560eb61369c161c53154435552","hash":"0b41d106428c2c62b9ae0343388a4b75f35e757644d2dbf2d3878204e229ba7a","processedContent":"class TorchVAEEncoder(torch.nn.Module):\n    def __init__(self, vae: AutoencoderKL):\n        \"\"\"This code initializes a new instance of the class, which is a subclass of another class. It takes an AutoencoderKL object as an argument and assigns it to the self.vae attribute.\"\"\"\n        pass\n\n    def forward(self, x: torch.Tensor):\n        return retrieve_latents(self.vae.encode(x))","documentation":"This code defines a class called TorchVAEEncoder that inherits from the torch.nn.Module class and takes an AutoencoderKL object as an argument in its constructor. It then defines a forward method that retrieves latent representations from the encoded input tensor using the encode method of the AutoencoderKL object assigned to the self.vae attribute."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/TorchVAEEncoder#__init__().","attributes":{"range":[17,4,19,22],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/TorchVAEEncoder#__init__().","content":"def __init__(self, vae: AutoencoderKL):\n        super().__init__()\n        self.vae = vae","file":"/src/streamdiffusion/acceleration/tensorrt/__init__.py","language":"python","fileHash":"439d68d7a93c08798da56cb4f569803613287b560eb61369c161c53154435552","hash":"b82d41c449e4985ac3b70f15592fa2f9d110a664b47799bc797fb6f2a1255cba","processedContent":"def __init__(self, vae: AutoencoderKL):\n        super().__init__()\n        self.vae = vae","documentation":"This code initializes a new instance of the class, which is a subclass of another class. It takes an AutoencoderKL object as an argument and assigns it to the self.vae attribute."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/TorchVAEEncoder#forward().","attributes":{"range":[21,4,22,51],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/TorchVAEEncoder#forward().","content":"def forward(self, x: torch.Tensor):\n        return retrieve_latents(self.vae.encode(x))","file":"/src/streamdiffusion/acceleration/tensorrt/__init__.py","language":"python","fileHash":"439d68d7a93c08798da56cb4f569803613287b560eb61369c161c53154435552","hash":"266d5196e2f2b31fa727c1e4869c57fdf3cdec51483b5b0a79d739509af2e08e","processedContent":"def forward(self, x: torch.Tensor):\n        return retrieve_latents(self.vae.encode(x))","documentation":"This code defines a function called \"forward\" that takes a PyTorch tensor as input and returns the output of a VAE model's encoding process."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/compile_vae_encoder().","attributes":{"range":[25,0,41,5],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/compile_vae_encoder().","content":"def compile_vae_encoder(\n    vae: TorchVAEEncoder,\n    model_data: BaseModel,\n    onnx_path: str,\n    onnx_opt_path: str,\n    engine_path: str,\n    opt_batch_size: int = 1,\n    engine_build_options: dict = {},\n):\n    builder = EngineBuilder(model_data, vae, device=torch.device(\"cuda\"))\n    builder.build(\n        onnx_path,\n        onnx_opt_path,\n        engine_path,\n        opt_batch_size=opt_batch_size,\n        **engine_build_options,\n    )","file":"/src/streamdiffusion/acceleration/tensorrt/__init__.py","language":"python","fileHash":"439d68d7a93c08798da56cb4f569803613287b560eb61369c161c53154435552","hash":"96a851b52ba191c37b498a516060d3aa77caf66850b633124add8b0b7ac2503b","processedContent":"def compile_vae_encoder(\n    vae: TorchVAEEncoder, #This code defines a class called TorchVAEEncoder that inherits from the torch.nn.Module class and takes an AutoencoderKL object as an argument in its constructor. It then defines a forward method that retrieves latent representations from the encoded input tensor using the encode method of the AutoencoderKL object assigned to the self.vae attribute.\n    model_data: BaseModel, #This code defines a class called \"SD Model\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested.\n    onnx_path: str,\n    onnx_opt_path: str,\n    engine_path: str,\n    opt_batch_size: int = 1,\n    engine_build_options: dict = {},\n):\n    builder = EngineBuilder(model_data, vae, device=torch.device(\"cuda\")) #This code defines a class called \"EngineBuilder\" that is used to build an image-text matching model. It has various functions to retrieve information about the model and its inputs and outputs, as well as an `optimize` function that optimizes an ONNX graph and exports it as an ONNX model if requested.\n    builder.build( #The code exports a PyTorch model to ONNX format, optimizes the ONNX graph, and builds a TensorRT engine from the optimized ONNX model.\n        onnx_path,\n        onnx_opt_path,\n        engine_path,\n        opt_batch_size=opt_batch_size,\n        **engine_build_options,\n    )","documentation":"This code defines a function called `compile_vae_encoder` that takes in several arguments and exports a PyTorch model to ONNX format, optimizes the ONNX graph, and builds a TensorRT engine from the optimized ONNX model."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/compile_vae_decoder().","attributes":{"range":[44,0,61,5],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/compile_vae_decoder().","content":"def compile_vae_decoder(\n    vae: AutoencoderKL,\n    model_data: BaseModel,\n    onnx_path: str,\n    onnx_opt_path: str,\n    engine_path: str,\n    opt_batch_size: int = 1,\n    engine_build_options: dict = {},\n):\n    vae = vae.to(torch.device(\"cuda\"))\n    builder = EngineBuilder(model_data, vae, device=torch.device(\"cuda\"))\n    builder.build(\n        onnx_path,\n        onnx_opt_path,\n        engine_path,\n        opt_batch_size=opt_batch_size,\n        **engine_build_options,\n    )","file":"/src/streamdiffusion/acceleration/tensorrt/__init__.py","language":"python","fileHash":"439d68d7a93c08798da56cb4f569803613287b560eb61369c161c53154435552","hash":"0f295fb7128f5c365813b920cc3834449b9faef6938c96c61e8f3eb6729c0d82","processedContent":"def compile_vae_decoder(\n    vae: AutoencoderKL,\n    model_data: BaseModel, #This code defines a class called \"SD Model\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested.\n    onnx_path: str,\n    onnx_opt_path: str,\n    engine_path: str,\n    opt_batch_size: int = 1,\n    engine_build_options: dict = {},\n):\n    vae = vae.to(torch.device(\"cuda\"))\n    builder = EngineBuilder(model_data, vae, device=torch.device(\"cuda\")) #This code defines a class called \"EngineBuilder\" that is used to build an image-text matching model. It has various functions to retrieve information about the model and its inputs and outputs, as well as an `optimize` function that optimizes an ONNX graph and exports it as an ONNX model if requested.\n    builder.build( #The code exports a PyTorch model to ONNX format, optimizes the ONNX graph, and builds a TensorRT engine from the optimized ONNX model.\n        onnx_path,\n        onnx_opt_path,\n        engine_path,\n        opt_batch_size=opt_batch_size,\n        **engine_build_options,\n    )","documentation":"This code defines a function called `compile_vae_decoder` that takes in an AutoencoderKL model, a BaseModel class, and various other parameters. It exports the PyTorch model to ONNX format, optimizes the ONNX graph, and builds a TensorRT engine from the optimized ONNX model."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/compile_unet().","attributes":{"range":[64,0,81,5],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/compile_unet().","content":"def compile_unet(\n    unet: UNet2DConditionModel,\n    model_data: BaseModel,\n    onnx_path: str,\n    onnx_opt_path: str,\n    engine_path: str,\n    opt_batch_size: int = 1,\n    engine_build_options: dict = {},\n):\n    unet = unet.to(torch.device(\"cuda\"), dtype=torch.float16)\n    builder = EngineBuilder(model_data, unet, device=torch.device(\"cuda\"))\n    builder.build(\n        onnx_path,\n        onnx_opt_path,\n        engine_path,\n        opt_batch_size=opt_batch_size,\n        **engine_build_options,\n    )","file":"/src/streamdiffusion/acceleration/tensorrt/__init__.py","language":"python","fileHash":"439d68d7a93c08798da56cb4f569803613287b560eb61369c161c53154435552","hash":"4d753f8ad8396ffdcdc472fa72aeb17784a08d052ca6ac22bb21503119b53016","processedContent":"def compile_unet(\n    unet: UNet2DConditionModel,\n    model_data: BaseModel, #This code defines a class called \"SD Model\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested.\n    onnx_path: str,\n    onnx_opt_path: str,\n    engine_path: str,\n    opt_batch_size: int = 1,\n    engine_build_options: dict = {},\n):\n    unet = unet.to(torch.device(\"cuda\"), dtype=torch.float16)\n    builder = EngineBuilder(model_data, unet, device=torch.device(\"cuda\")) #This code defines a class called \"EngineBuilder\" that is used to build an image-text matching model. It has various functions to retrieve information about the model and its inputs and outputs, as well as an `optimize` function that optimizes an ONNX graph and exports it as an ONNX model if requested.\n    builder.build( #The code exports a PyTorch model to ONNX format, optimizes the ONNX graph, and builds a TensorRT engine from the optimized ONNX model.\n        onnx_path,\n        onnx_opt_path,\n        engine_path,\n        opt_batch_size=opt_batch_size,\n        **engine_build_options,\n    )","documentation":"This code defines a function called `compile_unet` that takes in a PyTorch model, optimizes it using TensorRT, and exports the optimized model to ONNX format."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","attributes":{"range":[84,0,186,17],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","content":"def accelerate_with_tensorrt(\n    stream: StreamDiffusion,\n    engine_dir: str,\n    max_batch_size: int = 2,\n    min_batch_size: int = 1,\n    use_cuda_graph: bool = False,\n    engine_build_options: dict = {},\n):\n    if \"opt_batch_size\" not in engine_build_options or engine_build_options[\"opt_batch_size\"] is None:\n        engine_build_options[\"opt_batch_size\"] = max_batch_size\n    text_encoder = stream.text_encoder\n    unet = stream.unet\n    vae = stream.vae\n\n    del stream.unet, stream.vae, stream.pipe.unet, stream.pipe.vae\n\n    vae_config = vae.config\n    vae_dtype = vae.dtype\n\n    unet.to(torch.device(\"cpu\"))\n    vae.to(torch.device(\"cpu\"))\n\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    onnx_dir = os.path.join(engine_dir, \"onnx\")\n    os.makedirs(onnx_dir, exist_ok=True)\n\n    unet_engine_path = f\"{engine_dir}/unet.engine\"\n    vae_encoder_engine_path = f\"{engine_dir}/vae_encoder.engine\"\n    vae_decoder_engine_path = f\"{engine_dir}/vae_decoder.engine\"\n\n    unet_model = UNet(\n        fp16=True,\n        device=stream.device,\n        max_batch_size=max_batch_size,\n        min_batch_size=min_batch_size,\n        embedding_dim=text_encoder.config.hidden_size,\n        unet_dim=unet.config.in_channels,\n    )\n    vae_decoder_model = VAE(\n        device=stream.device,\n        max_batch_size=max_batch_size,\n        min_batch_size=min_batch_size,\n    )\n    vae_encoder_model = VAEEncoder(\n        device=stream.device,\n        max_batch_size=max_batch_size,\n        min_batch_size=min_batch_size,\n    )\n\n    if not os.path.exists(unet_engine_path):\n        compile_unet(\n            unet,\n            unet_model,\n            create_onnx_path(\"unet\", onnx_dir, opt=False),\n            create_onnx_path(\"unet\", onnx_dir, opt=True),\n            opt_batch_size=max_batch_size,\n            **engine_build_options,\n        )\n    else:\n        del unet\n\n    if not os.path.exists(vae_decoder_engine_path):\n        compile_vae_decoder(\n            vae,\n            vae_decoder_model,\n            create_onnx_path(\"vae_decoder\", onnx_dir, opt=False),\n            create_onnx_path(\"vae_decoder\", onnx_dir, opt=True),\n            opt_batch_size=max_batch_size,\n            **engine_build_options,\n        )\n\n    if not os.path.exists(vae_encoder_engine_path):\n        vae_encoder = TorchVAEEncoder(vae).to(torch.device(\"cuda\"))\n        compile_vae_encoder(\n            vae_encoder,\n            vae_encoder_model,\n            create_onnx_path(\"vae_encoder\", onnx_dir, opt=False),\n            create_onnx_path(\"vae_encoder\", onnx_dir, opt=True),\n            opt_batch_size=max_batch_size,\n            **engine_build_options,\n        )\n\n    del vae\n\n    cuda_steram = cuda.Stream()\n\n    stream.unet = UNet2DConditionModelEngine(unet_engine_path, cuda_steram, use_cuda_graph=use_cuda_graph)\n    stream.vae = AutoencoderKLEngine(\n        vae_encoder_engine_path,\n        vae_decoder_engine_path,\n        cuda_steram,\n        stream.pipe.vae_scale_factor,\n        use_cuda_graph=use_cuda_graph,\n    )\n    setattr(stream.vae, \"config\", vae_config)\n    setattr(stream.vae, \"dtype\", vae_dtype)\n\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    return stream","file":"/src/streamdiffusion/acceleration/tensorrt/__init__.py","language":"python","fileHash":"439d68d7a93c08798da56cb4f569803613287b560eb61369c161c53154435552","hash":"7d93863e691f4668cd9f50cef3b0782d3615755fb4c0415d4459842da41cc572","processedContent":"def accelerate_with_tensorrt(\n    stream: StreamDiffusion, #The code defines a class called StreamDiffusion that performs image denoising using a VAE model. It also includes functions for loading pre-trained weights, fusing the LoRA model with other models, and enabling or disabling a similar image filter.\n    engine_dir: str,\n    max_batch_size: int = 2,\n    min_batch_size: int = 1,\n    use_cuda_graph: bool = False,\n    engine_build_options: dict = {},\n):\n    if \"opt_batch_size\" not in engine_build_options or engine_build_options[\"opt_batch_size\"] is None:\n        engine_build_options[\"opt_batch_size\"] = max_batch_size\n    text_encoder = stream.text_encoder #undefined\n    unet = stream.unet #undefined\n    vae = stream.vae #undefined\n\n    del stream.unet, stream.vae, stream.pipe.unet, stream.pipe.vae\n    \"\"\"\n    scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet.: undefined\n    scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#vae.: undefined\n    scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#pipe.: undefined\n    scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#pipe.: undefined\n    \"\"\"\n\n    vae_config = vae.config\n    vae_dtype = vae.dtype\n\n    unet.to(torch.device(\"cpu\"))\n    vae.to(torch.device(\"cpu\"))\n\n    gc.collect()\n    \"\"\"\n    scip-python python python-stdlib 3.11 gc/__init__:: undefined\n    scip-python python python-stdlib 3.11 gc/collect().: undefined\n    \"\"\"\n    \"\"\"\n    scip-python python python-stdlib 3.11 gc/__init__:: undefined\n    scip-python python python-stdlib 3.11 gc/collect().: undefined\n    \"\"\"\n    torch.cuda.empty_cache()\n\n    onnx_dir = os.path.join(engine_dir, \"onnx\")\n    \"\"\"\n    scip-python python python-stdlib 3.11 os/__init__:: undefined\n    scip-python python python-stdlib 3.11 os/path.: undefined\n    scip-python python python-stdlib 3.11 ntpath/join().: undefined\n    \"\"\"\n    os.makedirs(onnx_dir, exist_ok=True)\n    \"\"\"\n    scip-python python python-stdlib 3.11 os/__init__:: undefined\n    scip-python python python-stdlib 3.11 os/makedirs().: undefined\n    \"\"\"\n\n    unet_engine_path = f\"{engine_dir}/unet.engine\"\n    vae_encoder_engine_path = f\"{engine_dir}/vae_encoder.engine\"\n    vae_decoder_engine_path = f\"{engine_dir}/vae_decoder.engine\"\n\n    unet_model = UNet( #This code defines a class called \"SD Model\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested.\n        fp16=True,\n        device=stream.device, #undefined\n        max_batch_size=max_batch_size,\n        min_batch_size=min_batch_size,\n        embedding_dim=text_encoder.config.hidden_size,\n        unet_dim=unet.config.in_channels,\n    )\n    vae_decoder_model = VAE( #This code defines a class called \"VAE decoder\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested.\n        device=stream.device, #undefined\n        max_batch_size=max_batch_size,\n        min_batch_size=min_batch_size,\n    )\n    vae_encoder_model = VAEEncoder( #This code defines a class called \"VAEEncoder\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested.\n        device=stream.device, #undefined\n        max_batch_size=max_batch_size,\n        min_batch_size=min_batch_size,\n    )\n\n    if not os.path.exists(unet_engine_path):\n    \"\"\"\n    scip-python python python-stdlib 3.11 os/__init__:: undefined\n    scip-python python python-stdlib 3.11 os/path.: undefined\n    scip-python python python-stdlib 3.11 ntpath/join().: undefined\n    \"\"\"\n        compile_unet( #This code defines a function called `compile_unet` that takes in a PyTorch model, optimizes it using TensorRT, and exports the optimized model to ONNX format.\n            unet,\n            unet_model,\n            create_onnx_path(\"unet\", onnx_dir, opt=False), #This code defines a function that creates an ONNX file path based on the name of the model, the directory where the ONNX files are stored, and whether or not to include the \".opt\" extension.\n            create_onnx_path(\"unet\", onnx_dir, opt=True), #This code defines a function that creates an ONNX file path based on the name of the model, the directory where the ONNX files are stored, and whether or not to include the \".opt\" extension.\n            opt_batch_size=max_batch_size,\n            **engine_build_options,\n        )\n    else:\n        del unet\n\n    if not os.path.exists(vae_decoder_engine_path):\n    \"\"\"\n    scip-python python python-stdlib 3.11 os/__init__:: undefined\n    scip-python python python-stdlib 3.11 os/path.: undefined\n    scip-python python python-stdlib 3.11 ntpath/join().: undefined\n    \"\"\"\n        compile_vae_decoder( #This code defines a function called `compile_vae_decoder` that takes in an AutoencoderKL model, a BaseModel class, and various other parameters. It exports the PyTorch model to ONNX format, optimizes the ONNX graph, and builds a TensorRT engine from the optimized ONNX model.\n            vae,\n            vae_decoder_model,\n            create_onnx_path(\"vae_decoder\", onnx_dir, opt=False), #This code defines a function that creates an ONNX file path based on the name of the model, the directory where the ONNX files are stored, and whether or not to include the \".opt\" extension.\n            create_onnx_path(\"vae_decoder\", onnx_dir, opt=True), #This code defines a function that creates an ONNX file path based on the name of the model, the directory where the ONNX files are stored, and whether or not to include the \".opt\" extension.\n            opt_batch_size=max_batch_size,\n            **engine_build_options,\n        )\n\n    if not os.path.exists(vae_encoder_engine_path):\n    \"\"\"\n    scip-python python python-stdlib 3.11 os/__init__:: undefined\n    scip-python python python-stdlib 3.11 os/path.: undefined\n    scip-python python python-stdlib 3.11 ntpath/join().: undefined\n    \"\"\"\n        vae_encoder = TorchVAEEncoder(vae).to(torch.device(\"cuda\")) #This code defines a class called TorchVAEEncoder that inherits from the torch.nn.Module class and takes an AutoencoderKL object as an argument in its constructor. It then defines a forward method that retrieves latent representations from the encoded input tensor using the encode method of the AutoencoderKL object assigned to the self.vae attribute.\n        compile_vae_encoder( #This code defines a function called `compile_vae_encoder` that takes in several arguments and exports a PyTorch model to ONNX format, optimizes the ONNX graph, and builds a TensorRT engine from the optimized ONNX model.\n            vae_encoder,\n            vae_encoder_model,\n            create_onnx_path(\"vae_encoder\", onnx_dir, opt=False), #This code defines a function that creates an ONNX file path based on the name of the model, the directory where the ONNX files are stored, and whether or not to include the \".opt\" extension.\n            create_onnx_path(\"vae_encoder\", onnx_dir, opt=True), #This code defines a function that creates an ONNX file path based on the name of the model, the directory where the ONNX files are stored, and whether or not to include the \".opt\" extension.\n            opt_batch_size=max_batch_size,\n            **engine_build_options,\n        )\n\n    del vae\n\n    cuda_steram = cuda.Stream()\n\n    stream.unet = UNet2DConditionModelEngine(unet_engine_path, cuda_steram, use_cuda_graph=use_cuda_graph)\n    \"\"\"\n    scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet.: undefined\n    scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#: This code defines a class called `UNet2DConditionModelEngine` that initializes an object of the `Engine` class, loads a TensorRT engine from a file path, and sets the tensor addresses for each tensor in the feed dictionary. It also defines a function called `infer` that takes in a feed dictionary, a stream, and an optional boolean flag indicating whether to use CUDA graph, and executes the inference using the TensorRT context.\n    \"\"\"\n    stream.vae = AutoencoderKLEngine(\n    \"\"\"\n    scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#vae.: undefined\n    scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#: The code defines a class called AutoencoderKLEngine that initializes two objects of the Engine class, encoder and decoder, with various attributes and methods for building, loading, and executing TensorRT engines. It also creates execution contexts for the engines using the activate() method.\n    \"\"\"\n        vae_encoder_engine_path,\n        vae_decoder_engine_path,\n        cuda_steram,\n        stream.pipe.vae_scale_factor, #undefined\n        use_cuda_graph=use_cuda_graph,\n    )\n    setattr(stream.vae, \"config\", vae_config) #undefined\n    setattr(stream.vae, \"dtype\", vae_dtype) #undefined\n\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    return stream","documentation":"This code defines a function called `accelerate_with_tensorrt` that takes in a StreamDiffusion object and various parameters, and optimizes the model using TensorRT. It then creates TensorRT engines for the UNet and VAE models and sets the tensor addresses for each tensor in the feed dictionary. The function also creates execution contexts for the engines and returns the modified StreamDiffusion object."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/__init__:","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/__init__:","range":[0,0,187,0],"content":"import gc\nimport os\n\nimport torch\nfrom diffusers import AutoencoderKL, UNet2DConditionModel\nfrom diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img import (\n    retrieve_latents,\n)\nfrom polygraphy import cuda\n\nfrom ...pipeline import StreamDiffusion\nfrom .builder import EngineBuilder, create_onnx_path\nfrom .engine import AutoencoderKLEngine, UNet2DConditionModelEngine\nfrom .models import VAE, BaseModel, UNet, VAEEncoder\n\n\nclass TorchVAEEncoder(torch.nn.Module):\n    def __init__(self, vae: AutoencoderKL):\n        super().__init__()\n        self.vae = vae\n\n    def forward(self, x: torch.Tensor):\n        return retrieve_latents(self.vae.encode(x))\n\n\ndef compile_vae_encoder(\n    vae: TorchVAEEncoder,\n    model_data: BaseModel,\n    onnx_path: str,\n    onnx_opt_path: str,\n    engine_path: str,\n    opt_batch_size: int = 1,\n    engine_build_options: dict = {},\n):\n    builder = EngineBuilder(model_data, vae, device=torch.device(\"cuda\"))\n    builder.build(\n        onnx_path,\n        onnx_opt_path,\n        engine_path,\n        opt_batch_size=opt_batch_size,\n        **engine_build_options,\n    )\n\n\ndef compile_vae_decoder(\n    vae: AutoencoderKL,\n    model_data: BaseModel,\n    onnx_path: str,\n    onnx_opt_path: str,\n    engine_path: str,\n    opt_batch_size: int = 1,\n    engine_build_options: dict = {},\n):\n    vae = vae.to(torch.device(\"cuda\"))\n    builder = EngineBuilder(model_data, vae, device=torch.device(\"cuda\"))\n    builder.build(\n        onnx_path,\n        onnx_opt_path,\n        engine_path,\n        opt_batch_size=opt_batch_size,\n        **engine_build_options,\n    )\n\n\ndef compile_unet(\n    unet: UNet2DConditionModel,\n    model_data: BaseModel,\n    onnx_path: str,\n    onnx_opt_path: str,\n    engine_path: str,\n    opt_batch_size: int = 1,\n    engine_build_options: dict = {},\n):\n    unet = unet.to(torch.device(\"cuda\"), dtype=torch.float16)\n    builder = EngineBuilder(model_data, unet, device=torch.device(\"cuda\"))\n    builder.build(\n        onnx_path,\n        onnx_opt_path,\n        engine_path,\n        opt_batch_size=opt_batch_size,\n        **engine_build_options,\n    )\n\n\ndef accelerate_with_tensorrt(\n    stream: StreamDiffusion,\n    engine_dir: str,\n    max_batch_size: int = 2,\n    min_batch_size: int = 1,\n    use_cuda_graph: bool = False,\n    engine_build_options: dict = {},\n):\n    if \"opt_batch_size\" not in engine_build_options or engine_build_options[\"opt_batch_size\"] is None:\n        engine_build_options[\"opt_batch_size\"] = max_batch_size\n    text_encoder = stream.text_encoder\n    unet = stream.unet\n    vae = stream.vae\n\n    del stream.unet, stream.vae, stream.pipe.unet, stream.pipe.vae\n\n    vae_config = vae.config\n    vae_dtype = vae.dtype\n\n    unet.to(torch.device(\"cpu\"))\n    vae.to(torch.device(\"cpu\"))\n\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    onnx_dir = os.path.join(engine_dir, \"onnx\")\n    os.makedirs(onnx_dir, exist_ok=True)\n\n    unet_engine_path = f\"{engine_dir}/unet.engine\"\n    vae_encoder_engine_path = f\"{engine_dir}/vae_encoder.engine\"\n    vae_decoder_engine_path = f\"{engine_dir}/vae_decoder.engine\"\n\n    unet_model = UNet(\n        fp16=True,\n        device=stream.device,\n        max_batch_size=max_batch_size,\n        min_batch_size=min_batch_size,\n        embedding_dim=text_encoder.config.hidden_size,\n        unet_dim=unet.config.in_channels,\n    )\n    vae_decoder_model = VAE(\n        device=stream.device,\n        max_batch_size=max_batch_size,\n        min_batch_size=min_batch_size,\n    )\n    vae_encoder_model = VAEEncoder(\n        device=stream.device,\n        max_batch_size=max_batch_size,\n        min_batch_size=min_batch_size,\n    )\n\n    if not os.path.exists(unet_engine_path):\n        compile_unet(\n            unet,\n            unet_model,\n            create_onnx_path(\"unet\", onnx_dir, opt=False),\n            create_onnx_path(\"unet\", onnx_dir, opt=True),\n            opt_batch_size=max_batch_size,\n            **engine_build_options,\n        )\n    else:\n        del unet\n\n    if not os.path.exists(vae_decoder_engine_path):\n        compile_vae_decoder(\n            vae,\n            vae_decoder_model,\n            create_onnx_path(\"vae_decoder\", onnx_dir, opt=False),\n            create_onnx_path(\"vae_decoder\", onnx_dir, opt=True),\n            opt_batch_size=max_batch_size,\n            **engine_build_options,\n        )\n\n    if not os.path.exists(vae_encoder_engine_path):\n        vae_encoder = TorchVAEEncoder(vae).to(torch.device(\"cuda\"))\n        compile_vae_encoder(\n            vae_encoder,\n            vae_encoder_model,\n            create_onnx_path(\"vae_encoder\", onnx_dir, opt=False),\n            create_onnx_path(\"vae_encoder\", onnx_dir, opt=True),\n            opt_batch_size=max_batch_size,\n            **engine_build_options,\n        )\n\n    del vae\n\n    cuda_steram = cuda.Stream()\n\n    stream.unet = UNet2DConditionModelEngine(unet_engine_path, cuda_steram, use_cuda_graph=use_cuda_graph)\n    stream.vae = AutoencoderKLEngine(\n        vae_encoder_engine_path,\n        vae_decoder_engine_path,\n        cuda_steram,\n        stream.pipe.vae_scale_factor,\n        use_cuda_graph=use_cuda_graph,\n    )\n    setattr(stream.vae, \"config\", vae_config)\n    setattr(stream.vae, \"dtype\", vae_dtype)\n\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    return stream\n","file":"/src/streamdiffusion/acceleration/tensorrt/__init__.py","language":"python","fileHash":"439d68d7a93c08798da56cb4f569803613287b560eb61369c161c53154435552","hash":"439d68d7a93c08798da56cb4f569803613287b560eb61369c161c53154435552","processedContent":"import gc #undefined\nimport os #undefined\n\nimport torch\nfrom diffusers import AutoencoderKL, UNet2DConditionModel #undefined\nfrom diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img import ( #undefined\n    retrieve_latents,\n)\nfrom polygraphy import cuda #undefined\n\nfrom ...pipeline import StreamDiffusion\n\"\"\"\nscip-python python temp indexer `src.streamdiffusion.pipeline`/__init__:: The code defines a class called StreamDiffusion that performs image denoising using a VAE model, with functions for loading pre-trained weights, fusing the LoRA model with other models, and enabling or disabling a similar image filter.\nscip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#: The code defines a class called StreamDiffusion that performs image denoising using a VAE model. It also includes functions for loading pre-trained weights, fusing the LoRA model with other models, and enabling or disabling a similar image filter.\n\"\"\"\nfrom .builder import EngineBuilder, create_onnx_path\n\"\"\"\nscip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/__init__:: undefined\nscip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#: This code defines a class called \"EngineBuilder\" that is used to build an image-text matching model. It has various functions to retrieve information about the model and its inputs and outputs, as well as an `optimize` function that optimizes an ONNX graph and exports it as an ONNX model if requested.\nscip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/create_onnx_path().: This code defines a function that creates an ONNX file path based on the name of the model, the directory where the ONNX files are stored, and whether or not to include the \".opt\" extension.\n\"\"\"\nfrom .engine import AutoencoderKLEngine, UNet2DConditionModelEngine\n\"\"\"\nscip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/__init__:: The code defines two classes, `UNet2DConditionModelEngine` and `AutoencoderKLEngine`, which are used to build and execute TensorRT engines for image processing tasks.\nscip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#: The code defines a class called AutoencoderKLEngine that initializes two objects of the Engine class, encoder and decoder, with various attributes and methods for building, loading, and executing TensorRT engines. It also creates execution contexts for the engines using the activate() method.\nscip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#: This code defines a class called `UNet2DConditionModelEngine` that initializes an object of the `Engine` class, loads a TensorRT engine from a file path, and sets the tensor addresses for each tensor in the feed dictionary. It also defines a function called `infer` that takes in a feed dictionary, a stream, and an optional boolean flag indicating whether to use CUDA graph, and executes the inference using the TensorRT context.\n\"\"\"\nfrom .models import VAE, BaseModel, UNet, VAEEncoder\n\"\"\"\nscip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/__init__:: The code defines a series of classes for image-text matching, including Optimizer, BaseModel, CLIP, UNet, VAE, and VAEEncoder. These classes are used to optimize ONNX graphs, retrieve information about the models and their inputs and outputs, and export optimized graphs as ONNX models if requested.\nscip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#: This code defines a class called \"VAE decoder\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested.\nscip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#: This code defines a class called \"SD Model\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested.\nscip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#: This code defines a class called \"SD Model\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested.\nscip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#: This code defines a class called \"VAEEncoder\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested.\n\"\"\"\n\n\nclass TorchVAEEncoder(torch.nn.Module):\n    \"\"\"This code defines a class called TorchVAEEncoder that inherits from the torch.nn.Module class and takes an AutoencoderKL object as an argument in its constructor. It then defines a forward method that retrieves latent representations from the encoded input tensor using the encode method of the AutoencoderKL object assigned to the self.vae attribute.\"\"\"\n    pass\n\n\ndef compile_vae_encoder(\n    \"\"\"This code defines a function called `compile_vae_encoder` that takes in several arguments and exports a PyTorch model to ONNX format, optimizes the ONNX graph, and builds a TensorRT engine from the optimized ONNX model.\"\"\"\n    pass\n\n\ndef compile_vae_decoder(\n    \"\"\"This code defines a function called `compile_vae_decoder` that takes in an AutoencoderKL model, a BaseModel class, and various other parameters. It exports the PyTorch model to ONNX format, optimizes the ONNX graph, and builds a TensorRT engine from the optimized ONNX model.\"\"\"\n    pass\n\n\ndef compile_unet(\n    \"\"\"This code defines a function called `compile_unet` that takes in a PyTorch model, optimizes it using TensorRT, and exports the optimized model to ONNX format.\"\"\"\n    pass\n\n\ndef accelerate_with_tensorrt(\n    \"\"\"This code defines a function called `accelerate_with_tensorrt` that takes in a StreamDiffusion object and various parameters, and optimizes the model using TensorRT. It then creates TensorRT engines for the UNet and VAE models and sets the tensor addresses for each tensor in the feed dictionary. The function also creates execution contexts for the engines and returns the modified StreamDiffusion object.\"\"\"\n    pass\n","documentation":"The code defines a class called StreamDiffusion that performs image denoising using a VAE model, with functions for loading pre-trained weights, fusing the LoRA model with other models, and enabling or disabling a similar image filter."}},{"key":"scip-python python python-stdlib 3.11 gc/__init__:","attributes":{"symbol":"scip-python python python-stdlib 3.11 gc/__init__:","language":"python"}},{"key":"scip-python python temp indexer polygraphy/__init__:","attributes":{"symbol":"scip-python python temp indexer polygraphy/__init__:","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/__init__:","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/__init__:","language":"python","range":[0,0,94,0],"content":"import gc\nimport os\nfrom typing import *\n\nimport torch\n\nfrom .models import BaseModel\nfrom .utilities import (\n    build_engine,\n    export_onnx,\n    optimize_onnx,\n)\n\n\ndef create_onnx_path(name, onnx_dir, opt=True):\n    return os.path.join(onnx_dir, name + (\".opt\" if opt else \"\") + \".onnx\")\n\n\nclass EngineBuilder:\n    def __init__(\n        self,\n        model: BaseModel,\n        network: Any,\n        device=torch.device(\"cuda\"),\n    ):\n        self.device = device\n\n        self.model = model\n        self.network = network\n\n    def build(\n        self,\n        onnx_path: str,\n        onnx_opt_path: str,\n        engine_path: str,\n        opt_image_height: int = 512,\n        opt_image_width: int = 512,\n        opt_batch_size: int = 1,\n        min_image_resolution: int = 256,\n        max_image_resolution: int = 1024,\n        build_enable_refit: bool = False,\n        build_static_batch: bool = False,\n        build_dynamic_shape: bool = False,\n        build_all_tactics: bool = False,\n        onnx_opset: int = 17,\n        force_engine_build: bool = False,\n        force_onnx_export: bool = False,\n        force_onnx_optimize: bool = False,\n    ):\n        if not force_onnx_export and os.path.exists(onnx_path):\n            print(f\"Found cached model: {onnx_path}\")\n        else:\n            print(f\"Exporting model: {onnx_path}\")\n            export_onnx(\n                self.network,\n                onnx_path=onnx_path,\n                model_data=self.model,\n                opt_image_height=opt_image_height,\n                opt_image_width=opt_image_width,\n                opt_batch_size=opt_batch_size,\n                onnx_opset=onnx_opset,\n            )\n            del self.network\n            gc.collect()\n            torch.cuda.empty_cache()\n        if not force_onnx_optimize and os.path.exists(onnx_opt_path):\n            print(f\"Found cached model: {onnx_opt_path}\")\n        else:\n            print(f\"Generating optimizing model: {onnx_opt_path}\")\n            optimize_onnx(\n                onnx_path=onnx_path,\n                onnx_opt_path=onnx_opt_path,\n                model_data=self.model,\n            )\n        self.model.min_latent_shape = min_image_resolution // 8\n        self.model.max_latent_shape = max_image_resolution // 8\n        if not force_engine_build and os.path.exists(engine_path):\n            print(f\"Found cached engine: {engine_path}\")\n        else:\n            build_engine(\n                engine_path=engine_path,\n                onnx_opt_path=onnx_opt_path,\n                model_data=self.model,\n                opt_image_height=opt_image_height,\n                opt_image_width=opt_image_width,\n                opt_batch_size=opt_batch_size,\n                build_static_batch=build_static_batch,\n                build_dynamic_shape=build_dynamic_shape,\n                build_all_tactics=build_all_tactics,\n                build_enable_refit=build_enable_refit,\n            )\n\n        gc.collect()\n        torch.cuda.empty_cache()\n","file":"/src/streamdiffusion/acceleration/tensorrt/builder.py","fileHash":"aa86d5cd723aa7f220318f2e9933ff7dbdc8e34e7840d30c4846726ed044fdc6","hash":"aa86d5cd723aa7f220318f2e9933ff7dbdc8e34e7840d30c4846726ed044fdc6","processedContent":"import gc #undefined\nimport os #undefined\nfrom typing import * #undefined\n\nimport torch\n\nfrom .models import BaseModel\n\"\"\"\nscip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/__init__:: The code defines a series of classes for image-text matching, including Optimizer, BaseModel, CLIP, UNet, VAE, and VAEEncoder. These classes are used to optimize ONNX graphs, retrieve information about the models and their inputs and outputs, and export optimized graphs as ONNX models if requested.\nscip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#: This code defines a class called \"SD Model\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested.\n\"\"\"\nfrom .utilities import ( #The code defines a class called StreamDiffusion that performs image denoising using a VAE model, with functions for loading pre-trained weights, fusing the LoRA model with other models, and enabling or disabling a similar image filter.\n    build_engine, #This code defines a function called `build_engine` that builds a TensorRT engine from an ONNX model, allowing for customization of the engine's performance through various parameters.\n    export_onnx, #This code exports a PyTorch model to ONNX format, which is a widely used open-source format for machine learning models. It takes in various parameters such as the model, input dimensions, and optimization options, and exports the model to an ONNX file.\n    optimize_onnx, #This code optimizes an ONNX graph and exports it as an ONNX model if requested. It uses the `Optimizer` class to clean up the graph, select outputs, fold constants, and infer shapes. Additionally, it clears the GPU cache using `torch.cuda.empty_cache()`.\n)\n\n\ndef create_onnx_path(name, onnx_dir, opt=True):\n    \"\"\"This code defines a function that creates an ONNX file path based on the name of the model, the directory where the ONNX files are stored, and whether or not to include the \".opt\" extension.\"\"\"\n    pass\n\n\nclass EngineBuilder:\n    \"\"\"This code defines a class called \"EngineBuilder\" that is used to build an image-text matching model. It has various functions to retrieve information about the model and its inputs and outputs, as well as an `optimize` function that optimizes an ONNX graph and exports it as an ONNX model if requested.\"\"\"\n    pass\n","documentation":"The code defines a series of classes and functions for image-text matching, including Optimizer, BaseModel, CLIP, UNet, VAE, and VAEEncoder. These classes and functions are used to optimize ONNX graphs, retrieve information about the models and their inputs and outputs, and export optimized graphs as ONNX models if requested."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#","language":"python","range":[18,0,93,32],"content":"class EngineBuilder:\n    def __init__(\n        self,\n        model: BaseModel,\n        network: Any,\n        device=torch.device(\"cuda\"),\n    ):\n        self.device = device\n\n        self.model = model\n        self.network = network\n\n    def build(\n        self,\n        onnx_path: str,\n        onnx_opt_path: str,\n        engine_path: str,\n        opt_image_height: int = 512,\n        opt_image_width: int = 512,\n        opt_batch_size: int = 1,\n        min_image_resolution: int = 256,\n        max_image_resolution: int = 1024,\n        build_enable_refit: bool = False,\n        build_static_batch: bool = False,\n        build_dynamic_shape: bool = False,\n        build_all_tactics: bool = False,\n        onnx_opset: int = 17,\n        force_engine_build: bool = False,\n        force_onnx_export: bool = False,\n        force_onnx_optimize: bool = False,\n    ):\n        if not force_onnx_export and os.path.exists(onnx_path):\n            print(f\"Found cached model: {onnx_path}\")\n        else:\n            print(f\"Exporting model: {onnx_path}\")\n            export_onnx(\n                self.network,\n                onnx_path=onnx_path,\n                model_data=self.model,\n                opt_image_height=opt_image_height,\n                opt_image_width=opt_image_width,\n                opt_batch_size=opt_batch_size,\n                onnx_opset=onnx_opset,\n            )\n            del self.network\n            gc.collect()\n            torch.cuda.empty_cache()\n        if not force_onnx_optimize and os.path.exists(onnx_opt_path):\n            print(f\"Found cached model: {onnx_opt_path}\")\n        else:\n            print(f\"Generating optimizing model: {onnx_opt_path}\")\n            optimize_onnx(\n                onnx_path=onnx_path,\n                onnx_opt_path=onnx_opt_path,\n                model_data=self.model,\n            )\n        self.model.min_latent_shape = min_image_resolution // 8\n        self.model.max_latent_shape = max_image_resolution // 8\n        if not force_engine_build and os.path.exists(engine_path):\n            print(f\"Found cached engine: {engine_path}\")\n        else:\n            build_engine(\n                engine_path=engine_path,\n                onnx_opt_path=onnx_opt_path,\n                model_data=self.model,\n                opt_image_height=opt_image_height,\n                opt_image_width=opt_image_width,\n                opt_batch_size=opt_batch_size,\n                build_static_batch=build_static_batch,\n                build_dynamic_shape=build_dynamic_shape,\n                build_all_tactics=build_all_tactics,\n                build_enable_refit=build_enable_refit,\n            )\n\n        gc.collect()\n        torch.cuda.empty_cache()","file":"/src/streamdiffusion/acceleration/tensorrt/builder.py","fileHash":"aa86d5cd723aa7f220318f2e9933ff7dbdc8e34e7840d30c4846726ed044fdc6","hash":"5eda6d50a3362d93b17f8d69417901f985a0a6b9259c2175e368604a39e7a360","processedContent":"class EngineBuilder:\n    def __init__(\n        \"\"\"This code defines a class called \"SD Model\" that is used for image-text matching. It has various functions to retrieve information about the model and its inputs and outputs, as well as an `optimize` function that optimizes an ONNX graph and exports it as an ONNX model if requested.\"\"\"\n        pass\n\n    def build(\n        self,\n        onnx_path: str,\n        onnx_opt_path: str,\n        engine_path: str,\n        opt_image_height: int = 512,\n        opt_image_width: int = 512,\n        opt_batch_size: int = 1,\n        min_image_resolution: int = 256,\n        max_image_resolution: int = 1024,\n        build_enable_refit: bool = False,\n        build_static_batch: bool = False,\n        build_dynamic_shape: bool = False,\n        build_all_tactics: bool = False,\n        onnx_opset: int = 17,\n        force_engine_build: bool = False,\n        force_onnx_export: bool = False,\n        force_onnx_optimize: bool = False,\n    ):\n        if not force_onnx_export and os.path.exists(onnx_path):\n            print(f\"Found cached model: {onnx_path}\")\n        else:\n            print(f\"Exporting model: {onnx_path}\")\n            export_onnx(\n                self.network,\n                onnx_path=onnx_path,\n                model_data=self.model,\n                opt_image_height=opt_image_height,\n                opt_image_width=opt_image_width,\n                opt_batch_size=opt_batch_size,\n                onnx_opset=onnx_opset,\n            )\n            del self.network\n            gc.collect()\n            torch.cuda.empty_cache()\n        if not force_onnx_optimize and os.path.exists(onnx_opt_path):\n            print(f\"Found cached model: {onnx_opt_path}\")\n        else:\n            print(f\"Generating optimizing model: {onnx_opt_path}\")\n            optimize_onnx(\n                onnx_path=onnx_path,\n                onnx_opt_path=onnx_opt_path,\n                model_data=self.model,\n            )\n        self.model.min_latent_shape = min_image_resolution // 8\n        self.model.max_latent_shape = max_image_resolution // 8\n        if not force_engine_build and os.path.exists(engine_path):\n            print(f\"Found cached engine: {engine_path}\")\n        else:\n            build_engine(\n                engine_path=engine_path,\n                onnx_opt_path=onnx_opt_path,\n                model_data=self.model,\n                opt_image_height=opt_image_height,\n                opt_image_width=opt_image_width,\n                opt_batch_size=opt_batch_size,\n                build_static_batch=build_static_batch,\n                build_dynamic_shape=build_dynamic_shape,\n                build_all_tactics=build_all_tactics,\n                build_enable_refit=build_enable_refit,\n            )\n\n        gc.collect()\n        torch.cuda.empty_cache()","documentation":"This code defines a class called \"EngineBuilder\" that is used to build an image-text matching model. It has various functions to retrieve information about the model and its inputs and outputs, as well as an `optimize` function that optimizes an ONNX graph and exports it as an ONNX model if requested."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/create_onnx_path().","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/create_onnx_path().","language":"python","range":[14,0,15,75],"content":"def create_onnx_path(name, onnx_dir, opt=True):\n    return os.path.join(onnx_dir, name + (\".opt\" if opt else \"\") + \".onnx\")","file":"/src/streamdiffusion/acceleration/tensorrt/builder.py","fileHash":"aa86d5cd723aa7f220318f2e9933ff7dbdc8e34e7840d30c4846726ed044fdc6","hash":"3579133955fae7c653993ec064faae553f06b688e37208a28b1acfe70be2d0be","processedContent":"def create_onnx_path(name, onnx_dir, opt=True):\n    return os.path.join(onnx_dir, name + (\".opt\" if opt else \"\") + \".onnx\")","documentation":"This code defines a function that creates an ONNX file path based on the name of the model, the directory where the ONNX files are stored, and whether or not to include the \".opt\" extension."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/__init__:","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/__init__:","language":"python","range":[0,0,123,0],"content":"from typing import *\n\nimport torch\nfrom diffusers.models.autoencoder_tiny import AutoencoderTinyOutput\nfrom diffusers.models.unet_2d_condition import UNet2DConditionOutput\nfrom diffusers.models.vae import DecoderOutput\nfrom polygraphy import cuda\n\nfrom .utilities import Engine\n\n\nclass UNet2DConditionModelEngine:\n    def __init__(self, filepath: str, stream: cuda.Stream, use_cuda_graph: bool = False):\n        self.engine = Engine(filepath)\n        self.stream = stream\n        self.use_cuda_graph = use_cuda_graph\n\n        self.engine.load()\n        self.engine.activate()\n\n    def __call__(\n        self,\n        latent_model_input: torch.Tensor,\n        timestep: torch.Tensor,\n        encoder_hidden_states: torch.Tensor,\n        **kwargs,\n    ) -> Any:\n        if timestep.dtype != torch.float32:\n            timestep = timestep.float()\n\n        self.engine.allocate_buffers(\n            shape_dict={\n                \"sample\": latent_model_input.shape,\n                \"timestep\": timestep.shape,\n                \"encoder_hidden_states\": encoder_hidden_states.shape,\n                \"latent\": latent_model_input.shape,\n            },\n            device=latent_model_input.device,\n        )\n\n        noise_pred = self.engine.infer(\n            {\n                \"sample\": latent_model_input,\n                \"timestep\": timestep,\n                \"encoder_hidden_states\": encoder_hidden_states,\n            },\n            self.stream,\n            use_cuda_graph=self.use_cuda_graph,\n        )[\"latent\"]\n        return UNet2DConditionOutput(sample=noise_pred)\n\n    def to(self, *args, **kwargs):\n        pass\n\n    def forward(self, *args, **kwargs):\n        pass\n\n\nclass AutoencoderKLEngine:\n    def __init__(\n        self,\n        encoder_path: str,\n        decoder_path: str,\n        stream: cuda.Stream,\n        scaling_factor: int,\n        use_cuda_graph: bool = False,\n    ):\n        self.encoder = Engine(encoder_path)\n        self.decoder = Engine(decoder_path)\n        self.stream = stream\n        self.vae_scale_factor = scaling_factor\n        self.use_cuda_graph = use_cuda_graph\n\n        self.encoder.load()\n        self.decoder.load()\n        self.encoder.activate()\n        self.decoder.activate()\n\n    def encode(self, images: torch.Tensor, **kwargs):\n        self.encoder.allocate_buffers(\n            shape_dict={\n                \"images\": images.shape,\n                \"latent\": (\n                    images.shape[0],\n                    4,\n                    images.shape[2] // self.vae_scale_factor,\n                    images.shape[3] // self.vae_scale_factor,\n                ),\n            },\n            device=images.device,\n        )\n        latents = self.encoder.infer(\n            {\"images\": images},\n            self.stream,\n            use_cuda_graph=self.use_cuda_graph,\n        )[\"latent\"]\n        return AutoencoderTinyOutput(latents=latents)\n\n    def decode(self, latent: torch.Tensor, **kwargs):\n        self.decoder.allocate_buffers(\n            shape_dict={\n                \"latent\": latent.shape,\n                \"images\": (\n                    latent.shape[0],\n                    3,\n                    latent.shape[2] * self.vae_scale_factor,\n                    latent.shape[3] * self.vae_scale_factor,\n                ),\n            },\n            device=latent.device,\n        )\n        images = self.decoder.infer(\n            {\"latent\": latent},\n            self.stream,\n            use_cuda_graph=self.use_cuda_graph,\n        )[\"images\"]\n        return DecoderOutput(sample=images)\n\n    def to(self, *args, **kwargs):\n        pass\n\n    def forward(self, *args, **kwargs):\n        pass\n","file":"/src/streamdiffusion/acceleration/tensorrt/engine.py","fileHash":"dc4f85c80de38d40817cb2185c46f634aee5ae1c4e35ed91586498131c2d2cf7","hash":"dc4f85c80de38d40817cb2185c46f634aee5ae1c4e35ed91586498131c2d2cf7","processedContent":"from typing import * #undefined\n\nimport torch\nfrom diffusers.models.autoencoder_tiny import AutoencoderTinyOutput #undefined\nfrom diffusers.models.unet_2d_condition import UNet2DConditionOutput #undefined\nfrom diffusers.models.vae import DecoderOutput #undefined\nfrom polygraphy import cuda #undefined\n\nfrom .utilities import Engine\n\"\"\"\nscip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:: undefined\nscip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#: The code defines a class called Engine that initializes an object with various attributes and methods for building, loading, and executing a TensorRT engine.\n\"\"\"\n\n\nclass UNet2DConditionModelEngine:\n    \"\"\"This code defines a class called `UNet2DConditionModelEngine` that initializes an object of the `Engine` class, loads a TensorRT engine from a file path, and sets the tensor addresses for each tensor in the feed dictionary. It also defines a function called `infer` that takes in a feed dictionary, a stream, and an optional boolean flag indicating whether to use CUDA graph, and executes the inference using the TensorRT context.\"\"\"\n    pass\n\n\nclass AutoencoderKLEngine:\n    \"\"\"The code defines a class called AutoencoderKLEngine that initializes two objects of the Engine class, encoder and decoder, with various attributes and methods for building, loading, and executing TensorRT engines. It also creates execution contexts for the engines using the activate() method.\"\"\"\n    pass\n","documentation":"The code defines two classes, `UNet2DConditionModelEngine` and `AutoencoderKLEngine`, which are used to build and execute TensorRT engines for image processing tasks."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#","language":"python","range":[58,0,122,12],"content":"class AutoencoderKLEngine:\n    def __init__(\n        self,\n        encoder_path: str,\n        decoder_path: str,\n        stream: cuda.Stream,\n        scaling_factor: int,\n        use_cuda_graph: bool = False,\n    ):\n        self.encoder = Engine(encoder_path)\n        self.decoder = Engine(decoder_path)\n        self.stream = stream\n        self.vae_scale_factor = scaling_factor\n        self.use_cuda_graph = use_cuda_graph\n\n        self.encoder.load()\n        self.decoder.load()\n        self.encoder.activate()\n        self.decoder.activate()\n\n    def encode(self, images: torch.Tensor, **kwargs):\n        self.encoder.allocate_buffers(\n            shape_dict={\n                \"images\": images.shape,\n                \"latent\": (\n                    images.shape[0],\n                    4,\n                    images.shape[2] // self.vae_scale_factor,\n                    images.shape[3] // self.vae_scale_factor,\n                ),\n            },\n            device=images.device,\n        )\n        latents = self.encoder.infer(\n            {\"images\": images},\n            self.stream,\n            use_cuda_graph=self.use_cuda_graph,\n        )[\"latent\"]\n        return AutoencoderTinyOutput(latents=latents)\n\n    def decode(self, latent: torch.Tensor, **kwargs):\n        self.decoder.allocate_buffers(\n            shape_dict={\n                \"latent\": latent.shape,\n                \"images\": (\n                    latent.shape[0],\n                    3,\n                    latent.shape[2] * self.vae_scale_factor,\n                    latent.shape[3] * self.vae_scale_factor,\n                ),\n            },\n            device=latent.device,\n        )\n        images = self.decoder.infer(\n            {\"latent\": latent},\n            self.stream,\n            use_cuda_graph=self.use_cuda_graph,\n        )[\"images\"]\n        return DecoderOutput(sample=images)\n\n    def to(self, *args, **kwargs):\n        pass\n\n    def forward(self, *args, **kwargs):\n        pass","file":"/src/streamdiffusion/acceleration/tensorrt/engine.py","fileHash":"dc4f85c80de38d40817cb2185c46f634aee5ae1c4e35ed91586498131c2d2cf7","hash":"a9d6251b1b6e9f4acebea6e596ba40ab3e1ba988e4d67d7ba1835e869ec4a843","processedContent":"class AutoencoderKLEngine:\n    def __init__(\n        \"\"\"The code defines a class called AutoencoderKLEngine that initializes two objects of the Engine class, encoder and decoder, with various attributes and methods for building, loading, and executing TensorRT engines. It also creates execution contexts for the engines using the activate() method.\"\"\"\n        pass\n\n    def encode(self, images: torch.Tensor, **kwargs):\n        \"\"\"The code defines a function called `encode` that takes in a tensor of images and returns a tensor of latent representations. It uses a TensorRT engine to perform the encoding, allocating memory for the input and output tensors and executing the inference using the engine's `infer` method.\"\"\"\n        pass\n\n    def decode(self, latent: torch.Tensor, **kwargs):\n        \"\"\"The code defines a function called `decode` that takes in a latent tensor and allocates memory for input and output tensors in a TensorRT engine. It then executes the inference using the TensorRT context and returns the decoded images.\"\"\"\n        pass\n\n    def to(self, *args, **kwargs):\n        \"\"\"This code defines a function called `to` that takes any number of positional arguments and keyword arguments, and does nothing with them. It is a placeholder function that can be used as a stub or a starting point for more complex functionality.\"\"\"\n        pass\n\n    def forward(self, *args, **kwargs):\n        pass","documentation":"The code defines a class called AutoencoderKLEngine that initializes two objects of the Engine class, encoder and decoder, with various attributes and methods for building, loading, and executing TensorRT engines. It also creates execution contexts for the engines using the activate() method."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#","language":"python","range":[11,0,55,12],"content":"class UNet2DConditionModelEngine:\n    def __init__(self, filepath: str, stream: cuda.Stream, use_cuda_graph: bool = False):\n        self.engine = Engine(filepath)\n        self.stream = stream\n        self.use_cuda_graph = use_cuda_graph\n\n        self.engine.load()\n        self.engine.activate()\n\n    def __call__(\n        self,\n        latent_model_input: torch.Tensor,\n        timestep: torch.Tensor,\n        encoder_hidden_states: torch.Tensor,\n        **kwargs,\n    ) -> Any:\n        if timestep.dtype != torch.float32:\n            timestep = timestep.float()\n\n        self.engine.allocate_buffers(\n            shape_dict={\n                \"sample\": latent_model_input.shape,\n                \"timestep\": timestep.shape,\n                \"encoder_hidden_states\": encoder_hidden_states.shape,\n                \"latent\": latent_model_input.shape,\n            },\n            device=latent_model_input.device,\n        )\n\n        noise_pred = self.engine.infer(\n            {\n                \"sample\": latent_model_input,\n                \"timestep\": timestep,\n                \"encoder_hidden_states\": encoder_hidden_states,\n            },\n            self.stream,\n            use_cuda_graph=self.use_cuda_graph,\n        )[\"latent\"]\n        return UNet2DConditionOutput(sample=noise_pred)\n\n    def to(self, *args, **kwargs):\n        pass\n\n    def forward(self, *args, **kwargs):\n        pass","file":"/src/streamdiffusion/acceleration/tensorrt/engine.py","fileHash":"dc4f85c80de38d40817cb2185c46f634aee5ae1c4e35ed91586498131c2d2cf7","hash":"d258c7120278d9cbb5bfa09ba6218f34c1f5e329b6795c181e210b87adee1872","processedContent":"class UNet2DConditionModelEngine:\n    def __init__(self, filepath: str, stream: cuda.Stream, use_cuda_graph: bool = False):\n        \"\"\"This code initializes an object of the `Engine` class, which loads a TensorRT engine from a file path and stores it in the `engine` variable.\"\"\"\n        pass\n\n    def __call__(\n        \"\"\"This code defines a function called `infer` that takes in a feed dictionary, a stream, and an optional boolean flag indicating whether to use CUDA graph. It then sets the tensor addresses for each tensor in the feed dictionary and executes the inference using the TensorRT context. If the CUDA graph flag is set, it captures the CUDA graph and instantiates it.\"\"\"\n        pass\n\n    def to(self, *args, **kwargs):\n        \"\"\"This code defines a function called `to` that takes any number of positional arguments and keyword arguments, and does nothing with them. It is a placeholder function that can be used as a stub or a starting point for more complex functionality.\"\"\"\n        pass\n\n    def forward(self, *args, **kwargs):\n        pass","documentation":"This code defines a class called `UNet2DConditionModelEngine` that initializes an object of the `Engine` class, loads a TensorRT engine from a file path, and sets the tensor addresses for each tensor in the feed dictionary. It also defines a function called `infer` that takes in a feed dictionary, a stream, and an optional boolean flag indicating whether to use CUDA graph, and executes the inference using the TensorRT context."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/__init__:","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/__init__:","language":"python","range":[0,0,434,0],"content":"#! fork: https://github.com/NVIDIA/TensorRT/blob/main/demo/Diffusion/models.py\n\n#\n# SPDX-FileCopyrightText: Copyright (c) 1993-2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: Apache-2.0\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport onnx_graphsurgeon as gs\nimport torch\nfrom onnx import shape_inference\nfrom polygraphy.backend.onnx.loader import fold_constants\n\n\nclass Optimizer:\n    def __init__(self, onnx_graph, verbose=False):\n        self.graph = gs.import_onnx(onnx_graph)\n        self.verbose = verbose\n\n    def info(self, prefix):\n        if self.verbose:\n            print(\n                f\"{prefix} .. {len(self.graph.nodes)} nodes, {len(self.graph.tensors().keys())} tensors, {len(self.graph.inputs)} inputs, {len(self.graph.outputs)} outputs\"\n            )\n\n    def cleanup(self, return_onnx=False):\n        self.graph.cleanup().toposort()\n        if return_onnx:\n            return gs.export_onnx(self.graph)\n\n    def select_outputs(self, keep, names=None):\n        self.graph.outputs = [self.graph.outputs[o] for o in keep]\n        if names:\n            for i, name in enumerate(names):\n                self.graph.outputs[i].name = name\n\n    def fold_constants(self, return_onnx=False):\n        onnx_graph = fold_constants(gs.export_onnx(self.graph), allow_onnxruntime_shape_inference=True)\n        self.graph = gs.import_onnx(onnx_graph)\n        if return_onnx:\n            return onnx_graph\n\n    def infer_shapes(self, return_onnx=False):\n        onnx_graph = gs.export_onnx(self.graph)\n        if onnx_graph.ByteSize() > 2147483648:\n            raise TypeError(\"ERROR: model size exceeds supported 2GB limit\")\n        else:\n            onnx_graph = shape_inference.infer_shapes(onnx_graph)\n\n        self.graph = gs.import_onnx(onnx_graph)\n        if return_onnx:\n            return onnx_graph\n\n\nclass BaseModel:\n    def __init__(\n        self,\n        fp16=False,\n        device=\"cuda\",\n        verbose=True,\n        max_batch_size=16,\n        min_batch_size=1,\n        embedding_dim=768,\n        text_maxlen=77,\n    ):\n        self.name = \"SD Model\"\n        self.fp16 = fp16\n        self.device = device\n        self.verbose = verbose\n\n        self.min_batch = min_batch_size\n        self.max_batch = max_batch_size\n        self.min_image_shape = 256  # min image resolution: 256x256\n        self.max_image_shape = 1024  # max image resolution: 1024x1024\n        self.min_latent_shape = self.min_image_shape // 8\n        self.max_latent_shape = self.max_image_shape // 8\n\n        self.embedding_dim = embedding_dim\n        self.text_maxlen = text_maxlen\n\n    def get_model(self):\n        pass\n\n    def get_input_names(self):\n        pass\n\n    def get_output_names(self):\n        pass\n\n    def get_dynamic_axes(self):\n        return None\n\n    def get_sample_input(self, batch_size, image_height, image_width):\n        pass\n\n    def get_input_profile(self, batch_size, image_height, image_width, static_batch, static_shape):\n        return None\n\n    def get_shape_dict(self, batch_size, image_height, image_width):\n        return None\n\n    def optimize(self, onnx_graph):\n        opt = Optimizer(onnx_graph, verbose=self.verbose)\n        opt.info(self.name + \": original\")\n        opt.cleanup()\n        opt.info(self.name + \": cleanup\")\n        opt.fold_constants()\n        opt.info(self.name + \": fold constants\")\n        opt.infer_shapes()\n        opt.info(self.name + \": shape inference\")\n        onnx_opt_graph = opt.cleanup(return_onnx=True)\n        opt.info(self.name + \": finished\")\n        return onnx_opt_graph\n\n    def check_dims(self, batch_size, image_height, image_width):\n        assert batch_size >= self.min_batch and batch_size <= self.max_batch\n        assert image_height % 8 == 0 or image_width % 8 == 0\n        latent_height = image_height // 8\n        latent_width = image_width // 8\n        assert latent_height >= self.min_latent_shape and latent_height <= self.max_latent_shape\n        assert latent_width >= self.min_latent_shape and latent_width <= self.max_latent_shape\n        return (latent_height, latent_width)\n\n    def get_minmax_dims(self, batch_size, image_height, image_width, static_batch, static_shape):\n        min_batch = batch_size if static_batch else self.min_batch\n        max_batch = batch_size if static_batch else self.max_batch\n        latent_height = image_height // 8\n        latent_width = image_width // 8\n        min_image_height = image_height if static_shape else self.min_image_shape\n        max_image_height = image_height if static_shape else self.max_image_shape\n        min_image_width = image_width if static_shape else self.min_image_shape\n        max_image_width = image_width if static_shape else self.max_image_shape\n        min_latent_height = latent_height if static_shape else self.min_latent_shape\n        max_latent_height = latent_height if static_shape else self.max_latent_shape\n        min_latent_width = latent_width if static_shape else self.min_latent_shape\n        max_latent_width = latent_width if static_shape else self.max_latent_shape\n        return (\n            min_batch,\n            max_batch,\n            min_image_height,\n            max_image_height,\n            min_image_width,\n            max_image_width,\n            min_latent_height,\n            max_latent_height,\n            min_latent_width,\n            max_latent_width,\n        )\n\n\nclass CLIP(BaseModel):\n    def __init__(self, device, max_batch_size, embedding_dim, min_batch_size=1):\n        super(CLIP, self).__init__(\n            device=device,\n            max_batch_size=max_batch_size,\n            min_batch_size=min_batch_size,\n            embedding_dim=embedding_dim,\n        )\n        self.name = \"CLIP\"\n\n    def get_input_names(self):\n        return [\"input_ids\"]\n\n    def get_output_names(self):\n        return [\"text_embeddings\", \"pooler_output\"]\n\n    def get_dynamic_axes(self):\n        return {\"input_ids\": {0: \"B\"}, \"text_embeddings\": {0: \"B\"}}\n\n    def get_input_profile(self, batch_size, image_height, image_width, static_batch, static_shape):\n        self.check_dims(batch_size, image_height, image_width)\n        min_batch, max_batch, _, _, _, _, _, _, _, _ = self.get_minmax_dims(\n            batch_size, image_height, image_width, static_batch, static_shape\n        )\n        return {\n            \"input_ids\": [\n                (min_batch, self.text_maxlen),\n                (batch_size, self.text_maxlen),\n                (max_batch, self.text_maxlen),\n            ]\n        }\n\n    def get_shape_dict(self, batch_size, image_height, image_width):\n        self.check_dims(batch_size, image_height, image_width)\n        return {\n            \"input_ids\": (batch_size, self.text_maxlen),\n            \"text_embeddings\": (batch_size, self.text_maxlen, self.embedding_dim),\n        }\n\n    def get_sample_input(self, batch_size, image_height, image_width):\n        self.check_dims(batch_size, image_height, image_width)\n        return torch.zeros(batch_size, self.text_maxlen, dtype=torch.int32, device=self.device)\n\n    def optimize(self, onnx_graph):\n        opt = Optimizer(onnx_graph)\n        opt.info(self.name + \": original\")\n        opt.select_outputs([0])  # delete graph output#1\n        opt.cleanup()\n        opt.info(self.name + \": remove output[1]\")\n        opt.fold_constants()\n        opt.info(self.name + \": fold constants\")\n        opt.infer_shapes()\n        opt.info(self.name + \": shape inference\")\n        opt.select_outputs([0], names=[\"text_embeddings\"])  # rename network output\n        opt.info(self.name + \": remove output[0]\")\n        opt_onnx_graph = opt.cleanup(return_onnx=True)\n        opt.info(self.name + \": finished\")\n        return opt_onnx_graph\n\n\nclass UNet(BaseModel):\n    def __init__(\n        self,\n        fp16=False,\n        device=\"cuda\",\n        max_batch_size=16,\n        min_batch_size=1,\n        embedding_dim=768,\n        text_maxlen=77,\n        unet_dim=4,\n    ):\n        super(UNet, self).__init__(\n            fp16=fp16,\n            device=device,\n            max_batch_size=max_batch_size,\n            min_batch_size=min_batch_size,\n            embedding_dim=embedding_dim,\n            text_maxlen=text_maxlen,\n        )\n        self.unet_dim = unet_dim\n        self.name = \"UNet\"\n\n    def get_input_names(self):\n        return [\"sample\", \"timestep\", \"encoder_hidden_states\"]\n\n    def get_output_names(self):\n        return [\"latent\"]\n\n    def get_dynamic_axes(self):\n        return {\n            \"sample\": {0: \"2B\", 2: \"H\", 3: \"W\"},\n            \"timestep\": {0: \"2B\"},\n            \"encoder_hidden_states\": {0: \"2B\"},\n            \"latent\": {0: \"2B\", 2: \"H\", 3: \"W\"},\n        }\n\n    def get_input_profile(self, batch_size, image_height, image_width, static_batch, static_shape):\n        latent_height, latent_width = self.check_dims(batch_size, image_height, image_width)\n        (\n            min_batch,\n            max_batch,\n            _,\n            _,\n            _,\n            _,\n            min_latent_height,\n            max_latent_height,\n            min_latent_width,\n            max_latent_width,\n        ) = self.get_minmax_dims(batch_size, image_height, image_width, static_batch, static_shape)\n        return {\n            \"sample\": [\n                (min_batch, self.unet_dim, min_latent_height, min_latent_width),\n                (batch_size, self.unet_dim, latent_height, latent_width),\n                (max_batch, self.unet_dim, max_latent_height, max_latent_width),\n            ],\n            \"timestep\": [(min_batch,), (batch_size,), (max_batch,)],\n            \"encoder_hidden_states\": [\n                (min_batch, self.text_maxlen, self.embedding_dim),\n                (batch_size, self.text_maxlen, self.embedding_dim),\n                (max_batch, self.text_maxlen, self.embedding_dim),\n            ],\n        }\n\n    def get_shape_dict(self, batch_size, image_height, image_width):\n        latent_height, latent_width = self.check_dims(batch_size, image_height, image_width)\n        return {\n            \"sample\": (2 * batch_size, self.unet_dim, latent_height, latent_width),\n            \"timestep\": (2 * batch_size,),\n            \"encoder_hidden_states\": (2 * batch_size, self.text_maxlen, self.embedding_dim),\n            \"latent\": (2 * batch_size, 4, latent_height, latent_width),\n        }\n\n    def get_sample_input(self, batch_size, image_height, image_width):\n        latent_height, latent_width = self.check_dims(batch_size, image_height, image_width)\n        dtype = torch.float16 if self.fp16 else torch.float32\n        return (\n            torch.randn(\n                2 * batch_size, self.unet_dim, latent_height, latent_width, dtype=torch.float32, device=self.device\n            ),\n            torch.ones((2 * batch_size,), dtype=torch.float32, device=self.device),\n            torch.randn(2 * batch_size, self.text_maxlen, self.embedding_dim, dtype=dtype, device=self.device),\n        )\n\n\nclass VAE(BaseModel):\n    def __init__(self, device, max_batch_size, min_batch_size=1):\n        super(VAE, self).__init__(\n            device=device,\n            max_batch_size=max_batch_size,\n            min_batch_size=min_batch_size,\n            embedding_dim=None,\n        )\n        self.name = \"VAE decoder\"\n\n    def get_input_names(self):\n        return [\"latent\"]\n\n    def get_output_names(self):\n        return [\"images\"]\n\n    def get_dynamic_axes(self):\n        return {\n            \"latent\": {0: \"B\", 2: \"H\", 3: \"W\"},\n            \"images\": {0: \"B\", 2: \"8H\", 3: \"8W\"},\n        }\n\n    def get_input_profile(self, batch_size, image_height, image_width, static_batch, static_shape):\n        latent_height, latent_width = self.check_dims(batch_size, image_height, image_width)\n        (\n            min_batch,\n            max_batch,\n            _,\n            _,\n            _,\n            _,\n            min_latent_height,\n            max_latent_height,\n            min_latent_width,\n            max_latent_width,\n        ) = self.get_minmax_dims(batch_size, image_height, image_width, static_batch, static_shape)\n        return {\n            \"latent\": [\n                (min_batch, 4, min_latent_height, min_latent_width),\n                (batch_size, 4, latent_height, latent_width),\n                (max_batch, 4, max_latent_height, max_latent_width),\n            ]\n        }\n\n    def get_shape_dict(self, batch_size, image_height, image_width):\n        latent_height, latent_width = self.check_dims(batch_size, image_height, image_width)\n        return {\n            \"latent\": (batch_size, 4, latent_height, latent_width),\n            \"images\": (batch_size, 3, image_height, image_width),\n        }\n\n    def get_sample_input(self, batch_size, image_height, image_width):\n        latent_height, latent_width = self.check_dims(batch_size, image_height, image_width)\n        return torch.randn(\n            batch_size,\n            4,\n            latent_height,\n            latent_width,\n            dtype=torch.float32,\n            device=self.device,\n        )\n\n\nclass VAEEncoder(BaseModel):\n    def __init__(self, device, max_batch_size, min_batch_size=1):\n        super(VAEEncoder, self).__init__(\n            device=device,\n            max_batch_size=max_batch_size,\n            min_batch_size=min_batch_size,\n            embedding_dim=None,\n        )\n        self.name = \"VAE encoder\"\n\n    def get_input_names(self):\n        return [\"images\"]\n\n    def get_output_names(self):\n        return [\"latent\"]\n\n    def get_dynamic_axes(self):\n        return {\n            \"images\": {0: \"B\", 2: \"8H\", 3: \"8W\"},\n            \"latent\": {0: \"B\", 2: \"H\", 3: \"W\"},\n        }\n\n    def get_input_profile(self, batch_size, image_height, image_width, static_batch, static_shape):\n        assert batch_size >= self.min_batch and batch_size <= self.max_batch\n        min_batch = batch_size if static_batch else self.min_batch\n        max_batch = batch_size if static_batch else self.max_batch\n        self.check_dims(batch_size, image_height, image_width)\n        (\n            min_batch,\n            max_batch,\n            min_image_height,\n            max_image_height,\n            min_image_width,\n            max_image_width,\n            _,\n            _,\n            _,\n            _,\n        ) = self.get_minmax_dims(batch_size, image_height, image_width, static_batch, static_shape)\n\n        return {\n            \"images\": [\n                (min_batch, 3, min_image_height, min_image_width),\n                (batch_size, 3, image_height, image_width),\n                (max_batch, 3, max_image_height, max_image_width),\n            ],\n        }\n\n    def get_shape_dict(self, batch_size, image_height, image_width):\n        latent_height, latent_width = self.check_dims(batch_size, image_height, image_width)\n        return {\n            \"images\": (batch_size, 3, image_height, image_width),\n            \"latent\": (batch_size, 4, latent_height, latent_width),\n        }\n\n    def get_sample_input(self, batch_size, image_height, image_width):\n        self.check_dims(batch_size, image_height, image_width)\n        return torch.randn(\n            batch_size,\n            3,\n            image_height,\n            image_width,\n            dtype=torch.float32,\n            device=self.device,\n        )\n","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","processedContent":"#! fork: https://github.com/NVIDIA/TensorRT/blob/main/demo/Diffusion/models.py\n\n#\n# SPDX-FileCopyrightText: Copyright (c) 1993-2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: Apache-2.0\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport onnx_graphsurgeon as gs\nimport torch\nfrom onnx import shape_inference #undefined\nfrom polygraphy.backend.onnx.loader import fold_constants #undefined\n\n\nclass Optimizer:\n    \"\"\"This code defines a class called `Optimizer` that optimizes an ONNX graph by cleaning it up, selecting outputs, folding constants, and inferring shapes. It also exports the optimized graph as an ONNX model if requested.\"\"\"\n    pass\n\n\nclass BaseModel:\n    \"\"\"This code defines a class called \"SD Model\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested.\"\"\"\n    pass\n\n\nclass CLIP(BaseModel):\n    \"\"\"This code defines a class called \"SD Model\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested.\"\"\"\n    pass\n\n\nclass UNet(BaseModel):\n    \"\"\"This code defines a class called \"SD Model\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested.\"\"\"\n    pass\n\n\nclass VAE(BaseModel):\n    \"\"\"This code defines a class called \"VAE decoder\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested.\"\"\"\n    pass\n\n\nclass VAEEncoder(BaseModel):\n    \"\"\"This code defines a class called \"VAEEncoder\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested.\"\"\"\n    pass\n","documentation":"The code defines a series of classes for image-text matching, including Optimizer, BaseModel, CLIP, UNet, VAE, and VAEEncoder. These classes are used to optimize ONNX graphs, retrieve information about the models and their inputs and outputs, and export optimized graphs as ONNX models if requested."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#","language":"python","range":[306,0,366,9],"content":"class VAE(BaseModel):\n    def __init__(self, device, max_batch_size, min_batch_size=1):\n        super(VAE, self).__init__(\n            device=device,\n            max_batch_size=max_batch_size,\n            min_batch_size=min_batch_size,\n            embedding_dim=None,\n        )\n        self.name = \"VAE decoder\"\n\n    def get_input_names(self):\n        return [\"latent\"]\n\n    def get_output_names(self):\n        return [\"images\"]\n\n    def get_dynamic_axes(self):\n        return {\n            \"latent\": {0: \"B\", 2: \"H\", 3: \"W\"},\n            \"images\": {0: \"B\", 2: \"8H\", 3: \"8W\"},\n        }\n\n    def get_input_profile(self, batch_size, image_height, image_width, static_batch, static_shape):\n        latent_height, latent_width = self.check_dims(batch_size, image_height, image_width)\n        (\n            min_batch,\n            max_batch,\n            _,\n            _,\n            _,\n            _,\n            min_latent_height,\n            max_latent_height,\n            min_latent_width,\n            max_latent_width,\n        ) = self.get_minmax_dims(batch_size, image_height, image_width, static_batch, static_shape)\n        return {\n            \"latent\": [\n                (min_batch, 4, min_latent_height, min_latent_width),\n                (batch_size, 4, latent_height, latent_width),\n                (max_batch, 4, max_latent_height, max_latent_width),\n            ]\n        }\n\n    def get_shape_dict(self, batch_size, image_height, image_width):\n        latent_height, latent_width = self.check_dims(batch_size, image_height, image_width)\n        return {\n            \"latent\": (batch_size, 4, latent_height, latent_width),\n            \"images\": (batch_size, 3, image_height, image_width),\n        }\n\n    def get_sample_input(self, batch_size, image_height, image_width):\n        latent_height, latent_width = self.check_dims(batch_size, image_height, image_width)\n        return torch.randn(\n            batch_size,\n            4,\n            latent_height,\n            latent_width,\n            dtype=torch.float32,\n            device=self.device,\n        )","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"5fb18ba242d994558bdf2fdfc79d4e167e43805123693a70436a4ea228b69c5a","processedContent":"class VAE(BaseModel): #This code defines a class called \"SD Model\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested.\n    def __init__(self, device, max_batch_size, min_batch_size=1):\n        \"\"\"This code defines a class called \"VAE decoder\" that is used for image-text matching. It takes in various parameters, such as the device to use and the maximum and minimum batch sizes, and has attributes for the embedding dimension, text max length, and the minimum and maximum latent shapes.\"\"\"\n        pass\n\n    def get_input_names(self):\n        \"\"\"This code defines a function called `get_input_names` that returns a list of strings representing the names of the input variables for a machine learning model.\"\"\"\n        pass\n\n    def get_output_names(self):\n        \"\"\"This code defines a function called `get_output_names` that returns a list of strings, where each string represents the name of an output. In this case, the only output is \"images\".\"\"\"\n        pass\n\n    def get_dynamic_axes(self):\n        \"\"\"This code defines a function called `get_dynamic_axes` that returns a dictionary with two keys: \"latent\" and \"images\". The values of these keys are dictionaries that map the indices of the input tensors to their corresponding batch, height, and width dimensions.\"\"\"\n        pass\n\n    def get_input_profile(self, batch_size, image_height, image_width, static_batch, static_shape):\n        \"\"\"This code defines a function that returns the minimum and maximum values for various dimensions in an image processing pipeline. It takes into account the batch size, image height and width, and whether the shapes are static or dynamic.\"\"\"\n        pass\n\n    def get_shape_dict(self, batch_size, image_height, image_width):\n        \"\"\"This code defines a function that returns a dictionary with two keys: \"latent\" and \"images\". The \"latent\" key has a shape of (batch_size, 4, latent_height, latent_width), while the \"images\" key has a shape of (batch_size, 3, image_height, image_width). The function checks the dimensions of an image and returns the latent height and width based on certain business rules.\"\"\"\n        pass\n\n    def get_sample_input(self, batch_size, image_height, image_width):\n        latent_height, latent_width = self.check_dims(batch_size, image_height, image_width)\n        return torch.randn(\n            batch_size,\n            4,\n            latent_height,\n            latent_width,\n            dtype=torch.float32,\n            device=self.device,\n        )","documentation":"This code defines a class called \"VAE decoder\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#","language":"python","range":[65,0,158,9],"content":"class BaseModel:\n    def __init__(\n        self,\n        fp16=False,\n        device=\"cuda\",\n        verbose=True,\n        max_batch_size=16,\n        min_batch_size=1,\n        embedding_dim=768,\n        text_maxlen=77,\n    ):\n        self.name = \"SD Model\"\n        self.fp16 = fp16\n        self.device = device\n        self.verbose = verbose\n\n        self.min_batch = min_batch_size\n        self.max_batch = max_batch_size\n        self.min_image_shape = 256  # min image resolution: 256x256\n        self.max_image_shape = 1024  # max image resolution: 1024x1024\n        self.min_latent_shape = self.min_image_shape // 8\n        self.max_latent_shape = self.max_image_shape // 8\n\n        self.embedding_dim = embedding_dim\n        self.text_maxlen = text_maxlen\n\n    def get_model(self):\n        pass\n\n    def get_input_names(self):\n        pass\n\n    def get_output_names(self):\n        pass\n\n    def get_dynamic_axes(self):\n        return None\n\n    def get_sample_input(self, batch_size, image_height, image_width):\n        pass\n\n    def get_input_profile(self, batch_size, image_height, image_width, static_batch, static_shape):\n        return None\n\n    def get_shape_dict(self, batch_size, image_height, image_width):\n        return None\n\n    def optimize(self, onnx_graph):\n        opt = Optimizer(onnx_graph, verbose=self.verbose)\n        opt.info(self.name + \": original\")\n        opt.cleanup()\n        opt.info(self.name + \": cleanup\")\n        opt.fold_constants()\n        opt.info(self.name + \": fold constants\")\n        opt.infer_shapes()\n        opt.info(self.name + \": shape inference\")\n        onnx_opt_graph = opt.cleanup(return_onnx=True)\n        opt.info(self.name + \": finished\")\n        return onnx_opt_graph\n\n    def check_dims(self, batch_size, image_height, image_width):\n        assert batch_size >= self.min_batch and batch_size <= self.max_batch\n        assert image_height % 8 == 0 or image_width % 8 == 0\n        latent_height = image_height // 8\n        latent_width = image_width // 8\n        assert latent_height >= self.min_latent_shape and latent_height <= self.max_latent_shape\n        assert latent_width >= self.min_latent_shape and latent_width <= self.max_latent_shape\n        return (latent_height, latent_width)\n\n    def get_minmax_dims(self, batch_size, image_height, image_width, static_batch, static_shape):\n        min_batch = batch_size if static_batch else self.min_batch\n        max_batch = batch_size if static_batch else self.max_batch\n        latent_height = image_height // 8\n        latent_width = image_width // 8\n        min_image_height = image_height if static_shape else self.min_image_shape\n        max_image_height = image_height if static_shape else self.max_image_shape\n        min_image_width = image_width if static_shape else self.min_image_shape\n        max_image_width = image_width if static_shape else self.max_image_shape\n        min_latent_height = latent_height if static_shape else self.min_latent_shape\n        max_latent_height = latent_height if static_shape else self.max_latent_shape\n        min_latent_width = latent_width if static_shape else self.min_latent_shape\n        max_latent_width = latent_width if static_shape else self.max_latent_shape\n        return (\n            min_batch,\n            max_batch,\n            min_image_height,\n            max_image_height,\n            min_image_width,\n            max_image_width,\n            min_latent_height,\n            max_latent_height,\n            min_latent_width,\n            max_latent_width,\n        )","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"67bc388175682367fd2057e2c584acf8e24ed9ca1a221740cdcd6500ba53a286","processedContent":"class BaseModel:\n    def __init__(\n        \"\"\"This code defines a class called \"SD Model\" that is used for image-text matching. It takes in various parameters, such as the device to use, whether to use fp16 precision, and the maximum and minimum batch sizes. The model also has attributes for the embedding dimension, text max length, and the minimum and maximum latent shapes.\"\"\"\n        pass\n\n    def get_model(self):\n        \"\"\"This code defines a function called `get_model` that does nothing. It is an empty function, meaning it does not perform any actions or return any values.\"\"\"\n        pass\n\n    def get_input_names(self):\n        \"\"\"This code defines a function called `get_input_names` that retrieves the names of the input fields in a form.\"\"\"\n        pass\n\n    def get_output_names(self):\n        \"\"\"This code defines a function called `get_output_names` that returns a list of output names for the current model.\"\"\"\n        pass\n\n    def get_dynamic_axes(self):\n        \"\"\"This code defines a function called `get_dynamic_axes` that returns `None`. It is used to dynamically determine the axes of a graph or chart based on the data being plotted.\"\"\"\n        pass\n\n    def get_sample_input(self, batch_size, image_height, image_width):\n        \"\"\"This code defines a function called `get_sample_input` that generates a batch of input data for training a machine learning model. The function takes three arguments: `batch_size`, `image_height`, and `image_width`. It returns a tensor with the specified dimensions.\"\"\"\n        pass\n\n    def get_input_profile(self, batch_size, image_height, image_width, static_batch, static_shape):\n        \"\"\"This code defines a function called `get_input_profile` that returns a set of input parameters for a machine learning model. The function takes in five arguments: `batch_size`, `image_height`, `image_width`, `static_batch`, and `static_shape`. These parameters are used to determine the shape of the input data and the batch size of the model.\"\"\"\n        pass\n\n    def get_shape_dict(self, batch_size, image_height, image_width):\n        \"\"\"This code defines a function called `get_shape_dict` that takes three arguments: `batch_size`, `image_height`, and `image_width`. It returns a dictionary with the shape of the input data.\"\"\"\n        pass\n\n    def optimize(self, onnx_graph):\n        \"\"\"The code defines a class called `Optimizer` that optimizes an ONNX graph by cleaning it up, selecting outputs, folding constants, and inferring shapes. It also exports the optimized graph as an ONNX model if requested.\"\"\"\n        pass\n\n    def check_dims(self, batch_size, image_height, image_width):\n        \"\"\"This code checks the dimensions of an image and returns the latent height and width based on certain business rules.\"\"\"\n        pass\n\n    def get_minmax_dims(self, batch_size, image_height, image_width, static_batch, static_shape):\n        min_batch = batch_size if static_batch else self.min_batch\n        max_batch = batch_size if static_batch else self.max_batch\n        latent_height = image_height // 8\n        latent_width = image_width // 8\n        min_image_height = image_height if static_shape else self.min_image_shape\n        max_image_height = image_height if static_shape else self.max_image_shape\n        min_image_width = image_width if static_shape else self.min_image_shape\n        max_image_width = image_width if static_shape else self.max_image_shape\n        min_latent_height = latent_height if static_shape else self.min_latent_shape\n        max_latent_height = latent_height if static_shape else self.max_latent_shape\n        min_latent_width = latent_width if static_shape else self.min_latent_shape\n        max_latent_width = latent_width if static_shape else self.max_latent_shape\n        return (\n            min_batch,\n            max_batch,\n            min_image_height,\n            max_image_height,\n            min_image_width,\n            max_image_width,\n            min_latent_height,\n            max_latent_height,\n            min_latent_width,\n            max_latent_width,\n        )","documentation":"This code defines a class called \"SD Model\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#","language":"python","range":[221,0,303,9],"content":"class UNet(BaseModel):\n    def __init__(\n        self,\n        fp16=False,\n        device=\"cuda\",\n        max_batch_size=16,\n        min_batch_size=1,\n        embedding_dim=768,\n        text_maxlen=77,\n        unet_dim=4,\n    ):\n        super(UNet, self).__init__(\n            fp16=fp16,\n            device=device,\n            max_batch_size=max_batch_size,\n            min_batch_size=min_batch_size,\n            embedding_dim=embedding_dim,\n            text_maxlen=text_maxlen,\n        )\n        self.unet_dim = unet_dim\n        self.name = \"UNet\"\n\n    def get_input_names(self):\n        return [\"sample\", \"timestep\", \"encoder_hidden_states\"]\n\n    def get_output_names(self):\n        return [\"latent\"]\n\n    def get_dynamic_axes(self):\n        return {\n            \"sample\": {0: \"2B\", 2: \"H\", 3: \"W\"},\n            \"timestep\": {0: \"2B\"},\n            \"encoder_hidden_states\": {0: \"2B\"},\n            \"latent\": {0: \"2B\", 2: \"H\", 3: \"W\"},\n        }\n\n    def get_input_profile(self, batch_size, image_height, image_width, static_batch, static_shape):\n        latent_height, latent_width = self.check_dims(batch_size, image_height, image_width)\n        (\n            min_batch,\n            max_batch,\n            _,\n            _,\n            _,\n            _,\n            min_latent_height,\n            max_latent_height,\n            min_latent_width,\n            max_latent_width,\n        ) = self.get_minmax_dims(batch_size, image_height, image_width, static_batch, static_shape)\n        return {\n            \"sample\": [\n                (min_batch, self.unet_dim, min_latent_height, min_latent_width),\n                (batch_size, self.unet_dim, latent_height, latent_width),\n                (max_batch, self.unet_dim, max_latent_height, max_latent_width),\n            ],\n            \"timestep\": [(min_batch,), (batch_size,), (max_batch,)],\n            \"encoder_hidden_states\": [\n                (min_batch, self.text_maxlen, self.embedding_dim),\n                (batch_size, self.text_maxlen, self.embedding_dim),\n                (max_batch, self.text_maxlen, self.embedding_dim),\n            ],\n        }\n\n    def get_shape_dict(self, batch_size, image_height, image_width):\n        latent_height, latent_width = self.check_dims(batch_size, image_height, image_width)\n        return {\n            \"sample\": (2 * batch_size, self.unet_dim, latent_height, latent_width),\n            \"timestep\": (2 * batch_size,),\n            \"encoder_hidden_states\": (2 * batch_size, self.text_maxlen, self.embedding_dim),\n            \"latent\": (2 * batch_size, 4, latent_height, latent_width),\n        }\n\n    def get_sample_input(self, batch_size, image_height, image_width):\n        latent_height, latent_width = self.check_dims(batch_size, image_height, image_width)\n        dtype = torch.float16 if self.fp16 else torch.float32\n        return (\n            torch.randn(\n                2 * batch_size, self.unet_dim, latent_height, latent_width, dtype=torch.float32, device=self.device\n            ),\n            torch.ones((2 * batch_size,), dtype=torch.float32, device=self.device),\n            torch.randn(2 * batch_size, self.text_maxlen, self.embedding_dim, dtype=dtype, device=self.device),\n        )","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"3174446249c71e5a6d5dd43a00f2bd1eb631d017a4f3ea1217f0167038f3973b","processedContent":"class UNet(BaseModel): #This code defines a class called \"SD Model\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested.\n    def __init__(\n        \"\"\"This code defines a class called \"SD Model\" that is used for image-text matching. It takes in various parameters and has attributes for the embedding dimension, text max length, and the minimum and maximum latent shapes.\"\"\"\n        pass\n\n    def get_input_names(self):\n        \"\"\"This code defines a function called `get_input_names` that returns a list of strings representing the names of the input variables required by the model.\"\"\"\n        pass\n\n    def get_output_names(self):\n        \"\"\"This code defines a function called `get_output_names` that returns a list of strings representing the names of the output variables for a machine learning model.\"\"\"\n        pass\n\n    def get_dynamic_axes(self):\n        \"\"\"This code defines a function called `get_dynamic_axes` that returns a dictionary with dynamic axes for a neural network. The dictionary maps the axis names to their corresponding sizes, which can vary depending on the input data.\"\"\"\n        pass\n\n    def get_input_profile(self, batch_size, image_height, image_width, static_batch, static_shape):\n        \"\"\"This code defines a function that returns a dictionary with various dimensions for an image processing pipeline. It takes into account the batch size, image height and width, and whether the shapes are static or dynamic. The function also defines a function that returns the minimum and maximum values for various dimensions in the pipeline.\"\"\"\n        pass\n\n    def get_shape_dict(self, batch_size, image_height, image_width):\n        \"\"\"This code defines a function that returns a dictionary with various shapes for different tensors used in a deep learning model. The function takes in three parameters: batch size, image height, and image width, and checks the dimensions of an image based on certain business rules to determine the latent height and width. The dictionary returned by the function contains shapes for the \"sample\", \"timestep\", \"encoder_hidden_states\", and \"latent\" tensors.\"\"\"\n        pass\n\n    def get_sample_input(self, batch_size, image_height, image_width):\n        latent_height, latent_width = self.check_dims(batch_size, image_height, image_width)\n        dtype = torch.float16 if self.fp16 else torch.float32\n        return (\n            torch.randn(\n                2 * batch_size, self.unet_dim, latent_height, latent_width, dtype=torch.float32, device=self.device\n            ),\n            torch.ones((2 * batch_size,), dtype=torch.float32, device=self.device),\n            torch.randn(2 * batch_size, self.text_maxlen, self.embedding_dim, dtype=dtype, device=self.device),\n        )","documentation":"This code defines a class called \"SD Model\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#","language":"python","range":[369,0,433,9],"content":"class VAEEncoder(BaseModel):\n    def __init__(self, device, max_batch_size, min_batch_size=1):\n        super(VAEEncoder, self).__init__(\n            device=device,\n            max_batch_size=max_batch_size,\n            min_batch_size=min_batch_size,\n            embedding_dim=None,\n        )\n        self.name = \"VAE encoder\"\n\n    def get_input_names(self):\n        return [\"images\"]\n\n    def get_output_names(self):\n        return [\"latent\"]\n\n    def get_dynamic_axes(self):\n        return {\n            \"images\": {0: \"B\", 2: \"8H\", 3: \"8W\"},\n            \"latent\": {0: \"B\", 2: \"H\", 3: \"W\"},\n        }\n\n    def get_input_profile(self, batch_size, image_height, image_width, static_batch, static_shape):\n        assert batch_size >= self.min_batch and batch_size <= self.max_batch\n        min_batch = batch_size if static_batch else self.min_batch\n        max_batch = batch_size if static_batch else self.max_batch\n        self.check_dims(batch_size, image_height, image_width)\n        (\n            min_batch,\n            max_batch,\n            min_image_height,\n            max_image_height,\n            min_image_width,\n            max_image_width,\n            _,\n            _,\n            _,\n            _,\n        ) = self.get_minmax_dims(batch_size, image_height, image_width, static_batch, static_shape)\n\n        return {\n            \"images\": [\n                (min_batch, 3, min_image_height, min_image_width),\n                (batch_size, 3, image_height, image_width),\n                (max_batch, 3, max_image_height, max_image_width),\n            ],\n        }\n\n    def get_shape_dict(self, batch_size, image_height, image_width):\n        latent_height, latent_width = self.check_dims(batch_size, image_height, image_width)\n        return {\n            \"images\": (batch_size, 3, image_height, image_width),\n            \"latent\": (batch_size, 4, latent_height, latent_width),\n        }\n\n    def get_sample_input(self, batch_size, image_height, image_width):\n        self.check_dims(batch_size, image_height, image_width)\n        return torch.randn(\n            batch_size,\n            3,\n            image_height,\n            image_width,\n            dtype=torch.float32,\n            device=self.device,\n        )","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"b36119c753d829a90a9c82af6723cfaf685f11b6727938feb3b1855081984d4e","processedContent":"class VAEEncoder(BaseModel): #This code defines a class called \"SD Model\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested.\n    def __init__(self, device, max_batch_size, min_batch_size=1):\n        \"\"\"This code defines a class called \"VAEEncoder\" that is used for image-text matching. It takes in various parameters, such as the device to use and the maximum and minimum batch sizes, and has attributes for the embedding dimension, text max length, and the minimum and maximum latent shapes.\"\"\"\n        pass\n\n    def get_input_names(self):\n        \"\"\"This code defines a function called `get_input_names` that returns a list of strings representing the names of input data required for the model to run.\"\"\"\n        pass\n\n    def get_output_names(self):\n        \"\"\"This code defines a function called `get_output_names` that returns a list of strings representing the names of the output variables for a machine learning model.\"\"\"\n        pass\n\n    def get_dynamic_axes(self):\n        \"\"\"This code defines a function called `get_dynamic_axes` that returns a dictionary with two keys: \"images\" and \"latent\". The values of these keys are also dictionaries that map the indices of the input tensors to their corresponding batch, height, and width dimensions.\"\"\"\n        pass\n\n    def get_input_profile(self, batch_size, image_height, image_width, static_batch, static_shape):\n        \"\"\"This code defines a function that returns the minimum and maximum values for various dimensions in an image processing pipeline. It takes into account the batch size, image height and width, and whether the shapes are static or dynamic.\"\"\"\n        pass\n\n    def get_shape_dict(self, batch_size, image_height, image_width):\n        \"\"\"This code defines a function that returns a dictionary with two keys: \"images\" and \"latent\". The \"images\" key has a shape of (batch_size, 3, image_height, image_width), while the \"latent\" key has a shape of (batch_size, 4, latent_height, latent_width). The function checks the dimensions of an image and returns the latent height and width based on certain business rules.\"\"\"\n        pass\n\n    def get_sample_input(self, batch_size, image_height, image_width):\n        self.check_dims(batch_size, image_height, image_width)\n        return torch.randn(\n            batch_size,\n            3,\n            image_height,\n            image_width,\n            dtype=torch.float32,\n            device=self.device,\n        )","documentation":"This code defines a class called \"VAEEncoder\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/TorchVAEEncoder#vae.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/TorchVAEEncoder#vae.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#build().","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#build().","language":"python","range":[30,4,93,32],"content":"def build(\n        self,\n        onnx_path: str,\n        onnx_opt_path: str,\n        engine_path: str,\n        opt_image_height: int = 512,\n        opt_image_width: int = 512,\n        opt_batch_size: int = 1,\n        min_image_resolution: int = 256,\n        max_image_resolution: int = 1024,\n        build_enable_refit: bool = False,\n        build_static_batch: bool = False,\n        build_dynamic_shape: bool = False,\n        build_all_tactics: bool = False,\n        onnx_opset: int = 17,\n        force_engine_build: bool = False,\n        force_onnx_export: bool = False,\n        force_onnx_optimize: bool = False,\n    ):\n        if not force_onnx_export and os.path.exists(onnx_path):\n            print(f\"Found cached model: {onnx_path}\")\n        else:\n            print(f\"Exporting model: {onnx_path}\")\n            export_onnx(\n                self.network,\n                onnx_path=onnx_path,\n                model_data=self.model,\n                opt_image_height=opt_image_height,\n                opt_image_width=opt_image_width,\n                opt_batch_size=opt_batch_size,\n                onnx_opset=onnx_opset,\n            )\n            del self.network\n            gc.collect()\n            torch.cuda.empty_cache()\n        if not force_onnx_optimize and os.path.exists(onnx_opt_path):\n            print(f\"Found cached model: {onnx_opt_path}\")\n        else:\n            print(f\"Generating optimizing model: {onnx_opt_path}\")\n            optimize_onnx(\n                onnx_path=onnx_path,\n                onnx_opt_path=onnx_opt_path,\n                model_data=self.model,\n            )\n        self.model.min_latent_shape = min_image_resolution // 8\n        self.model.max_latent_shape = max_image_resolution // 8\n        if not force_engine_build and os.path.exists(engine_path):\n            print(f\"Found cached engine: {engine_path}\")\n        else:\n            build_engine(\n                engine_path=engine_path,\n                onnx_opt_path=onnx_opt_path,\n                model_data=self.model,\n                opt_image_height=opt_image_height,\n                opt_image_width=opt_image_width,\n                opt_batch_size=opt_batch_size,\n                build_static_batch=build_static_batch,\n                build_dynamic_shape=build_dynamic_shape,\n                build_all_tactics=build_all_tactics,\n                build_enable_refit=build_enable_refit,\n            )\n\n        gc.collect()\n        torch.cuda.empty_cache()","file":"/src/streamdiffusion/acceleration/tensorrt/builder.py","fileHash":"aa86d5cd723aa7f220318f2e9933ff7dbdc8e34e7840d30c4846726ed044fdc6","hash":"2dccc1201b6a1bb09cc6a7879ad99a3f9e143248b3936c12f14bf7e95eb27a63","processedContent":"def build(\n        self,\n        onnx_path: str,\n        onnx_opt_path: str,\n        engine_path: str,\n        opt_image_height: int = 512,\n        opt_image_width: int = 512,\n        opt_batch_size: int = 1,\n        min_image_resolution: int = 256,\n        max_image_resolution: int = 1024,\n        build_enable_refit: bool = False,\n        build_static_batch: bool = False,\n        build_dynamic_shape: bool = False,\n        build_all_tactics: bool = False,\n        onnx_opset: int = 17,\n        force_engine_build: bool = False,\n        force_onnx_export: bool = False,\n        force_onnx_optimize: bool = False,\n    ):\n        if not force_onnx_export and os.path.exists(onnx_path):\n        \"\"\"\n        scip-python python python-stdlib 3.11 os/__init__:: undefined\n        scip-python python python-stdlib 3.11 os/path.: undefined\n        scip-python python python-stdlib 3.11 ntpath/join().: undefined\n        \"\"\"\n            print(f\"Found cached model: {onnx_path}\")\n        else:\n            print(f\"Exporting model: {onnx_path}\")\n            export_onnx( #This code exports a PyTorch model to ONNX format, which is a widely used open-source format for machine learning models. It takes in various parameters such as the model, input dimensions, and optimization options, and exports the model to an ONNX file.\n                self.network, #undefined\n                onnx_path=onnx_path,\n                model_data=self.model, #undefined\n                opt_image_height=opt_image_height,\n                opt_image_width=opt_image_width,\n                opt_batch_size=opt_batch_size,\n                onnx_opset=onnx_opset,\n            )\n            del self.network #undefined\n            gc.collect()\n            \"\"\"\n            scip-python python python-stdlib 3.11 gc/__init__:: undefined\n            scip-python python python-stdlib 3.11 gc/collect().: undefined\n            \"\"\"\n        \"\"\"\n        scip-python python python-stdlib 3.11 gc/__init__:: undefined\n        scip-python python python-stdlib 3.11 gc/collect().: undefined\n        \"\"\"\n            torch.cuda.empty_cache()\n        if not force_onnx_optimize and os.path.exists(onnx_opt_path):\n        \"\"\"\n        scip-python python python-stdlib 3.11 os/__init__:: undefined\n        scip-python python python-stdlib 3.11 os/path.: undefined\n        scip-python python python-stdlib 3.11 ntpath/join().: undefined\n        \"\"\"\n            print(f\"Found cached model: {onnx_opt_path}\")\n        else:\n            print(f\"Generating optimizing model: {onnx_opt_path}\")\n            optimize_onnx( #This code optimizes an ONNX graph and exports it as an ONNX model if requested. It uses the `Optimizer` class to clean up the graph, select outputs, fold constants, and infer shapes. Additionally, it clears the GPU cache using `torch.cuda.empty_cache()`.\n                onnx_path=onnx_path,\n                onnx_opt_path=onnx_opt_path,\n                model_data=self.model, #undefined\n            )\n        self.model.min_latent_shape = min_image_resolution // 8\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#model.: undefined\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#min_latent_shape.: undefined\n        \"\"\"\n        self.model.max_latent_shape = max_image_resolution // 8\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#model.: undefined\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#max_latent_shape.: undefined\n        \"\"\"\n        if not force_engine_build and os.path.exists(engine_path):\n        \"\"\"\n        scip-python python python-stdlib 3.11 os/__init__:: undefined\n        scip-python python python-stdlib 3.11 os/path.: undefined\n        scip-python python python-stdlib 3.11 ntpath/join().: undefined\n        \"\"\"\n            print(f\"Found cached engine: {engine_path}\")\n        else:\n            build_engine( #This code defines a function called `build_engine` that builds a TensorRT engine from an ONNX model, allowing for customization of the engine's performance through various parameters.\n                engine_path=engine_path,\n                onnx_opt_path=onnx_opt_path,\n                model_data=self.model, #undefined\n                opt_image_height=opt_image_height,\n                opt_image_width=opt_image_width,\n                opt_batch_size=opt_batch_size,\n                build_static_batch=build_static_batch,\n                build_dynamic_shape=build_dynamic_shape,\n                build_all_tactics=build_all_tactics,\n                build_enable_refit=build_enable_refit,\n            )\n\n        gc.collect()\n        torch.cuda.empty_cache()","documentation":"The code exports a PyTorch model to ONNX format, optimizes the ONNX graph, and builds a TensorRT engine from the optimized ONNX model."}},{"key":"scip-python python python-stdlib 3.11 gc/collect().","attributes":{"symbol":"scip-python python python-stdlib 3.11 gc/collect().","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#__init__().","attributes":{"range":[19,4,28,30],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#__init__().","content":"def __init__(\n        self,\n        model: BaseModel,\n        network: Any,\n        device=torch.device(\"cuda\"),\n    ):\n        self.device = device\n\n        self.model = model\n        self.network = network","file":"/src/streamdiffusion/acceleration/tensorrt/builder.py","language":"python","fileHash":"aa86d5cd723aa7f220318f2e9933ff7dbdc8e34e7840d30c4846726ed044fdc6","hash":"0f40e29cec8b0278187a70653384ce03d4908b7e5d5dde8e2481117af9524f4f","processedContent":"def __init__(\n        self,\n        model: BaseModel, #This code defines a class called \"SD Model\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested.\n        network: Any, #undefined\n        device=torch.device(\"cuda\"),\n    ):\n        self.device = device\n\n        self.model = model\n        self.network = network","documentation":"This code defines a class called \"SD Model\" that is used for image-text matching. It has various functions to retrieve information about the model and its inputs and outputs, as well as an `optimize` function that optimizes an ONNX graph and exports it as an ONNX model if requested."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","language":"python","range":[0,0,441,0],"content":"#! fork: https://github.com/NVIDIA/TensorRT/blob/main/demo/Diffusion/utilities.py\n\n#\n# Copyright 2022 The HuggingFace Inc. team.\n# SPDX-FileCopyrightText: Copyright (c) 1993-2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: Apache-2.0\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport gc\nfrom collections import OrderedDict\nfrom typing import *\n\nimport numpy as np\nimport onnx\nimport onnx_graphsurgeon as gs\nimport tensorrt as trt\nimport torch\nfrom cuda import cudart\nfrom PIL import Image\nfrom polygraphy import cuda\nfrom polygraphy.backend.common import bytes_from_path\nfrom polygraphy.backend.trt import (\n    CreateConfig,\n    Profile,\n    engine_from_bytes,\n    engine_from_network,\n    network_from_onnx_path,\n    save_engine,\n)\nfrom polygraphy.backend.trt import util as trt_util\n\nfrom .models import CLIP, VAE, BaseModel, UNet, VAEEncoder\n\n\nTRT_LOGGER = trt.Logger(trt.Logger.ERROR)\n\n# Map of numpy dtype -> torch dtype\nnumpy_to_torch_dtype_dict = {\n    np.uint8: torch.uint8,\n    np.int8: torch.int8,\n    np.int16: torch.int16,\n    np.int32: torch.int32,\n    np.int64: torch.int64,\n    np.float16: torch.float16,\n    np.float32: torch.float32,\n    np.float64: torch.float64,\n    np.complex64: torch.complex64,\n    np.complex128: torch.complex128,\n}\nif np.version.full_version >= \"1.24.0\":\n    numpy_to_torch_dtype_dict[np.bool_] = torch.bool\nelse:\n    numpy_to_torch_dtype_dict[np.bool] = torch.bool\n\n# Map of torch dtype -> numpy dtype\ntorch_to_numpy_dtype_dict = {value: key for (key, value) in numpy_to_torch_dtype_dict.items()}\n\n\ndef CUASSERT(cuda_ret):\n    err = cuda_ret[0]\n    if err != cudart.cudaError_t.cudaSuccess:\n        raise RuntimeError(\n            f\"CUDA ERROR: {err}, error code reference: https://nvidia.github.io/cuda-python/module/cudart.html#cuda.cudart.cudaError_t\"\n        )\n    if len(cuda_ret) > 1:\n        return cuda_ret[1]\n    return None\n\n\nclass Engine:\n    def __init__(\n        self,\n        engine_path,\n    ):\n        self.engine_path = engine_path\n        self.engine = None\n        self.context = None\n        self.buffers = OrderedDict()\n        self.tensors = OrderedDict()\n        self.cuda_graph_instance = None  # cuda graph\n\n    def __del__(self):\n        [buf.free() for buf in self.buffers.values() if isinstance(buf, cuda.DeviceArray)]\n        del self.engine\n        del self.context\n        del self.buffers\n        del self.tensors\n\n    def refit(self, onnx_path, onnx_refit_path):\n        def convert_int64(arr):\n            # TODO: smarter conversion\n            if len(arr.shape) == 0:\n                return np.int32(arr)\n            return arr\n\n        def add_to_map(refit_dict, name, values):\n            if name in refit_dict:\n                assert refit_dict[name] is None\n                if values.dtype == np.int64:\n                    values = convert_int64(values)\n                refit_dict[name] = values\n\n        print(f\"Refitting TensorRT engine with {onnx_refit_path} weights\")\n        refit_nodes = gs.import_onnx(onnx.load(onnx_refit_path)).toposort().nodes\n\n        # Construct mapping from weight names in refit model -> original model\n        name_map = {}\n        for n, node in enumerate(gs.import_onnx(onnx.load(onnx_path)).toposort().nodes):\n            refit_node = refit_nodes[n]\n            assert node.op == refit_node.op\n            # Constant nodes in ONNX do not have inputs but have a constant output\n            if node.op == \"Constant\":\n                name_map[refit_node.outputs[0].name] = node.outputs[0].name\n            # Handle scale and bias weights\n            elif node.op == \"Conv\":\n                if node.inputs[1].__class__ == gs.Constant:\n                    name_map[refit_node.name + \"_TRTKERNEL\"] = node.name + \"_TRTKERNEL\"\n                if node.inputs[2].__class__ == gs.Constant:\n                    name_map[refit_node.name + \"_TRTBIAS\"] = node.name + \"_TRTBIAS\"\n            # For all other nodes: find node inputs that are initializers (gs.Constant)\n            else:\n                for i, inp in enumerate(node.inputs):\n                    if inp.__class__ == gs.Constant:\n                        name_map[refit_node.inputs[i].name] = inp.name\n\n        def map_name(name):\n            if name in name_map:\n                return name_map[name]\n            return name\n\n        # Construct refit dictionary\n        refit_dict = {}\n        refitter = trt.Refitter(self.engine, TRT_LOGGER)\n        all_weights = refitter.get_all()\n        for layer_name, role in zip(all_weights[0], all_weights[1]):\n            # for speciailized roles, use a unique name in the map:\n            if role == trt.WeightsRole.KERNEL:\n                name = layer_name + \"_TRTKERNEL\"\n            elif role == trt.WeightsRole.BIAS:\n                name = layer_name + \"_TRTBIAS\"\n            else:\n                name = layer_name\n\n            assert name not in refit_dict, \"Found duplicate layer: \" + name\n            refit_dict[name] = None\n\n        for n in refit_nodes:\n            # Constant nodes in ONNX do not have inputs but have a constant output\n            if n.op == \"Constant\":\n                name = map_name(n.outputs[0].name)\n                print(f\"Add Constant {name}\\n\")\n                add_to_map(refit_dict, name, n.outputs[0].values)\n\n            # Handle scale and bias weights\n            elif n.op == \"Conv\":\n                if n.inputs[1].__class__ == gs.Constant:\n                    name = map_name(n.name + \"_TRTKERNEL\")\n                    add_to_map(refit_dict, name, n.inputs[1].values)\n\n                if n.inputs[2].__class__ == gs.Constant:\n                    name = map_name(n.name + \"_TRTBIAS\")\n                    add_to_map(refit_dict, name, n.inputs[2].values)\n\n            # For all other nodes: find node inputs that are initializers (AKA gs.Constant)\n            else:\n                for inp in n.inputs:\n                    name = map_name(inp.name)\n                    if inp.__class__ == gs.Constant:\n                        add_to_map(refit_dict, name, inp.values)\n\n        for layer_name, weights_role in zip(all_weights[0], all_weights[1]):\n            if weights_role == trt.WeightsRole.KERNEL:\n                custom_name = layer_name + \"_TRTKERNEL\"\n            elif weights_role == trt.WeightsRole.BIAS:\n                custom_name = layer_name + \"_TRTBIAS\"\n            else:\n                custom_name = layer_name\n\n            # Skip refitting Trilu for now; scalar weights of type int64 value 1 - for clip model\n            if layer_name.startswith(\"onnx::Trilu\"):\n                continue\n\n            if refit_dict[custom_name] is not None:\n                refitter.set_weights(layer_name, weights_role, refit_dict[custom_name])\n            else:\n                print(f\"[W] No refit weights for layer: {layer_name}\")\n\n        if not refitter.refit_cuda_engine():\n            print(\"Failed to refit!\")\n            exit(0)\n\n    def build(\n        self,\n        onnx_path,\n        fp16,\n        input_profile=None,\n        enable_refit=False,\n        enable_all_tactics=False,\n        timing_cache=None,\n        workspace_size=0,\n    ):\n        print(f\"Building TensorRT engine for {onnx_path}: {self.engine_path}\")\n        p = Profile()\n        if input_profile:\n            for name, dims in input_profile.items():\n                assert len(dims) == 3\n                p.add(name, min=dims[0], opt=dims[1], max=dims[2])\n\n        config_kwargs = {}\n\n        if workspace_size > 0:\n            config_kwargs[\"memory_pool_limits\"] = {trt.MemoryPoolType.WORKSPACE: workspace_size}\n        if not enable_all_tactics:\n            config_kwargs[\"tactic_sources\"] = []\n\n        engine = engine_from_network(\n            network_from_onnx_path(onnx_path, flags=[trt.OnnxParserFlag.NATIVE_INSTANCENORM]),\n            config=CreateConfig(\n                fp16=fp16, refittable=enable_refit, profiles=[p], load_timing_cache=timing_cache, **config_kwargs\n            ),\n            save_timing_cache=timing_cache,\n        )\n        save_engine(engine, path=self.engine_path)\n\n    def load(self):\n        print(f\"Loading TensorRT engine: {self.engine_path}\")\n        self.engine = engine_from_bytes(bytes_from_path(self.engine_path))\n\n    def activate(self, reuse_device_memory=None):\n        if reuse_device_memory:\n            self.context = self.engine.create_execution_context_without_device_memory()\n            self.context.device_memory = reuse_device_memory\n        else:\n            self.context = self.engine.create_execution_context()\n\n    def allocate_buffers(self, shape_dict=None, device=\"cuda\"):\n        for idx in range(trt_util.get_bindings_per_profile(self.engine)):\n            binding = self.engine[idx]\n            if shape_dict and binding in shape_dict:\n                shape = shape_dict[binding]\n            else:\n                shape = self.engine.get_binding_shape(binding)\n            dtype = trt.nptype(self.engine.get_binding_dtype(binding))\n            if self.engine.binding_is_input(binding):\n                self.context.set_binding_shape(idx, shape)\n            tensor = torch.empty(tuple(shape), dtype=numpy_to_torch_dtype_dict[dtype]).to(device=device)\n            self.tensors[binding] = tensor\n\n    def infer(self, feed_dict, stream, use_cuda_graph=False):\n        for name, buf in feed_dict.items():\n            self.tensors[name].copy_(buf)\n\n        for name, tensor in self.tensors.items():\n            self.context.set_tensor_address(name, tensor.data_ptr())\n\n        if use_cuda_graph:\n            if self.cuda_graph_instance is not None:\n                CUASSERT(cudart.cudaGraphLaunch(self.cuda_graph_instance, stream.ptr))\n                CUASSERT(cudart.cudaStreamSynchronize(stream.ptr))\n            else:\n                # do inference before CUDA graph capture\n                noerror = self.context.execute_async_v3(stream.ptr)\n                if not noerror:\n                    raise ValueError(\"ERROR: inference failed.\")\n                # capture cuda graph\n                CUASSERT(\n                    cudart.cudaStreamBeginCapture(stream.ptr, cudart.cudaStreamCaptureMode.cudaStreamCaptureModeGlobal)\n                )\n                self.context.execute_async_v3(stream.ptr)\n                self.graph = CUASSERT(cudart.cudaStreamEndCapture(stream.ptr))\n                self.cuda_graph_instance = CUASSERT(cudart.cudaGraphInstantiate(self.graph, 0))\n        else:\n            noerror = self.context.execute_async_v3(stream.ptr)\n            if not noerror:\n                raise ValueError(\"ERROR: inference failed.\")\n\n        return self.tensors\n\n\ndef decode_images(images: torch.Tensor):\n    images = (\n        ((images + 1) * 255 / 2).clamp(0, 255).detach().permute(0, 2, 3, 1).round().type(torch.uint8).cpu().numpy()\n    )\n    return [Image.fromarray(x) for x in images]\n\n\ndef preprocess_image(image: Image.Image):\n    w, h = image.size\n    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n    image = image.resize((w, h))\n    init_image = np.array(image).astype(np.float32) / 255.0\n    init_image = init_image[None].transpose(0, 3, 1, 2)\n    init_image = torch.from_numpy(init_image).contiguous()\n    return 2.0 * init_image - 1.0\n\n\ndef prepare_mask_and_masked_image(image: Image.Image, mask: Image.Image):\n    if isinstance(image, Image.Image):\n        image = np.array(image.convert(\"RGB\"))\n    image = image[None].transpose(0, 3, 1, 2)\n    image = torch.from_numpy(image).to(dtype=torch.float32).contiguous() / 127.5 - 1.0\n    if isinstance(mask, Image.Image):\n        mask = np.array(mask.convert(\"L\"))\n        mask = mask.astype(np.float32) / 255.0\n    mask = mask[None, None]\n    mask[mask < 0.5] = 0\n    mask[mask >= 0.5] = 1\n    mask = torch.from_numpy(mask).to(dtype=torch.float32).contiguous()\n\n    masked_image = image * (mask < 0.5)\n\n    return mask, masked_image\n\n\ndef create_models(\n    model_id: str,\n    use_auth_token: Optional[str],\n    device: Union[str, torch.device],\n    max_batch_size: int,\n    unet_in_channels: int = 4,\n    embedding_dim: int = 768,\n):\n    models = {\n        \"clip\": CLIP(\n            hf_token=use_auth_token,\n            device=device,\n            max_batch_size=max_batch_size,\n            embedding_dim=embedding_dim,\n        ),\n        \"unet\": UNet(\n            hf_token=use_auth_token,\n            fp16=True,\n            device=device,\n            max_batch_size=max_batch_size,\n            embedding_dim=embedding_dim,\n            unet_dim=unet_in_channels,\n        ),\n        \"vae\": VAE(\n            hf_token=use_auth_token,\n            device=device,\n            max_batch_size=max_batch_size,\n            embedding_dim=embedding_dim,\n        ),\n        \"vae_encoder\": VAEEncoder(\n            hf_token=use_auth_token,\n            device=device,\n            max_batch_size=max_batch_size,\n            embedding_dim=embedding_dim,\n        ),\n    }\n    return models\n\n\ndef build_engine(\n    engine_path: str,\n    onnx_opt_path: str,\n    model_data: BaseModel,\n    opt_image_height: int,\n    opt_image_width: int,\n    opt_batch_size: int,\n    build_static_batch: bool = False,\n    build_dynamic_shape: bool = False,\n    build_all_tactics: bool = False,\n    build_enable_refit: bool = False,\n):\n    _, free_mem, _ = cudart.cudaMemGetInfo()\n    GiB = 2**30\n    if free_mem > 6 * GiB:\n        activation_carveout = 4 * GiB\n        max_workspace_size = free_mem - activation_carveout\n    else:\n        max_workspace_size = 0\n    engine = Engine(engine_path)\n    input_profile = model_data.get_input_profile(\n        opt_batch_size,\n        opt_image_height,\n        opt_image_width,\n        static_batch=build_static_batch,\n        static_shape=not build_dynamic_shape,\n    )\n    engine.build(\n        onnx_opt_path,\n        fp16=True,\n        input_profile=input_profile,\n        enable_refit=build_enable_refit,\n        enable_all_tactics=build_all_tactics,\n        workspace_size=max_workspace_size,\n    )\n\n    return engine\n\n\ndef export_onnx(\n    model,\n    onnx_path: str,\n    model_data: BaseModel,\n    opt_image_height: int,\n    opt_image_width: int,\n    opt_batch_size: int,\n    onnx_opset: int,\n):\n    with torch.inference_mode(), torch.autocast(\"cuda\"):\n        inputs = model_data.get_sample_input(opt_batch_size, opt_image_height, opt_image_width)\n        torch.onnx.export(\n            model,\n            inputs,\n            onnx_path,\n            export_params=True,\n            opset_version=onnx_opset,\n            do_constant_folding=True,\n            input_names=model_data.get_input_names(),\n            output_names=model_data.get_output_names(),\n            dynamic_axes=model_data.get_dynamic_axes(),\n        )\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()\n\n\ndef optimize_onnx(\n    onnx_path: str,\n    onnx_opt_path: str,\n    model_data: BaseModel,\n):\n    onnx_opt_graph = model_data.optimize(onnx.load(onnx_path))\n    onnx.save(onnx_opt_graph, onnx_opt_path)\n    del onnx_opt_graph\n    gc.collect()\n    torch.cuda.empty_cache()\n","file":"/src/streamdiffusion/acceleration/tensorrt/utilities.py","fileHash":"63a6abfeb186705edc5dba1605e60b5eaa1a86e848db1ddcffe555e16476075c","hash":"63a6abfeb186705edc5dba1605e60b5eaa1a86e848db1ddcffe555e16476075c","processedContent":"#! fork: https://github.com/NVIDIA/TensorRT/blob/main/demo/Diffusion/utilities.py\n\n#\n# Copyright 2022 The HuggingFace Inc. team.\n# SPDX-FileCopyrightText: Copyright (c) 1993-2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: Apache-2.0\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport gc #undefined\nfrom collections import OrderedDict\n\"\"\"\nscip-python python python-stdlib 3.11 collections/__init__:: undefined\nscip-python python python-stdlib 3.11 collections/OrderedDict#: undefined\n\"\"\"\nfrom typing import * #undefined\n\nimport numpy as np #undefined\nimport onnx\nimport onnx_graphsurgeon as gs\nimport tensorrt as trt #The code defines a class called StreamDiffusion that performs image denoising using a VAE model, with functions for loading pre-trained weights, fusing the LoRA model with other models, and enabling or disabling a similar image filter.\nimport torch\nfrom cuda import cudart #undefined\nfrom PIL import Image\n\"\"\"\nscip-python python Pillow 10.0.0 PIL/__init__:: undefined\nscip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\n\"\"\"\nfrom polygraphy import cuda #undefined\nfrom polygraphy.backend.common import bytes_from_path #undefined\nfrom polygraphy.backend.trt import ( #undefined\n    CreateConfig,\n    Profile,\n    engine_from_bytes,\n    engine_from_network,\n    network_from_onnx_path,\n    save_engine,\n)\nfrom polygraphy.backend.trt import util as trt_util #undefined\n\nfrom .models import CLIP, VAE, BaseModel, UNet, VAEEncoder\n\"\"\"\nscip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/__init__:: The code defines a series of classes for image-text matching, including Optimizer, BaseModel, CLIP, UNet, VAE, and VAEEncoder. These classes are used to optimize ONNX graphs, retrieve information about the models and their inputs and outputs, and export optimized graphs as ONNX models if requested.\nscip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#: This code defines a class called \"SD Model\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested.\nscip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#: This code defines a class called \"VAE decoder\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested.\nscip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#: This code defines a class called \"SD Model\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested.\nscip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#: This code defines a class called \"SD Model\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested.\nscip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#: This code defines a class called \"VAEEncoder\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested.\n\"\"\"\n\n\nTRT_LOGGER = trt.Logger(trt.Logger.ERROR)\n\n# Map of numpy dtype -> torch dtype\nnumpy_to_torch_dtype_dict = {\n    np.uint8: torch.uint8, #undefined\n    np.int8: torch.int8, #undefined\n    np.int16: torch.int16, #undefined\n    np.int32: torch.int32, #undefined\n    np.int64: torch.int64, #undefined\n    np.float16: torch.float16, #undefined\n    np.float32: torch.float32, #undefined\n    np.float64: torch.float64, #undefined\n    np.complex64: torch.complex64, #undefined\n    np.complex128: torch.complex128, #undefined\n}\nif np.version.full_version >= \"1.24.0\":\n\"\"\"\nscip-python python numpy 1.25.2 `numpy.version`/__init__:: undefined\nscip-python python numpy 1.25.2 `numpy.version`/full_version.: undefined\n\"\"\"\n    numpy_to_torch_dtype_dict[np.bool_] = torch.bool\n    \"\"\"\n    scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/numpy_to_torch_dtype_dict.: undefined\n    scip-python python numpy 1.25.2 numpy/bool_#: undefined\n    \"\"\"\nelse:\n    numpy_to_torch_dtype_dict[np.bool] = torch.bool #undefined\n\n# Map of torch dtype -> numpy dtype\ntorch_to_numpy_dtype_dict = {value: key for (key, value) in numpy_to_torch_dtype_dict.items()} #undefined\n\n\ndef CUASSERT(cuda_ret):\n    \"\"\"This code defines a function called CUASSERT that checks the return value of a CUDA call and raises an error if it fails. It also returns the result of the CUDA call if it succeeds.\"\"\"\n    pass\n\n\nclass Engine:\n    \"\"\"The code defines a class called Engine that initializes an object with various attributes and methods for building, loading, and executing a TensorRT engine.\"\"\"\n    pass\n\n\ndef decode_images(images: torch.Tensor):\n    \"\"\"This code takes a tensor of images and converts it into a list of PIL Image objects, each representing a single image.\"\"\"\n    pass\n\n\ndef preprocess_image(image: Image.Image):\n    \"\"\"This code preprocesses an image by resizing it to a multiple of 32, converting it to a numpy array, normalizing the values, and then converting it to a PyTorch tensor.\"\"\"\n    pass\n\n\ndef prepare_mask_and_masked_image(image: Image.Image, mask: Image.Image):\n    \"\"\"The code prepares an image and its mask for a deep learning model by converting the image to a numpy array, transposing it, normalizing it, and creating a binary mask from the original mask.\"\"\"\n    pass\n\n\ndef create_models(\n    \"\"\"This code defines a function called `create_models` that takes in several parameters and returns a dictionary of models. The models are defined as classes with various functions, including `get_model`, `get_input_names`, and `get_output_names`. Additionally, the function has an `optimize` function that optimizes an ONNX graph and exports it as an ONNX model if requested.\"\"\"\n    pass\n\n\ndef build_engine(\n    \"\"\"This code defines a function called `build_engine` that builds a TensorRT engine from an ONNX model, allowing for customization of the engine's performance through various parameters.\"\"\"\n    pass\n\n\ndef export_onnx(\n    \"\"\"This code exports a PyTorch model to ONNX format, which is a widely used open-source format for machine learning models. It takes in various parameters such as the model, input dimensions, and optimization options, and exports the model to an ONNX file.\"\"\"\n    pass\n\n\ndef optimize_onnx(\n    \"\"\"This code optimizes an ONNX graph and exports it as an ONNX model if requested. It uses the `Optimizer` class to clean up the graph, select outputs, fold constants, and infer shapes. Additionally, it clears the GPU cache using `torch.cuda.empty_cache()`.\"\"\"\n    pass\n","documentation":"The code defines a class called StreamDiffusion that performs image denoising using a VAE model, with functions for loading pre-trained weights, fusing the LoRA model with other models, and enabling or disabling a similar image filter."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/build_engine().","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/build_engine().","language":"python","range":[365,0,401,17],"content":"def build_engine(\n    engine_path: str,\n    onnx_opt_path: str,\n    model_data: BaseModel,\n    opt_image_height: int,\n    opt_image_width: int,\n    opt_batch_size: int,\n    build_static_batch: bool = False,\n    build_dynamic_shape: bool = False,\n    build_all_tactics: bool = False,\n    build_enable_refit: bool = False,\n):\n    _, free_mem, _ = cudart.cudaMemGetInfo()\n    GiB = 2**30\n    if free_mem > 6 * GiB:\n        activation_carveout = 4 * GiB\n        max_workspace_size = free_mem - activation_carveout\n    else:\n        max_workspace_size = 0\n    engine = Engine(engine_path)\n    input_profile = model_data.get_input_profile(\n        opt_batch_size,\n        opt_image_height,\n        opt_image_width,\n        static_batch=build_static_batch,\n        static_shape=not build_dynamic_shape,\n    )\n    engine.build(\n        onnx_opt_path,\n        fp16=True,\n        input_profile=input_profile,\n        enable_refit=build_enable_refit,\n        enable_all_tactics=build_all_tactics,\n        workspace_size=max_workspace_size,\n    )\n\n    return engine","file":"/src/streamdiffusion/acceleration/tensorrt/utilities.py","fileHash":"63a6abfeb186705edc5dba1605e60b5eaa1a86e848db1ddcffe555e16476075c","hash":"20de494f36ae02007e5797006de318431c11bfcb0041369b7dd840ae41c8cf31","processedContent":"def build_engine(\n    engine_path: str,\n    onnx_opt_path: str,\n    model_data: BaseModel, #This code defines a class called \"SD Model\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested.\n    opt_image_height: int,\n    opt_image_width: int,\n    opt_batch_size: int,\n    build_static_batch: bool = False,\n    build_dynamic_shape: bool = False,\n    build_all_tactics: bool = False,\n    build_enable_refit: bool = False,\n):\n    _, free_mem, _ = cudart.cudaMemGetInfo()\n    GiB = 2**30\n    if free_mem > 6 * GiB:\n        activation_carveout = 4 * GiB\n        max_workspace_size = free_mem - activation_carveout\n    else:\n        max_workspace_size = 0\n    engine = Engine(engine_path) #The code defines a class called Engine that initializes an object with various attributes and methods for building, loading, and executing a TensorRT engine.\n    input_profile = model_data.get_input_profile( #This code defines a function called `get_input_profile` that returns a set of input parameters for a machine learning model. The function takes in five arguments: `batch_size`, `image_height`, `image_width`, `static_batch`, and `static_shape`. These parameters are used to determine the shape of the input data and the batch size of the model.\n        opt_batch_size,\n        opt_image_height,\n        opt_image_width,\n        static_batch=build_static_batch,\n        static_shape=not build_dynamic_shape,\n    )\n    engine.build( #This code builds a TensorRT engine from an ONNX model, allowing for customization of the engine's performance through various parameters.\n        onnx_opt_path,\n        fp16=True,\n        input_profile=input_profile,\n        enable_refit=build_enable_refit,\n        enable_all_tactics=build_all_tactics,\n        workspace_size=max_workspace_size,\n    )\n\n    return engine","documentation":"This code defines a function called `build_engine` that builds a TensorRT engine from an ONNX model, allowing for customization of the engine's performance through various parameters."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/export_onnx().","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/export_onnx().","language":"python","range":[404,0,428,28],"content":"def export_onnx(\n    model,\n    onnx_path: str,\n    model_data: BaseModel,\n    opt_image_height: int,\n    opt_image_width: int,\n    opt_batch_size: int,\n    onnx_opset: int,\n):\n    with torch.inference_mode(), torch.autocast(\"cuda\"):\n        inputs = model_data.get_sample_input(opt_batch_size, opt_image_height, opt_image_width)\n        torch.onnx.export(\n            model,\n            inputs,\n            onnx_path,\n            export_params=True,\n            opset_version=onnx_opset,\n            do_constant_folding=True,\n            input_names=model_data.get_input_names(),\n            output_names=model_data.get_output_names(),\n            dynamic_axes=model_data.get_dynamic_axes(),\n        )\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()","file":"/src/streamdiffusion/acceleration/tensorrt/utilities.py","fileHash":"63a6abfeb186705edc5dba1605e60b5eaa1a86e848db1ddcffe555e16476075c","hash":"299090072b7ba602bfd946315a1833468900af37f4887c041ab6b39693c61dc4","processedContent":"def export_onnx(\n    model,\n    onnx_path: str,\n    model_data: BaseModel, #This code defines a class called \"SD Model\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested.\n    opt_image_height: int,\n    opt_image_width: int,\n    opt_batch_size: int,\n    onnx_opset: int,\n):\n    with torch.inference_mode(), torch.autocast(\"cuda\"):\n        inputs = model_data.get_sample_input(opt_batch_size, opt_image_height, opt_image_width) #This code defines a function called `get_sample_input` that generates a batch of input data for training a machine learning model. The function takes three arguments: `batch_size`, `image_height`, and `image_width`. It returns a tensor with the specified dimensions.\n        torch.onnx.export(\n            model,\n            inputs,\n            onnx_path,\n            export_params=True,\n            opset_version=onnx_opset,\n            do_constant_folding=True,\n            input_names=model_data.get_input_names(), #This code defines a function called `get_input_names` that retrieves the names of the input fields in a form.\n            output_names=model_data.get_output_names(), #This code defines a function called `get_output_names` that returns a list of output names for the current model.\n            dynamic_axes=model_data.get_dynamic_axes(), #This code defines a function called `get_dynamic_axes` that returns `None`. It is used to dynamically determine the axes of a graph or chart based on the data being plotted.\n        )\n    del model\n    gc.collect()\n    \"\"\"\n    scip-python python python-stdlib 3.11 gc/__init__:: undefined\n    scip-python python python-stdlib 3.11 gc/collect().: undefined\n    \"\"\"\n    torch.cuda.empty_cache()","documentation":"This code exports a PyTorch model to ONNX format, which is a widely used open-source format for machine learning models. It takes in various parameters such as the model, input dimensions, and optimization options, and exports the model to an ONNX file."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/optimize_onnx().","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/optimize_onnx().","language":"python","range":[431,0,440,28],"content":"def optimize_onnx(\n    onnx_path: str,\n    onnx_opt_path: str,\n    model_data: BaseModel,\n):\n    onnx_opt_graph = model_data.optimize(onnx.load(onnx_path))\n    onnx.save(onnx_opt_graph, onnx_opt_path)\n    del onnx_opt_graph\n    gc.collect()\n    torch.cuda.empty_cache()","file":"/src/streamdiffusion/acceleration/tensorrt/utilities.py","fileHash":"63a6abfeb186705edc5dba1605e60b5eaa1a86e848db1ddcffe555e16476075c","hash":"1b800ef4b635955d4dbad39a11eee0646e1c62da79e1e9e13adf1dfeca2a24f8","processedContent":"def optimize_onnx(\n    onnx_path: str,\n    onnx_opt_path: str,\n    model_data: BaseModel, #This code defines a class called \"SD Model\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested.\n):\n    onnx_opt_graph = model_data.optimize(onnx.load(onnx_path)) #The code defines a class called `Optimizer` that optimizes an ONNX graph by cleaning it up, selecting outputs, folding constants, and inferring shapes. It also exports the optimized graph as an ONNX model if requested.\n    onnx.save(onnx_opt_graph, onnx_opt_path)\n    del onnx_opt_graph\n    gc.collect()\n    \"\"\"\n    scip-python python python-stdlib 3.11 gc/__init__:: undefined\n    scip-python python python-stdlib 3.11 gc/collect().: undefined\n    \"\"\"\n    torch.cuda.empty_cache()","documentation":"This code optimizes an ONNX graph and exports it as an ONNX model if requested. It uses the `Optimizer` class to clean up the graph, select outputs, fold constants, and infer shapes. Additionally, it clears the GPU cache using `torch.cuda.empty_cache()`."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#network.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#network.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#model.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#model.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#min_latent_shape.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#min_latent_shape.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#max_latent_shape.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#max_latent_shape.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#__init__().","attributes":{"range":[12,4,18,30],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#__init__().","content":"def __init__(self, filepath: str, stream: cuda.Stream, use_cuda_graph: bool = False):\n        self.engine = Engine(filepath)\n        self.stream = stream\n        self.use_cuda_graph = use_cuda_graph\n\n        self.engine.load()\n        self.engine.activate()","file":"/src/streamdiffusion/acceleration/tensorrt/engine.py","language":"python","fileHash":"dc4f85c80de38d40817cb2185c46f634aee5ae1c4e35ed91586498131c2d2cf7","hash":"a5baa4ff62a55ecc51dabdb24fa293df29a4786629bfc73fb44cdbaab0722c24","processedContent":"def __init__(self, filepath: str, stream: cuda.Stream, use_cuda_graph: bool = False):\n        self.engine = Engine(filepath) #The code defines a class called Engine that initializes an object with various attributes and methods for building, loading, and executing a TensorRT engine.\n        self.stream = stream\n        self.use_cuda_graph = use_cuda_graph\n\n        self.engine.load()\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#engine.: undefined\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#load().: This code loads a TensorRT engine from a file path and stores it in the `engine` variable.\n        \"\"\"\n        self.engine.activate()","documentation":"This code initializes an object of the `Engine` class, which loads a TensorRT engine from a file path and stores it in the `engine` variable."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#__call__().","attributes":{"range":[20,4,49,55],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#__call__().","content":"def __call__(\n        self,\n        latent_model_input: torch.Tensor,\n        timestep: torch.Tensor,\n        encoder_hidden_states: torch.Tensor,\n        **kwargs,\n    ) -> Any:\n        if timestep.dtype != torch.float32:\n            timestep = timestep.float()\n\n        self.engine.allocate_buffers(\n            shape_dict={\n                \"sample\": latent_model_input.shape,\n                \"timestep\": timestep.shape,\n                \"encoder_hidden_states\": encoder_hidden_states.shape,\n                \"latent\": latent_model_input.shape,\n            },\n            device=latent_model_input.device,\n        )\n\n        noise_pred = self.engine.infer(\n            {\n                \"sample\": latent_model_input,\n                \"timestep\": timestep,\n                \"encoder_hidden_states\": encoder_hidden_states,\n            },\n            self.stream,\n            use_cuda_graph=self.use_cuda_graph,\n        )[\"latent\"]\n        return UNet2DConditionOutput(sample=noise_pred)","file":"/src/streamdiffusion/acceleration/tensorrt/engine.py","language":"python","fileHash":"dc4f85c80de38d40817cb2185c46f634aee5ae1c4e35ed91586498131c2d2cf7","hash":"69ce8648c2c046c1d59ea9f6161ed8d00dd0aa2c80c1e896ddcdb30d5188a1d5","processedContent":"def __call__(\n        self,\n        latent_model_input: torch.Tensor,\n        timestep: torch.Tensor,\n        encoder_hidden_states: torch.Tensor,\n        **kwargs,\n    ) -> Any: #undefined\n        if timestep.dtype != torch.float32:\n            timestep = timestep.float()\n\n        self.engine.allocate_buffers(\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#engine.: undefined\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#allocate_buffers().: This code allocates memory for input and output tensors in a TensorRT engine, using the shapes and data types provided by the engine.\n        \"\"\"\n            shape_dict={\n                \"sample\": latent_model_input.shape,\n                \"timestep\": timestep.shape,\n                \"encoder_hidden_states\": encoder_hidden_states.shape,\n                \"latent\": latent_model_input.shape,\n            },\n            device=latent_model_input.device,\n        )\n\n        noise_pred = self.engine.infer(\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#engine.: undefined\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#infer().: This code defines a function called `infer` that takes in a feed dictionary, a stream, and an optional boolean flag indicating whether to use CUDA graph. It then sets the tensor addresses for each tensor in the feed dictionary and executes the inference using the TensorRT context. If the CUDA graph flag is set, it captures the CUDA graph and instantiates it.\n        \"\"\"\n            {\n                \"sample\": latent_model_input,\n                \"timestep\": timestep,\n                \"encoder_hidden_states\": encoder_hidden_states,\n            },\n            self.stream, #undefined\n            use_cuda_graph=self.use_cuda_graph, #undefined\n        )[\"latent\"]\n        return UNet2DConditionOutput(sample=noise_pred)","documentation":"This code defines a function called `infer` that takes in a feed dictionary, a stream, and an optional boolean flag indicating whether to use CUDA graph. It then sets the tensor addresses for each tensor in the feed dictionary and executes the inference using the TensorRT context. If the CUDA graph flag is set, it captures the CUDA graph and instantiates it."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#to().","attributes":{"range":[51,4,52,12],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#to().","content":"def to(self, *args, **kwargs):\n        pass","file":"/src/streamdiffusion/acceleration/tensorrt/engine.py","language":"python","fileHash":"dc4f85c80de38d40817cb2185c46f634aee5ae1c4e35ed91586498131c2d2cf7","hash":"719cb34b47ef259fbda3790867172c917c9f4a598c5185ea0a81050459991e43","processedContent":"def to(self, *args, **kwargs):\n        pass","documentation":"This code defines a function called `to` that takes any number of positional arguments and keyword arguments, and does nothing with them. It is a placeholder function that can be used as a stub or a starting point for more complex functionality."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#forward().","attributes":{"range":[54,4,55,12],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#forward().","content":"def forward(self, *args, **kwargs):\n        pass","file":"/src/streamdiffusion/acceleration/tensorrt/engine.py","language":"python","fileHash":"dc4f85c80de38d40817cb2185c46f634aee5ae1c4e35ed91586498131c2d2cf7","hash":"5ec3d0795c4fe75edbd2eefc41a33dc81216d810596927a56d0689d1cb29b9eb","processedContent":"def forward(self, *args, **kwargs):\n        pass","documentation":"This code defines a function called \"forward\" that takes in any number of arguments and keyword arguments, but does not perform any actual computation. It is likely used as a placeholder or a stub for future development."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#__init__().","attributes":{"range":[59,4,76,31],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#__init__().","content":"def __init__(\n        self,\n        encoder_path: str,\n        decoder_path: str,\n        stream: cuda.Stream,\n        scaling_factor: int,\n        use_cuda_graph: bool = False,\n    ):\n        self.encoder = Engine(encoder_path)\n        self.decoder = Engine(decoder_path)\n        self.stream = stream\n        self.vae_scale_factor = scaling_factor\n        self.use_cuda_graph = use_cuda_graph\n\n        self.encoder.load()\n        self.decoder.load()\n        self.encoder.activate()\n        self.decoder.activate()","file":"/src/streamdiffusion/acceleration/tensorrt/engine.py","language":"python","fileHash":"dc4f85c80de38d40817cb2185c46f634aee5ae1c4e35ed91586498131c2d2cf7","hash":"973fd1439a85d8f09ef6d7b3bd926e202f53717ecbf3293851bc43f96b2ceb90","processedContent":"def __init__(\n        self,\n        encoder_path: str,\n        decoder_path: str,\n        stream: cuda.Stream,\n        scaling_factor: int,\n        use_cuda_graph: bool = False,\n    ):\n        self.encoder = Engine(encoder_path) #The code defines a class called Engine that initializes an object with various attributes and methods for building, loading, and executing a TensorRT engine.\n        self.decoder = Engine(decoder_path) #The code defines a class called Engine that initializes an object with various attributes and methods for building, loading, and executing a TensorRT engine.\n        self.stream = stream\n        self.vae_scale_factor = scaling_factor\n        self.use_cuda_graph = use_cuda_graph\n\n        self.encoder.load()\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#encoder.: undefined\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#load().: This code loads a TensorRT engine from a file path and stores it in the `engine` variable.\n        \"\"\"\n        self.decoder.load()\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#load().: This code loads a TensorRT engine from a file path and stores it in the `engine` variable.\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#decoder.: undefined\n        \"\"\"\n        self.encoder.activate()\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#encoder.: undefined\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#activate().: This code creates an execution context for a TensorRT engine, which is used to perform inference on a neural network. The context can be reused by passing in a device memory object, which allows the engine to use the same memory for future inferences.\n        \"\"\"\n        self.decoder.activate()","documentation":"The code defines a class called AutoencoderKLEngine that initializes two objects of the Engine class, encoder and decoder, with various attributes and methods for building, loading, and executing TensorRT engines. It also creates execution contexts for the engines using the activate() method."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#encode().","attributes":{"range":[78,4,96,53],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#encode().","content":"def encode(self, images: torch.Tensor, **kwargs):\n        self.encoder.allocate_buffers(\n            shape_dict={\n                \"images\": images.shape,\n                \"latent\": (\n                    images.shape[0],\n                    4,\n                    images.shape[2] // self.vae_scale_factor,\n                    images.shape[3] // self.vae_scale_factor,\n                ),\n            },\n            device=images.device,\n        )\n        latents = self.encoder.infer(\n            {\"images\": images},\n            self.stream,\n            use_cuda_graph=self.use_cuda_graph,\n        )[\"latent\"]\n        return AutoencoderTinyOutput(latents=latents)","file":"/src/streamdiffusion/acceleration/tensorrt/engine.py","language":"python","fileHash":"dc4f85c80de38d40817cb2185c46f634aee5ae1c4e35ed91586498131c2d2cf7","hash":"d8a3e9b7fe50de57fcab67b041c4d56f813239d6640954cc4e1b9ada75b95ddc","processedContent":"def encode(self, images: torch.Tensor, **kwargs):\n        self.encoder.allocate_buffers(\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#encoder.: undefined\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#allocate_buffers().: This code allocates memory for input and output tensors in a TensorRT engine, using the shapes and data types provided by the engine.\n        \"\"\"\n            shape_dict={\n                \"images\": images.shape,\n                \"latent\": (\n                    images.shape[0],\n                    4,\n                    images.shape[2] // self.vae_scale_factor, #undefined\n                    images.shape[3] // self.vae_scale_factor, #undefined\n                ),\n            },\n            device=images.device,\n        )\n        latents = self.encoder.infer(\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#encoder.: undefined\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#infer().: This code defines a function called `infer` that takes in a feed dictionary, a stream, and an optional boolean flag indicating whether to use CUDA graph. It then sets the tensor addresses for each tensor in the feed dictionary and executes the inference using the TensorRT context. If the CUDA graph flag is set, it captures the CUDA graph and instantiates it.\n        \"\"\"\n            {\"images\": images},\n            self.stream, #undefined\n            use_cuda_graph=self.use_cuda_graph, #undefined\n        )[\"latent\"]\n        return AutoencoderTinyOutput(latents=latents)","documentation":"The code defines a function called `encode` that takes in a tensor of images and returns a tensor of latent representations. It uses a TensorRT engine to perform the encoding, allocating memory for the input and output tensors and executing the inference using the engine's `infer` method."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#decode().","attributes":{"range":[98,4,116,43],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#decode().","content":"def decode(self, latent: torch.Tensor, **kwargs):\n        self.decoder.allocate_buffers(\n            shape_dict={\n                \"latent\": latent.shape,\n                \"images\": (\n                    latent.shape[0],\n                    3,\n                    latent.shape[2] * self.vae_scale_factor,\n                    latent.shape[3] * self.vae_scale_factor,\n                ),\n            },\n            device=latent.device,\n        )\n        images = self.decoder.infer(\n            {\"latent\": latent},\n            self.stream,\n            use_cuda_graph=self.use_cuda_graph,\n        )[\"images\"]\n        return DecoderOutput(sample=images)","file":"/src/streamdiffusion/acceleration/tensorrt/engine.py","language":"python","fileHash":"dc4f85c80de38d40817cb2185c46f634aee5ae1c4e35ed91586498131c2d2cf7","hash":"da31ba99092f7a95694e9c8370339c087e7166524abb792f24ebf60f2fb2577d","processedContent":"def decode(self, latent: torch.Tensor, **kwargs):\n        self.decoder.allocate_buffers(\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#decoder.: undefined\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#allocate_buffers().: This code allocates memory for input and output tensors in a TensorRT engine, using the shapes and data types provided by the engine.\n        \"\"\"\n            shape_dict={\n                \"latent\": latent.shape,\n                \"images\": (\n                    latent.shape[0],\n                    3,\n                    latent.shape[2] * self.vae_scale_factor, #undefined\n                    latent.shape[3] * self.vae_scale_factor, #undefined\n                ),\n            },\n            device=latent.device,\n        )\n        images = self.decoder.infer(\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#decoder.: undefined\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#infer().: This code defines a function called `infer` that takes in a feed dictionary, a stream, and an optional boolean flag indicating whether to use CUDA graph. It then sets the tensor addresses for each tensor in the feed dictionary and executes the inference using the TensorRT context. If the CUDA graph flag is set, it captures the CUDA graph and instantiates it.\n        \"\"\"\n            {\"latent\": latent},\n            self.stream, #undefined\n            use_cuda_graph=self.use_cuda_graph, #undefined\n        )[\"images\"]\n        return DecoderOutput(sample=images)","documentation":"The code defines a function called `decode` that takes in a latent tensor and allocates memory for input and output tensors in a TensorRT engine. It then executes the inference using the TensorRT context and returns the decoded images."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#to().","attributes":{"range":[118,4,119,12],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#to().","content":"def to(self, *args, **kwargs):\n        pass","file":"/src/streamdiffusion/acceleration/tensorrt/engine.py","language":"python","fileHash":"dc4f85c80de38d40817cb2185c46f634aee5ae1c4e35ed91586498131c2d2cf7","hash":"719cb34b47ef259fbda3790867172c917c9f4a598c5185ea0a81050459991e43","processedContent":"def to(self, *args, **kwargs):\n        pass","documentation":"This code defines a function called `to` that takes any number of positional arguments and keyword arguments, and does nothing with them. It is a placeholder function that can be used as a stub or a starting point for more complex functionality."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#forward().","attributes":{"range":[121,4,122,12],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#forward().","content":"def forward(self, *args, **kwargs):\n        pass","file":"/src/streamdiffusion/acceleration/tensorrt/engine.py","language":"python","fileHash":"dc4f85c80de38d40817cb2185c46f634aee5ae1c4e35ed91586498131c2d2cf7","hash":"5ec3d0795c4fe75edbd2eefc41a33dc81216d810596927a56d0689d1cb29b9eb","processedContent":"def forward(self, *args, **kwargs):\n        pass","documentation":"This code defines a function called \"forward\" that takes in any number of arguments and keyword arguments, but does not perform any actual computation. It is likely used as a placeholder or a stub for future development."}},{"key":"scip-python python temp indexer `diffusers.models.autoencoder_tiny`/__init__:","attributes":{"symbol":"scip-python python temp indexer `diffusers.models.autoencoder_tiny`/__init__:","language":"python"}},{"key":"scip-python python temp indexer `diffusers.models.unet_2d_condition`/__init__:","attributes":{"symbol":"scip-python python temp indexer `diffusers.models.unet_2d_condition`/__init__:","language":"python"}},{"key":"scip-python python temp indexer `diffusers.models.vae`/__init__:","attributes":{"symbol":"scip-python python temp indexer `diffusers.models.vae`/__init__:","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#","language":"python","range":[81,0,288,27],"content":"class Engine:\n    def __init__(\n        self,\n        engine_path,\n    ):\n        self.engine_path = engine_path\n        self.engine = None\n        self.context = None\n        self.buffers = OrderedDict()\n        self.tensors = OrderedDict()\n        self.cuda_graph_instance = None  # cuda graph\n\n    def __del__(self):\n        [buf.free() for buf in self.buffers.values() if isinstance(buf, cuda.DeviceArray)]\n        del self.engine\n        del self.context\n        del self.buffers\n        del self.tensors\n\n    def refit(self, onnx_path, onnx_refit_path):\n        def convert_int64(arr):\n            # TODO: smarter conversion\n            if len(arr.shape) == 0:\n                return np.int32(arr)\n            return arr\n\n        def add_to_map(refit_dict, name, values):\n            if name in refit_dict:\n                assert refit_dict[name] is None\n                if values.dtype == np.int64:\n                    values = convert_int64(values)\n                refit_dict[name] = values\n\n        print(f\"Refitting TensorRT engine with {onnx_refit_path} weights\")\n        refit_nodes = gs.import_onnx(onnx.load(onnx_refit_path)).toposort().nodes\n\n        # Construct mapping from weight names in refit model -> original model\n        name_map = {}\n        for n, node in enumerate(gs.import_onnx(onnx.load(onnx_path)).toposort().nodes):\n            refit_node = refit_nodes[n]\n            assert node.op == refit_node.op\n            # Constant nodes in ONNX do not have inputs but have a constant output\n            if node.op == \"Constant\":\n                name_map[refit_node.outputs[0].name] = node.outputs[0].name\n            # Handle scale and bias weights\n            elif node.op == \"Conv\":\n                if node.inputs[1].__class__ == gs.Constant:\n                    name_map[refit_node.name + \"_TRTKERNEL\"] = node.name + \"_TRTKERNEL\"\n                if node.inputs[2].__class__ == gs.Constant:\n                    name_map[refit_node.name + \"_TRTBIAS\"] = node.name + \"_TRTBIAS\"\n            # For all other nodes: find node inputs that are initializers (gs.Constant)\n            else:\n                for i, inp in enumerate(node.inputs):\n                    if inp.__class__ == gs.Constant:\n                        name_map[refit_node.inputs[i].name] = inp.name\n\n        def map_name(name):\n            if name in name_map:\n                return name_map[name]\n            return name\n\n        # Construct refit dictionary\n        refit_dict = {}\n        refitter = trt.Refitter(self.engine, TRT_LOGGER)\n        all_weights = refitter.get_all()\n        for layer_name, role in zip(all_weights[0], all_weights[1]):\n            # for speciailized roles, use a unique name in the map:\n            if role == trt.WeightsRole.KERNEL:\n                name = layer_name + \"_TRTKERNEL\"\n            elif role == trt.WeightsRole.BIAS:\n                name = layer_name + \"_TRTBIAS\"\n            else:\n                name = layer_name\n\n            assert name not in refit_dict, \"Found duplicate layer: \" + name\n            refit_dict[name] = None\n\n        for n in refit_nodes:\n            # Constant nodes in ONNX do not have inputs but have a constant output\n            if n.op == \"Constant\":\n                name = map_name(n.outputs[0].name)\n                print(f\"Add Constant {name}\\n\")\n                add_to_map(refit_dict, name, n.outputs[0].values)\n\n            # Handle scale and bias weights\n            elif n.op == \"Conv\":\n                if n.inputs[1].__class__ == gs.Constant:\n                    name = map_name(n.name + \"_TRTKERNEL\")\n                    add_to_map(refit_dict, name, n.inputs[1].values)\n\n                if n.inputs[2].__class__ == gs.Constant:\n                    name = map_name(n.name + \"_TRTBIAS\")\n                    add_to_map(refit_dict, name, n.inputs[2].values)\n\n            # For all other nodes: find node inputs that are initializers (AKA gs.Constant)\n            else:\n                for inp in n.inputs:\n                    name = map_name(inp.name)\n                    if inp.__class__ == gs.Constant:\n                        add_to_map(refit_dict, name, inp.values)\n\n        for layer_name, weights_role in zip(all_weights[0], all_weights[1]):\n            if weights_role == trt.WeightsRole.KERNEL:\n                custom_name = layer_name + \"_TRTKERNEL\"\n            elif weights_role == trt.WeightsRole.BIAS:\n                custom_name = layer_name + \"_TRTBIAS\"\n            else:\n                custom_name = layer_name\n\n            # Skip refitting Trilu for now; scalar weights of type int64 value 1 - for clip model\n            if layer_name.startswith(\"onnx::Trilu\"):\n                continue\n\n            if refit_dict[custom_name] is not None:\n                refitter.set_weights(layer_name, weights_role, refit_dict[custom_name])\n            else:\n                print(f\"[W] No refit weights for layer: {layer_name}\")\n\n        if not refitter.refit_cuda_engine():\n            print(\"Failed to refit!\")\n            exit(0)\n\n    def build(\n        self,\n        onnx_path,\n        fp16,\n        input_profile=None,\n        enable_refit=False,\n        enable_all_tactics=False,\n        timing_cache=None,\n        workspace_size=0,\n    ):\n        print(f\"Building TensorRT engine for {onnx_path}: {self.engine_path}\")\n        p = Profile()\n        if input_profile:\n            for name, dims in input_profile.items():\n                assert len(dims) == 3\n                p.add(name, min=dims[0], opt=dims[1], max=dims[2])\n\n        config_kwargs = {}\n\n        if workspace_size > 0:\n            config_kwargs[\"memory_pool_limits\"] = {trt.MemoryPoolType.WORKSPACE: workspace_size}\n        if not enable_all_tactics:\n            config_kwargs[\"tactic_sources\"] = []\n\n        engine = engine_from_network(\n            network_from_onnx_path(onnx_path, flags=[trt.OnnxParserFlag.NATIVE_INSTANCENORM]),\n            config=CreateConfig(\n                fp16=fp16, refittable=enable_refit, profiles=[p], load_timing_cache=timing_cache, **config_kwargs\n            ),\n            save_timing_cache=timing_cache,\n        )\n        save_engine(engine, path=self.engine_path)\n\n    def load(self):\n        print(f\"Loading TensorRT engine: {self.engine_path}\")\n        self.engine = engine_from_bytes(bytes_from_path(self.engine_path))\n\n    def activate(self, reuse_device_memory=None):\n        if reuse_device_memory:\n            self.context = self.engine.create_execution_context_without_device_memory()\n            self.context.device_memory = reuse_device_memory\n        else:\n            self.context = self.engine.create_execution_context()\n\n    def allocate_buffers(self, shape_dict=None, device=\"cuda\"):\n        for idx in range(trt_util.get_bindings_per_profile(self.engine)):\n            binding = self.engine[idx]\n            if shape_dict and binding in shape_dict:\n                shape = shape_dict[binding]\n            else:\n                shape = self.engine.get_binding_shape(binding)\n            dtype = trt.nptype(self.engine.get_binding_dtype(binding))\n            if self.engine.binding_is_input(binding):\n                self.context.set_binding_shape(idx, shape)\n            tensor = torch.empty(tuple(shape), dtype=numpy_to_torch_dtype_dict[dtype]).to(device=device)\n            self.tensors[binding] = tensor\n\n    def infer(self, feed_dict, stream, use_cuda_graph=False):\n        for name, buf in feed_dict.items():\n            self.tensors[name].copy_(buf)\n\n        for name, tensor in self.tensors.items():\n            self.context.set_tensor_address(name, tensor.data_ptr())\n\n        if use_cuda_graph:\n            if self.cuda_graph_instance is not None:\n                CUASSERT(cudart.cudaGraphLaunch(self.cuda_graph_instance, stream.ptr))\n                CUASSERT(cudart.cudaStreamSynchronize(stream.ptr))\n            else:\n                # do inference before CUDA graph capture\n                noerror = self.context.execute_async_v3(stream.ptr)\n                if not noerror:\n                    raise ValueError(\"ERROR: inference failed.\")\n                # capture cuda graph\n                CUASSERT(\n                    cudart.cudaStreamBeginCapture(stream.ptr, cudart.cudaStreamCaptureMode.cudaStreamCaptureModeGlobal)\n                )\n                self.context.execute_async_v3(stream.ptr)\n                self.graph = CUASSERT(cudart.cudaStreamEndCapture(stream.ptr))\n                self.cuda_graph_instance = CUASSERT(cudart.cudaGraphInstantiate(self.graph, 0))\n        else:\n            noerror = self.context.execute_async_v3(stream.ptr)\n            if not noerror:\n                raise ValueError(\"ERROR: inference failed.\")\n\n        return self.tensors","file":"/src/streamdiffusion/acceleration/tensorrt/utilities.py","fileHash":"63a6abfeb186705edc5dba1605e60b5eaa1a86e848db1ddcffe555e16476075c","hash":"77909d50063c68d146315a6856d508dc1494aa048c796c5e8a608b90d674601f","processedContent":"class Engine:\n    def __init__(\n        \"\"\"This code defines a class that initializes an object with various attributes, including engine_path, engine, context, buffers, tensors, and cuda_graph_instance.\"\"\"\n        pass\n\n    def __del__(self):\n        \"\"\"This code is a destructor method for an object that frees memory and deletes references to other objects when the object is no longer needed.\"\"\"\n        pass\n\n    def refit(self, onnx_path, onnx_refit_path):\n        \"\"\"The code is a Python script that refits a TensorRT engine with new weights from an ONNX model. It maps the original weights to the refit engine and sets the new weights for each layer based on the mapping.\"\"\"\n        pass\n\n    def build(\n        \"\"\"This code builds a TensorRT engine from an ONNX model, allowing for customization of the engine's performance through various parameters.\"\"\"\n        pass\n\n    def load(self):\n        \"\"\"This code loads a TensorRT engine from a file path and stores it in the `engine` variable.\"\"\"\n        pass\n\n    def activate(self, reuse_device_memory=None):\n        \"\"\"This code creates an execution context for a TensorRT engine, which is used to perform inference on a neural network. The context can be reused by passing in a device memory object, which allows the engine to use the same memory for future inferences.\"\"\"\n        pass\n\n    def allocate_buffers(self, shape_dict=None, device=\"cuda\"):\n        \"\"\"This code allocates memory for input and output tensors in a TensorRT engine, using the shapes and data types provided by the engine.\"\"\"\n        pass\n\n    def infer(self, feed_dict, stream, use_cuda_graph=False):\n        for name, buf in feed_dict.items():\n            self.tensors[name].copy_(buf)\n\n        for name, tensor in self.tensors.items():\n            self.context.set_tensor_address(name, tensor.data_ptr())\n\n        if use_cuda_graph:\n            if self.cuda_graph_instance is not None:\n                CUASSERT(cudart.cudaGraphLaunch(self.cuda_graph_instance, stream.ptr))\n                CUASSERT(cudart.cudaStreamSynchronize(stream.ptr))\n            else:\n                # do inference before CUDA graph capture\n                noerror = self.context.execute_async_v3(stream.ptr)\n                if not noerror:\n                    raise ValueError(\"ERROR: inference failed.\")\n                # capture cuda graph\n                CUASSERT(\n                    cudart.cudaStreamBeginCapture(stream.ptr, cudart.cudaStreamCaptureMode.cudaStreamCaptureModeGlobal)\n                )\n                self.context.execute_async_v3(stream.ptr)\n                self.graph = CUASSERT(cudart.cudaStreamEndCapture(stream.ptr))\n                self.cuda_graph_instance = CUASSERT(cudart.cudaGraphInstantiate(self.graph, 0))\n        else:\n            noerror = self.context.execute_async_v3(stream.ptr)\n            if not noerror:\n                raise ValueError(\"ERROR: inference failed.\")\n\n        return self.tensors","documentation":"The code defines a class called Engine that initializes an object with various attributes and methods for building, loading, and executing a TensorRT engine."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#engine.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#engine.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#load().","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#load().","language":"python","range":[236,4,238,74],"content":"def load(self):\n        print(f\"Loading TensorRT engine: {self.engine_path}\")\n        self.engine = engine_from_bytes(bytes_from_path(self.engine_path))","file":"/src/streamdiffusion/acceleration/tensorrt/utilities.py","fileHash":"63a6abfeb186705edc5dba1605e60b5eaa1a86e848db1ddcffe555e16476075c","hash":"aa8b6c3aa32bed70d72d324cf4036ecdcbfd0ab782e710b9404fe59d9ced948b","processedContent":"def load(self):\n        print(f\"Loading TensorRT engine: {self.engine_path}\") #undefined\n        self.engine = engine_from_bytes(bytes_from_path(self.engine_path))","documentation":"This code loads a TensorRT engine from a file path and stores it in the `engine` variable."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#activate().","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#activate().","language":"python","range":[240,4,245,65],"content":"def activate(self, reuse_device_memory=None):\n        if reuse_device_memory:\n            self.context = self.engine.create_execution_context_without_device_memory()\n            self.context.device_memory = reuse_device_memory\n        else:\n            self.context = self.engine.create_execution_context()","file":"/src/streamdiffusion/acceleration/tensorrt/utilities.py","fileHash":"63a6abfeb186705edc5dba1605e60b5eaa1a86e848db1ddcffe555e16476075c","hash":"fcec9bbae4a299393c4ceead7a5aeda78f45dce60b70712d019f1e5cde2a367e","processedContent":"def activate(self, reuse_device_memory=None):\n        if reuse_device_memory:\n            self.context = self.engine.create_execution_context_without_device_memory()\n            \"\"\"\n            scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#context.: undefined\n            scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#engine.: undefined\n            \"\"\"\n            self.context.device_memory = reuse_device_memory #undefined\n        else:\n            self.context = self.engine.create_execution_context()","documentation":"This code creates an execution context for a TensorRT engine, which is used to perform inference on a neural network. The context can be reused by passing in a device memory object, which allows the engine to use the same memory for future inferences."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#allocate_buffers().","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#allocate_buffers().","language":"python","range":[247,4,258,42],"content":"def allocate_buffers(self, shape_dict=None, device=\"cuda\"):\n        for idx in range(trt_util.get_bindings_per_profile(self.engine)):\n            binding = self.engine[idx]\n            if shape_dict and binding in shape_dict:\n                shape = shape_dict[binding]\n            else:\n                shape = self.engine.get_binding_shape(binding)\n            dtype = trt.nptype(self.engine.get_binding_dtype(binding))\n            if self.engine.binding_is_input(binding):\n                self.context.set_binding_shape(idx, shape)\n            tensor = torch.empty(tuple(shape), dtype=numpy_to_torch_dtype_dict[dtype]).to(device=device)\n            self.tensors[binding] = tensor","file":"/src/streamdiffusion/acceleration/tensorrt/utilities.py","fileHash":"63a6abfeb186705edc5dba1605e60b5eaa1a86e848db1ddcffe555e16476075c","hash":"3e7c163f21262278a2e4128c035e453ea770dd7f9556132adb6ad7a204c20410","processedContent":"def allocate_buffers(self, shape_dict=None, device=\"cuda\"):\n        for idx in range(trt_util.get_bindings_per_profile(self.engine)): #undefined\n            binding = self.engine[idx] #undefined\n            if shape_dict and binding in shape_dict:\n                shape = shape_dict[binding]\n            else:\n                shape = self.engine.get_binding_shape(binding) #undefined\n            dtype = trt.nptype(self.engine.get_binding_dtype(binding)) #undefined\n            if self.engine.binding_is_input(binding): #undefined\n                self.context.set_binding_shape(idx, shape) #undefined\n            tensor = torch.empty(tuple(shape), dtype=numpy_to_torch_dtype_dict[dtype]).to(device=device) #undefined\n            self.tensors[binding] = tensor","documentation":"This code allocates memory for input and output tensors in a TensorRT engine, using the shapes and data types provided by the engine."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#infer().","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#infer().","language":"python","range":[260,4,288,27],"content":"def infer(self, feed_dict, stream, use_cuda_graph=False):\n        for name, buf in feed_dict.items():\n            self.tensors[name].copy_(buf)\n\n        for name, tensor in self.tensors.items():\n            self.context.set_tensor_address(name, tensor.data_ptr())\n\n        if use_cuda_graph:\n            if self.cuda_graph_instance is not None:\n                CUASSERT(cudart.cudaGraphLaunch(self.cuda_graph_instance, stream.ptr))\n                CUASSERT(cudart.cudaStreamSynchronize(stream.ptr))\n            else:\n                # do inference before CUDA graph capture\n                noerror = self.context.execute_async_v3(stream.ptr)\n                if not noerror:\n                    raise ValueError(\"ERROR: inference failed.\")\n                # capture cuda graph\n                CUASSERT(\n                    cudart.cudaStreamBeginCapture(stream.ptr, cudart.cudaStreamCaptureMode.cudaStreamCaptureModeGlobal)\n                )\n                self.context.execute_async_v3(stream.ptr)\n                self.graph = CUASSERT(cudart.cudaStreamEndCapture(stream.ptr))\n                self.cuda_graph_instance = CUASSERT(cudart.cudaGraphInstantiate(self.graph, 0))\n        else:\n            noerror = self.context.execute_async_v3(stream.ptr)\n            if not noerror:\n                raise ValueError(\"ERROR: inference failed.\")\n\n        return self.tensors","file":"/src/streamdiffusion/acceleration/tensorrt/utilities.py","fileHash":"63a6abfeb186705edc5dba1605e60b5eaa1a86e848db1ddcffe555e16476075c","hash":"e498d94a7152990d975c83bbac52a030e427bc84a9d1e76491d5a60736f0460d","processedContent":"def infer(self, feed_dict, stream, use_cuda_graph=False):\n        for name, buf in feed_dict.items():\n            self.tensors[name].copy_(buf) #undefined\n\n        for name, tensor in self.tensors.items():\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#tensors.: undefined\n        scip-python python python-stdlib 3.11 collections/OrderedDict#items().: undefined\n        \"\"\"\n            self.context.set_tensor_address(name, tensor.data_ptr()) #undefined\n\n        if use_cuda_graph:\n            if self.cuda_graph_instance is not None: #undefined\n                CUASSERT(cudart.cudaGraphLaunch(self.cuda_graph_instance, stream.ptr))\n                \"\"\"\n                scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#cuda_graph_instance.: undefined\n                scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/CUASSERT().: This code defines a function called CUASSERT that checks the return value of a CUDA call and raises an error if it fails. It also returns the result of the CUDA call if it succeeds.\n                \"\"\"\n                CUASSERT(cudart.cudaStreamSynchronize(stream.ptr)) #This code defines a function called CUASSERT that checks the return value of a CUDA call and raises an error if it fails. It also returns the result of the CUDA call if it succeeds.\n            else:\n                # do inference before CUDA graph capture\n                noerror = self.context.execute_async_v3(stream.ptr) #undefined\n                if not noerror:\n                    raise ValueError(\"ERROR: inference failed.\")\n                # capture cuda graph\n                CUASSERT( #This code defines a function called CUASSERT that checks the return value of a CUDA call and raises an error if it fails. It also returns the result of the CUDA call if it succeeds.\n                    cudart.cudaStreamBeginCapture(stream.ptr, cudart.cudaStreamCaptureMode.cudaStreamCaptureModeGlobal)\n                )\n                self.context.execute_async_v3(stream.ptr) #undefined\n                self.graph = CUASSERT(cudart.cudaStreamEndCapture(stream.ptr)) #This code defines a function called CUASSERT that checks the return value of a CUDA call and raises an error if it fails. It also returns the result of the CUDA call if it succeeds.\n                self.cuda_graph_instance = CUASSERT(cudart.cudaGraphInstantiate(self.graph, 0))\n                \"\"\"\n                scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#cuda_graph_instance.: undefined\n                scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/CUASSERT().: This code defines a function called CUASSERT that checks the return value of a CUDA call and raises an error if it fails. It also returns the result of the CUDA call if it succeeds.\n                scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#graph.: undefined\n                \"\"\"\n        else:\n            noerror = self.context.execute_async_v3(stream.ptr)\n            if not noerror:\n                raise ValueError(\"ERROR: inference failed.\")\n\n        return self.tensors","documentation":"This code defines a function called `infer` that takes in a feed dictionary, a stream, and an optional boolean flag indicating whether to use CUDA graph. It then sets the tensor addresses for each tensor in the feed dictionary and executes the inference using the TensorRT context. If the CUDA graph flag is set, it captures the CUDA graph and instantiates it."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#stream.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#stream.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#use_cuda_graph.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#use_cuda_graph.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#encoder.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#encoder.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#decoder.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#decoder.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#vae_scale_factor.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#vae_scale_factor.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#stream.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#stream.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#use_cuda_graph.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#use_cuda_graph.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#","attributes":{"range":[25,0,62,29],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#","content":"class Optimizer:\n    def __init__(self, onnx_graph, verbose=False):\n        self.graph = gs.import_onnx(onnx_graph)\n        self.verbose = verbose\n\n    def info(self, prefix):\n        if self.verbose:\n            print(\n                f\"{prefix} .. {len(self.graph.nodes)} nodes, {len(self.graph.tensors().keys())} tensors, {len(self.graph.inputs)} inputs, {len(self.graph.outputs)} outputs\"\n            )\n\n    def cleanup(self, return_onnx=False):\n        self.graph.cleanup().toposort()\n        if return_onnx:\n            return gs.export_onnx(self.graph)\n\n    def select_outputs(self, keep, names=None):\n        self.graph.outputs = [self.graph.outputs[o] for o in keep]\n        if names:\n            for i, name in enumerate(names):\n                self.graph.outputs[i].name = name\n\n    def fold_constants(self, return_onnx=False):\n        onnx_graph = fold_constants(gs.export_onnx(self.graph), allow_onnxruntime_shape_inference=True)\n        self.graph = gs.import_onnx(onnx_graph)\n        if return_onnx:\n            return onnx_graph\n\n    def infer_shapes(self, return_onnx=False):\n        onnx_graph = gs.export_onnx(self.graph)\n        if onnx_graph.ByteSize() > 2147483648:\n            raise TypeError(\"ERROR: model size exceeds supported 2GB limit\")\n        else:\n            onnx_graph = shape_inference.infer_shapes(onnx_graph)\n\n        self.graph = gs.import_onnx(onnx_graph)\n        if return_onnx:\n            return onnx_graph","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"b5e20fd768304896480150a61f99c0323fa0d08fef80ee40188445358649bfd7","processedContent":"class Optimizer:\n    def __init__(self, onnx_graph, verbose=False):\n        \"\"\"This code initializes a new instance of the class, which imports an ONNX graph and sets a flag for verbose output.\"\"\"\n        pass\n\n    def info(self, prefix):\n        \"\"\"This code defines a function called `info` that prints information about the graph, including the number of nodes, tensors, inputs, and outputs.\"\"\"\n        pass\n\n    def cleanup(self, return_onnx=False):\n        \"\"\"The code cleans up the graph and sorts it in a topological order. If return_onnx is set to True, it exports the graph as an ONNX model.\"\"\"\n        pass\n\n    def select_outputs(self, keep, names=None):\n        \"\"\"This code is a function that takes two arguments, \"keep\" and \"names\". It modifies the graph's outputs by selecting only the ones specified in the \"keep\" argument, and then assigns names to the remaining outputs if the \"names\" argument is provided.\"\"\"\n        pass\n\n    def fold_constants(self, return_onnx=False):\n        \"\"\"This code folds constants in a graph, which is a technique used to optimize the performance of machine learning models. It exports the graph as an ONNX format and then imports it back into the model, which helps to reduce the computational complexity of the model.\"\"\"\n        pass\n\n    def infer_shapes(self, return_onnx=False):\n        onnx_graph = gs.export_onnx(self.graph)\n        if onnx_graph.ByteSize() > 2147483648:\n            raise TypeError(\"ERROR: model size exceeds supported 2GB limit\")\n        else:\n            onnx_graph = shape_inference.infer_shapes(onnx_graph)\n\n        self.graph = gs.import_onnx(onnx_graph)\n        if return_onnx:\n            return onnx_graph","documentation":"This code defines a class called `Optimizer` that optimizes an ONNX graph by cleaning it up, selecting outputs, folding constants, and inferring shapes. It also exports the optimized graph as an ONNX model if requested."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#__init__().","attributes":{"range":[26,4,28,30],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#__init__().","content":"def __init__(self, onnx_graph, verbose=False):\n        self.graph = gs.import_onnx(onnx_graph)\n        self.verbose = verbose","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"c3d8c8e57dd771e6975bc7fa29f110cc69c237fe8d949ad060f9aa898bfb1574","processedContent":"def __init__(self, onnx_graph, verbose=False):\n        self.graph = gs.import_onnx(onnx_graph)\n        self.verbose = verbose","documentation":"This code initializes a new instance of the class, which imports an ONNX graph and sets a flag for verbose output."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#info().","attributes":{"range":[30,4,34,13],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#info().","content":"def info(self, prefix):\n        if self.verbose:\n            print(\n                f\"{prefix} .. {len(self.graph.nodes)} nodes, {len(self.graph.tensors().keys())} tensors, {len(self.graph.inputs)} inputs, {len(self.graph.outputs)} outputs\"\n            )","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"292bf0ed8eb9fe46e62725710bddadc2b961b82648475a44a06b67dc4420b151","processedContent":"def info(self, prefix):\n        if self.verbose: #undefined\n            print(\n                f\"{prefix} .. {len(self.graph.nodes)} nodes, {len(self.graph.tensors().keys())} tensors, {len(self.graph.inputs)} inputs, {len(self.graph.outputs)} outputs\"\n                \"\"\"\n                scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#graph.: undefined\n                scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#graph.: undefined\n                scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#graph.: undefined\n                scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#graph.: undefined\n                \"\"\"\n            )","documentation":"This code defines a function called `info` that prints information about the graph, including the number of nodes, tensors, inputs, and outputs."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#cleanup().","attributes":{"range":[36,4,39,45],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#cleanup().","content":"def cleanup(self, return_onnx=False):\n        self.graph.cleanup().toposort()\n        if return_onnx:\n            return gs.export_onnx(self.graph)","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"748073a91c7890ed96e39bfb868ce0fc48e78b383c6aaf21d5a46b0b857d4e1b","processedContent":"def cleanup(self, return_onnx=False):\n        self.graph.cleanup().toposort() #undefined\n        if return_onnx:\n            return gs.export_onnx(self.graph)","documentation":"The code cleans up the graph and sorts it in a topological order. If return_onnx is set to True, it exports the graph as an ONNX model."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#select_outputs().","attributes":{"range":[41,4,45,49],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#select_outputs().","content":"def select_outputs(self, keep, names=None):\n        self.graph.outputs = [self.graph.outputs[o] for o in keep]\n        if names:\n            for i, name in enumerate(names):\n                self.graph.outputs[i].name = name","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"13629fe26d8e7f25ee98c51cfaacec276603e17c8156cedce3c811c1ebd85d03","processedContent":"def select_outputs(self, keep, names=None):\n        self.graph.outputs = [self.graph.outputs[o] for o in keep]\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#graph.: undefined\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#graph.: undefined\n        \"\"\"\n        if names:\n            for i, name in enumerate(names):\n                self.graph.outputs[i].name = name","documentation":"This code is a function that takes two arguments, \"keep\" and \"names\". It modifies the graph's outputs by selecting only the ones specified in the \"keep\" argument, and then assigns names to the remaining outputs if the \"names\" argument is provided."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#fold_constants().","attributes":{"range":[47,4,51,29],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#fold_constants().","content":"def fold_constants(self, return_onnx=False):\n        onnx_graph = fold_constants(gs.export_onnx(self.graph), allow_onnxruntime_shape_inference=True)\n        self.graph = gs.import_onnx(onnx_graph)\n        if return_onnx:\n            return onnx_graph","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"5033cd8d4cbe54cc8937413aeaa6a32eae22eb390c9e633d2172cf20cc857028","processedContent":"def fold_constants(self, return_onnx=False):\n        onnx_graph = fold_constants(gs.export_onnx(self.graph), allow_onnxruntime_shape_inference=True) #undefined\n        self.graph = gs.import_onnx(onnx_graph) #undefined\n        if return_onnx:\n            return onnx_graph","documentation":"This code folds constants in a graph, which is a technique used to optimize the performance of machine learning models. It exports the graph as an ONNX format and then imports it back into the model, which helps to reduce the computational complexity of the model."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#infer_shapes().","attributes":{"range":[53,4,62,29],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#infer_shapes().","content":"def infer_shapes(self, return_onnx=False):\n        onnx_graph = gs.export_onnx(self.graph)\n        if onnx_graph.ByteSize() > 2147483648:\n            raise TypeError(\"ERROR: model size exceeds supported 2GB limit\")\n        else:\n            onnx_graph = shape_inference.infer_shapes(onnx_graph)\n\n        self.graph = gs.import_onnx(onnx_graph)\n        if return_onnx:\n            return onnx_graph","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"8deac6efb0572508ae951b9474f23192dc584f9998123edfd1af88975ee20c38","processedContent":"def infer_shapes(self, return_onnx=False):\n        onnx_graph = gs.export_onnx(self.graph) #undefined\n        if onnx_graph.ByteSize() > 2147483648:\n            raise TypeError(\"ERROR: model size exceeds supported 2GB limit\")\n        else:\n            onnx_graph = shape_inference.infer_shapes(onnx_graph)\n\n        self.graph = gs.import_onnx(onnx_graph) #undefined\n        if return_onnx:\n            return onnx_graph","documentation":"This code is a method that infers the shapes of a graph, which is an important step in deploying machine learning models. It exports the graph to ONNX format, checks the size of the model, and then imports it back into the graph. The method returns the ONNX graph if requested."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#__init__().","attributes":{"range":[66,4,89,38],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#__init__().","content":"def __init__(\n        self,\n        fp16=False,\n        device=\"cuda\",\n        verbose=True,\n        max_batch_size=16,\n        min_batch_size=1,\n        embedding_dim=768,\n        text_maxlen=77,\n    ):\n        self.name = \"SD Model\"\n        self.fp16 = fp16\n        self.device = device\n        self.verbose = verbose\n\n        self.min_batch = min_batch_size\n        self.max_batch = max_batch_size\n        self.min_image_shape = 256  # min image resolution: 256x256\n        self.max_image_shape = 1024  # max image resolution: 1024x1024\n        self.min_latent_shape = self.min_image_shape // 8\n        self.max_latent_shape = self.max_image_shape // 8\n\n        self.embedding_dim = embedding_dim\n        self.text_maxlen = text_maxlen","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"de375fe087b24de4afbda1a657599bfa08916d71d5eb473caab95d5a8b4c6d3a","processedContent":"def __init__(\n        self,\n        fp16=False,\n        device=\"cuda\",\n        verbose=True,\n        max_batch_size=16,\n        min_batch_size=1,\n        embedding_dim=768,\n        text_maxlen=77,\n    ):\n        self.name = \"SD Model\"\n        self.fp16 = fp16\n        self.device = device\n        self.verbose = verbose\n\n        self.min_batch = min_batch_size\n        self.max_batch = max_batch_size\n        self.min_image_shape = 256  # min image resolution: 256x256\n        self.max_image_shape = 1024  # max image resolution: 1024x1024\n        self.min_latent_shape = self.min_image_shape // 8 #undefined\n        self.max_latent_shape = self.max_image_shape // 8 #undefined\n\n        self.embedding_dim = embedding_dim\n        self.text_maxlen = text_maxlen","documentation":"This code defines a class called \"SD Model\" that is used for image-text matching. It takes in various parameters, such as the device to use, whether to use fp16 precision, and the maximum and minimum batch sizes. The model also has attributes for the embedding dimension, text max length, and the minimum and maximum latent shapes."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_model().","attributes":{"range":[91,4,92,12],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_model().","content":"def get_model(self):\n        pass","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"86138a6fe6708ef12ff2ffb93b4ac1cb0f505e0f5b26d18b63113821a0aa47ef","processedContent":"def get_model(self):\n        pass","documentation":"This code defines a function called `get_model` that does nothing. It is an empty function, meaning it does not perform any actions or return any values."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_input_names().","attributes":{"range":[94,4,95,12],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_input_names().","content":"def get_input_names(self):\n        pass","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"16cd9cea2b143af745de59ecff1e48c493bf19387f959fc816fa6843dee09574","processedContent":"def get_input_names(self):\n        pass","documentation":"This code defines a function called `get_input_names` that retrieves the names of the input fields in a form."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_output_names().","attributes":{"range":[97,4,98,12],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_output_names().","content":"def get_output_names(self):\n        pass","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"1193b42c39ce106691f11c49f6689b62be5c8b2139ce1a27c5e25c6825db4ede","processedContent":"def get_output_names(self):\n        pass","documentation":"This code defines a function called `get_output_names` that returns a list of output names for the current model."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_dynamic_axes().","attributes":{"range":[100,4,101,19],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_dynamic_axes().","content":"def get_dynamic_axes(self):\n        return None","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"debbaa0c5774c95a62cf76b1ff1721cd5ddf98c8e937d1bd0d3dc48c7251021e","processedContent":"def get_dynamic_axes(self):\n        return None","documentation":"This code defines a function called `get_dynamic_axes` that returns `None`. It is used to dynamically determine the axes of a graph or chart based on the data being plotted."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_sample_input().","attributes":{"range":[103,4,104,12],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_sample_input().","content":"def get_sample_input(self, batch_size, image_height, image_width):\n        pass","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"828b08a1af674e818683e45bac58da89b60778f61549f0517be6fb7fae581813","processedContent":"def get_sample_input(self, batch_size, image_height, image_width):\n        pass","documentation":"This code defines a function called `get_sample_input` that generates a batch of input data for training a machine learning model. The function takes three arguments: `batch_size`, `image_height`, and `image_width`. It returns a tensor with the specified dimensions."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_input_profile().","attributes":{"range":[106,4,107,19],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_input_profile().","content":"def get_input_profile(self, batch_size, image_height, image_width, static_batch, static_shape):\n        return None","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"ac6ee11bb44739bd35b4a03776a336375bb92ac9ef718893a7548318a5411220","processedContent":"def get_input_profile(self, batch_size, image_height, image_width, static_batch, static_shape):\n        return None","documentation":"This code defines a function called `get_input_profile` that returns a set of input parameters for a machine learning model. The function takes in five arguments: `batch_size`, `image_height`, `image_width`, `static_batch`, and `static_shape`. These parameters are used to determine the shape of the input data and the batch size of the model."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_shape_dict().","attributes":{"range":[109,4,110,19],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_shape_dict().","content":"def get_shape_dict(self, batch_size, image_height, image_width):\n        return None","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"a63ada964b012cbfc932ba0ff74fa99cfacc41d436e997b6966b2991c9243940","processedContent":"def get_shape_dict(self, batch_size, image_height, image_width):\n        return None","documentation":"This code defines a function called `get_shape_dict` that takes three arguments: `batch_size`, `image_height`, and `image_width`. It returns a dictionary with the shape of the input data."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#optimize().","attributes":{"range":[112,4,123,29],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#optimize().","content":"def optimize(self, onnx_graph):\n        opt = Optimizer(onnx_graph, verbose=self.verbose)\n        opt.info(self.name + \": original\")\n        opt.cleanup()\n        opt.info(self.name + \": cleanup\")\n        opt.fold_constants()\n        opt.info(self.name + \": fold constants\")\n        opt.infer_shapes()\n        opt.info(self.name + \": shape inference\")\n        onnx_opt_graph = opt.cleanup(return_onnx=True)\n        opt.info(self.name + \": finished\")\n        return onnx_opt_graph","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"d19bd23e80ab9eca8749a73b147bef88c7fc719d4516afb2f1e19607f99a2cb6","processedContent":"def optimize(self, onnx_graph):\n        opt = Optimizer(onnx_graph, verbose=self.verbose)\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#: This code defines a class called `Optimizer` that optimizes an ONNX graph by cleaning it up, selecting outputs, folding constants, and inferring shapes. It also exports the optimized graph as an ONNX model if requested.\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#verbose.: undefined\n        \"\"\"\n        opt.info(self.name + \": original\")\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#info().: This code defines a function called `info` that prints information about the graph, including the number of nodes, tensors, inputs, and outputs.\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#name.: undefined\n        \"\"\"\n        opt.cleanup() #The code cleans up the graph and sorts it in a topological order. If return_onnx is set to True, it exports the graph as an ONNX model.\n        opt.info(self.name + \": cleanup\")\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#info().: This code defines a function called `info` that prints information about the graph, including the number of nodes, tensors, inputs, and outputs.\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#name.: undefined\n        \"\"\"\n        opt.fold_constants() #This code folds constants in a graph, which is a technique used to optimize the performance of machine learning models. It exports the graph as an ONNX format and then imports it back into the model, which helps to reduce the computational complexity of the model.\n        opt.info(self.name + \": fold constants\")\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#info().: This code defines a function called `info` that prints information about the graph, including the number of nodes, tensors, inputs, and outputs.\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#name.: undefined\n        \"\"\"\n        opt.infer_shapes() #This code is a method that infers the shapes of a graph, which is an important step in deploying machine learning models. It exports the graph to ONNX format, checks the size of the model, and then imports it back into the graph. The method returns the ONNX graph if requested.\n        opt.info(self.name + \": shape inference\")\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#info().: This code defines a function called `info` that prints information about the graph, including the number of nodes, tensors, inputs, and outputs.\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#name.: undefined\n        \"\"\"\n        onnx_opt_graph = opt.cleanup(return_onnx=True) #The code cleans up the graph and sorts it in a topological order. If return_onnx is set to True, it exports the graph as an ONNX model.\n        opt.info(self.name + \": finished\")\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#info().: This code defines a function called `info` that prints information about the graph, including the number of nodes, tensors, inputs, and outputs.\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#name.: undefined\n        \"\"\"\n        return onnx_opt_graph","documentation":"The code defines a class called `Optimizer` that optimizes an ONNX graph by cleaning it up, selecting outputs, folding constants, and inferring shapes. It also exports the optimized graph as an ONNX model if requested."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#check_dims().","attributes":{"range":[125,4,132,44],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#check_dims().","content":"def check_dims(self, batch_size, image_height, image_width):\n        assert batch_size >= self.min_batch and batch_size <= self.max_batch\n        assert image_height % 8 == 0 or image_width % 8 == 0\n        latent_height = image_height // 8\n        latent_width = image_width // 8\n        assert latent_height >= self.min_latent_shape and latent_height <= self.max_latent_shape\n        assert latent_width >= self.min_latent_shape and latent_width <= self.max_latent_shape\n        return (latent_height, latent_width)","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"2ccbb44bcb95c6b19aa5468846d38633ff8c7b41cd0cb4abc16c4ca3d165d73e","processedContent":"def check_dims(self, batch_size, image_height, image_width):\n        assert batch_size >= self.min_batch and batch_size <= self.max_batch\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#min_batch.: undefined\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#max_batch.: undefined\n        \"\"\"\n        assert image_height % 8 == 0 or image_width % 8 == 0\n        latent_height = image_height // 8\n        latent_width = image_width // 8\n        assert latent_height >= self.min_latent_shape and latent_height <= self.max_latent_shape\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#min_latent_shape.: undefined\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#max_latent_shape.: undefined\n        \"\"\"\n        assert latent_width >= self.min_latent_shape and latent_width <= self.max_latent_shape\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#min_latent_shape.: undefined\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#max_latent_shape.: undefined\n        \"\"\"\n        return (latent_height, latent_width)","documentation":"This code checks the dimensions of an image and returns the latent height and width based on certain business rules."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_minmax_dims().","attributes":{"range":[134,4,158,9],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_minmax_dims().","content":"def get_minmax_dims(self, batch_size, image_height, image_width, static_batch, static_shape):\n        min_batch = batch_size if static_batch else self.min_batch\n        max_batch = batch_size if static_batch else self.max_batch\n        latent_height = image_height // 8\n        latent_width = image_width // 8\n        min_image_height = image_height if static_shape else self.min_image_shape\n        max_image_height = image_height if static_shape else self.max_image_shape\n        min_image_width = image_width if static_shape else self.min_image_shape\n        max_image_width = image_width if static_shape else self.max_image_shape\n        min_latent_height = latent_height if static_shape else self.min_latent_shape\n        max_latent_height = latent_height if static_shape else self.max_latent_shape\n        min_latent_width = latent_width if static_shape else self.min_latent_shape\n        max_latent_width = latent_width if static_shape else self.max_latent_shape\n        return (\n            min_batch,\n            max_batch,\n            min_image_height,\n            max_image_height,\n            min_image_width,\n            max_image_width,\n            min_latent_height,\n            max_latent_height,\n            min_latent_width,\n            max_latent_width,\n        )","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"dd54aa81d01f67e35a89d46edfef50b7be842473c6fb59663ab9dbd69537e364","processedContent":"def get_minmax_dims(self, batch_size, image_height, image_width, static_batch, static_shape):\n        min_batch = batch_size if static_batch else self.min_batch #undefined\n        max_batch = batch_size if static_batch else self.max_batch #undefined\n        latent_height = image_height // 8\n        latent_width = image_width // 8\n        min_image_height = image_height if static_shape else self.min_image_shape #undefined\n        max_image_height = image_height if static_shape else self.max_image_shape #undefined\n        min_image_width = image_width if static_shape else self.min_image_shape #undefined\n        max_image_width = image_width if static_shape else self.max_image_shape #undefined\n        min_latent_height = latent_height if static_shape else self.min_latent_shape #undefined\n        max_latent_height = latent_height if static_shape else self.max_latent_shape #undefined\n        min_latent_width = latent_width if static_shape else self.min_latent_shape #undefined\n        max_latent_width = latent_width if static_shape else self.max_latent_shape #undefined\n        return (\n            min_batch,\n            max_batch,\n            min_image_height,\n            max_image_height,\n            min_image_width,\n            max_image_width,\n            min_latent_height,\n            max_latent_height,\n            min_latent_width,\n            max_latent_width,\n        )","documentation":"This code defines a function that returns the minimum and maximum values for various dimensions in an image processing pipeline. It takes into account the batch size, image height and width, and whether the shapes are static or dynamic."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#","attributes":{"range":[161,0,218,29],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#","content":"class CLIP(BaseModel):\n    def __init__(self, device, max_batch_size, embedding_dim, min_batch_size=1):\n        super(CLIP, self).__init__(\n            device=device,\n            max_batch_size=max_batch_size,\n            min_batch_size=min_batch_size,\n            embedding_dim=embedding_dim,\n        )\n        self.name = \"CLIP\"\n\n    def get_input_names(self):\n        return [\"input_ids\"]\n\n    def get_output_names(self):\n        return [\"text_embeddings\", \"pooler_output\"]\n\n    def get_dynamic_axes(self):\n        return {\"input_ids\": {0: \"B\"}, \"text_embeddings\": {0: \"B\"}}\n\n    def get_input_profile(self, batch_size, image_height, image_width, static_batch, static_shape):\n        self.check_dims(batch_size, image_height, image_width)\n        min_batch, max_batch, _, _, _, _, _, _, _, _ = self.get_minmax_dims(\n            batch_size, image_height, image_width, static_batch, static_shape\n        )\n        return {\n            \"input_ids\": [\n                (min_batch, self.text_maxlen),\n                (batch_size, self.text_maxlen),\n                (max_batch, self.text_maxlen),\n            ]\n        }\n\n    def get_shape_dict(self, batch_size, image_height, image_width):\n        self.check_dims(batch_size, image_height, image_width)\n        return {\n            \"input_ids\": (batch_size, self.text_maxlen),\n            \"text_embeddings\": (batch_size, self.text_maxlen, self.embedding_dim),\n        }\n\n    def get_sample_input(self, batch_size, image_height, image_width):\n        self.check_dims(batch_size, image_height, image_width)\n        return torch.zeros(batch_size, self.text_maxlen, dtype=torch.int32, device=self.device)\n\n    def optimize(self, onnx_graph):\n        opt = Optimizer(onnx_graph)\n        opt.info(self.name + \": original\")\n        opt.select_outputs([0])  # delete graph output#1\n        opt.cleanup()\n        opt.info(self.name + \": remove output[1]\")\n        opt.fold_constants()\n        opt.info(self.name + \": fold constants\")\n        opt.infer_shapes()\n        opt.info(self.name + \": shape inference\")\n        opt.select_outputs([0], names=[\"text_embeddings\"])  # rename network output\n        opt.info(self.name + \": remove output[0]\")\n        opt_onnx_graph = opt.cleanup(return_onnx=True)\n        opt.info(self.name + \": finished\")\n        return opt_onnx_graph","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"a68fdb52eaf3e27eab3be13fe01a59b199c087d76aa6879643ce345c421eac23","processedContent":"class CLIP(BaseModel): #This code defines a class called \"SD Model\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested.\n    def __init__(self, device, max_batch_size, embedding_dim, min_batch_size=1):\n        \"\"\"This code defines a class called \"SD Model\" that is used for image-text matching. It takes in various parameters, such as the device to use, whether to use fp16 precision, and the maximum and minimum batch sizes.\"\"\"\n        pass\n\n    def get_input_names(self):\n        \"\"\"This code defines a function called `get_input_names` that returns a list of input names.\"\"\"\n        pass\n\n    def get_output_names(self):\n        \"\"\"This code defines a function called `get_output_names` that returns a list of two strings: \"text_embeddings\" and \"pooler_output\".\"\"\"\n        pass\n\n    def get_dynamic_axes(self):\n        \"\"\"This code defines a function that returns a dictionary with two keys, \"input_ids\" and \"text_embeddings\", where the values are both dictionaries with a single key-value pair where the key is 0 and the value is \"B\". The purpose of this function is to provide dynamic axes for a machine learning model.\"\"\"\n        pass\n\n    def get_input_profile(self, batch_size, image_height, image_width, static_batch, static_shape):\n        \"\"\"This code defines a function that returns the input profile for an image processing pipeline. It checks the dimensions of an image and returns the latent height and width based on certain business rules.\"\"\"\n        pass\n\n    def get_shape_dict(self, batch_size, image_height, image_width):\n        \"\"\"This code defines a function called `get_shape_dict` that returns a dictionary with the shapes of two tensors based on the dimensions of an image. The function checks the dimensions of the image and returns the latent height and width based on certain business rules.\"\"\"\n        pass\n\n    def get_sample_input(self, batch_size, image_height, image_width):\n        \"\"\"This code defines a function called `get_sample_input` that takes in three parameters: batch size, image height, and image width. It checks the dimensions of an image based on certain business rules and returns a tensor with the latent height and width.\"\"\"\n        pass\n\n    def optimize(self, onnx_graph):\n        opt = Optimizer(onnx_graph)\n        opt.info(self.name + \": original\")\n        opt.select_outputs([0])  # delete graph output#1\n        opt.cleanup()\n        opt.info(self.name + \": remove output[1]\")\n        opt.fold_constants()\n        opt.info(self.name + \": fold constants\")\n        opt.infer_shapes()\n        opt.info(self.name + \": shape inference\")\n        opt.select_outputs([0], names=[\"text_embeddings\"])  # rename network output\n        opt.info(self.name + \": remove output[0]\")\n        opt_onnx_graph = opt.cleanup(return_onnx=True)\n        opt.info(self.name + \": finished\")\n        return opt_onnx_graph","documentation":"This code defines a class called \"SD Model\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#__init__().","attributes":{"range":[162,4,169,26],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#__init__().","content":"def __init__(self, device, max_batch_size, embedding_dim, min_batch_size=1):\n        super(CLIP, self).__init__(\n            device=device,\n            max_batch_size=max_batch_size,\n            min_batch_size=min_batch_size,\n            embedding_dim=embedding_dim,\n        )\n        self.name = \"CLIP\"","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"15415a8eb64c74ddb0445bb8b61a2a90b59aa62337b1e17719c2864a0122715b","processedContent":"def __init__(self, device, max_batch_size, embedding_dim, min_batch_size=1):\n        super(CLIP, self).__init__(\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#: undefined\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#__init__().: This code defines a class called \"SD Model\" that is used for image-text matching. It takes in various parameters, such as the device to use, whether to use fp16 precision, and the maximum and minimum batch sizes. The model also has attributes for the embedding dimension, text max length, and the minimum and maximum latent shapes.\n        \"\"\"\n            device=device,\n            max_batch_size=max_batch_size,\n            min_batch_size=min_batch_size,\n            embedding_dim=embedding_dim,\n        )\n        self.name = \"CLIP\"","documentation":"This code defines a class called \"SD Model\" that is used for image-text matching. It takes in various parameters, such as the device to use, whether to use fp16 precision, and the maximum and minimum batch sizes."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#get_input_names().","attributes":{"range":[171,4,172,28],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#get_input_names().","content":"def get_input_names(self):\n        return [\"input_ids\"]","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"3542eee92a27f4d789c523328d59338ab8bfc46082350fcc584ea991952e1094","processedContent":"def get_input_names(self):\n        return [\"input_ids\"]","documentation":"This code defines a function called `get_input_names` that returns a list of input names."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#get_output_names().","attributes":{"range":[174,4,175,51],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#get_output_names().","content":"def get_output_names(self):\n        return [\"text_embeddings\", \"pooler_output\"]","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"97bdedbd36c04a9062100c53633a167d6e341f1f0593ed550385e0422ebf9156","processedContent":"def get_output_names(self):\n        return [\"text_embeddings\", \"pooler_output\"]","documentation":"This code defines a function called `get_output_names` that returns a list of two strings: \"text_embeddings\" and \"pooler_output\"."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#get_dynamic_axes().","attributes":{"range":[177,4,178,67],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#get_dynamic_axes().","content":"def get_dynamic_axes(self):\n        return {\"input_ids\": {0: \"B\"}, \"text_embeddings\": {0: \"B\"}}","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"89742237f25838a9b12ac91aac3b092b40ee8efea6eacfba4e3dca789cc90bf4","processedContent":"def get_dynamic_axes(self):\n        return {\"input_ids\": {0: \"B\"}, \"text_embeddings\": {0: \"B\"}}","documentation":"This code defines a function that returns a dictionary with two keys, \"input_ids\" and \"text_embeddings\", where the values are both dictionaries with a single key-value pair where the key is 0 and the value is \"B\". The purpose of this function is to provide dynamic axes for a machine learning model."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#get_input_profile().","attributes":{"range":[180,4,191,9],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#get_input_profile().","content":"def get_input_profile(self, batch_size, image_height, image_width, static_batch, static_shape):\n        self.check_dims(batch_size, image_height, image_width)\n        min_batch, max_batch, _, _, _, _, _, _, _, _ = self.get_minmax_dims(\n            batch_size, image_height, image_width, static_batch, static_shape\n        )\n        return {\n            \"input_ids\": [\n                (min_batch, self.text_maxlen),\n                (batch_size, self.text_maxlen),\n                (max_batch, self.text_maxlen),\n            ]\n        }","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"e395ad62a2156ff12102477291034dd422ca1c0be292a21a9df0a8f3b114dc74","processedContent":"def get_input_profile(self, batch_size, image_height, image_width, static_batch, static_shape):\n        self.check_dims(batch_size, image_height, image_width) #This code checks the dimensions of an image and returns the latent height and width based on certain business rules.\n        min_batch, max_batch, _, _, _, _, _, _, _, _ = self.get_minmax_dims( #This code defines a function that returns the minimum and maximum values for various dimensions in an image processing pipeline. It takes into account the batch size, image height and width, and whether the shapes are static or dynamic.\n            batch_size, image_height, image_width, static_batch, static_shape\n        )\n        return {\n            \"input_ids\": [\n                (min_batch, self.text_maxlen), #undefined\n                (batch_size, self.text_maxlen), #undefined\n                (max_batch, self.text_maxlen), #undefined\n            ]\n        }","documentation":"This code defines a function that returns the input profile for an image processing pipeline. It checks the dimensions of an image and returns the latent height and width based on certain business rules."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#get_shape_dict().","attributes":{"range":[193,4,198,9],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#get_shape_dict().","content":"def get_shape_dict(self, batch_size, image_height, image_width):\n        self.check_dims(batch_size, image_height, image_width)\n        return {\n            \"input_ids\": (batch_size, self.text_maxlen),\n            \"text_embeddings\": (batch_size, self.text_maxlen, self.embedding_dim),\n        }","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"4b4ef5d565259f579b1332d113f7b842f73d79a15fe0314ea0a656826b9c5d9a","processedContent":"def get_shape_dict(self, batch_size, image_height, image_width):\n        self.check_dims(batch_size, image_height, image_width) #This code checks the dimensions of an image and returns the latent height and width based on certain business rules.\n        return {\n            \"input_ids\": (batch_size, self.text_maxlen), #undefined\n            \"text_embeddings\": (batch_size, self.text_maxlen, self.embedding_dim),\n            \"\"\"\n            scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#text_maxlen.: undefined\n            scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#embedding_dim.: undefined\n            \"\"\"\n        }","documentation":"This code defines a function called `get_shape_dict` that returns a dictionary with the shapes of two tensors based on the dimensions of an image. The function checks the dimensions of the image and returns the latent height and width based on certain business rules."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#get_sample_input().","attributes":{"range":[200,4,202,95],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#get_sample_input().","content":"def get_sample_input(self, batch_size, image_height, image_width):\n        self.check_dims(batch_size, image_height, image_width)\n        return torch.zeros(batch_size, self.text_maxlen, dtype=torch.int32, device=self.device)","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"99f930b35f37d28f80a40e1192878023c5315cdca8411edca4f14924a3b94873","processedContent":"def get_sample_input(self, batch_size, image_height, image_width):\n        self.check_dims(batch_size, image_height, image_width) #This code checks the dimensions of an image and returns the latent height and width based on certain business rules.\n        return torch.zeros(batch_size, self.text_maxlen, dtype=torch.int32, device=self.device)","documentation":"This code defines a function called `get_sample_input` that takes in three parameters: batch size, image height, and image width. It checks the dimensions of an image based on certain business rules and returns a tensor with the latent height and width."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#optimize().","attributes":{"range":[204,4,218,29],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#optimize().","content":"def optimize(self, onnx_graph):\n        opt = Optimizer(onnx_graph)\n        opt.info(self.name + \": original\")\n        opt.select_outputs([0])  # delete graph output#1\n        opt.cleanup()\n        opt.info(self.name + \": remove output[1]\")\n        opt.fold_constants()\n        opt.info(self.name + \": fold constants\")\n        opt.infer_shapes()\n        opt.info(self.name + \": shape inference\")\n        opt.select_outputs([0], names=[\"text_embeddings\"])  # rename network output\n        opt.info(self.name + \": remove output[0]\")\n        opt_onnx_graph = opt.cleanup(return_onnx=True)\n        opt.info(self.name + \": finished\")\n        return opt_onnx_graph","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"f386d0aaca6b1343453d0a2b840269265363a1837bf363cf3c0abdc1f1422e13","processedContent":"def optimize(self, onnx_graph):\n        opt = Optimizer(onnx_graph) #This code defines a class called `Optimizer` that optimizes an ONNX graph by cleaning it up, selecting outputs, folding constants, and inferring shapes. It also exports the optimized graph as an ONNX model if requested.\n        opt.info(self.name + \": original\")\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#info().: This code defines a function called `info` that prints information about the graph, including the number of nodes, tensors, inputs, and outputs.\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#name.: undefined\n        \"\"\"\n        opt.select_outputs([0])  # delete graph output#1 #This code is a function that takes two arguments, \"keep\" and \"names\". It modifies the graph's outputs by selecting only the ones specified in the \"keep\" argument, and then assigns names to the remaining outputs if the \"names\" argument is provided.\n        opt.cleanup() #The code cleans up the graph and sorts it in a topological order. If return_onnx is set to True, it exports the graph as an ONNX model.\n        opt.info(self.name + \": remove output[1]\")\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#info().: This code defines a function called `info` that prints information about the graph, including the number of nodes, tensors, inputs, and outputs.\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#name.: undefined\n        \"\"\"\n        opt.fold_constants() #This code folds constants in a graph, which is a technique used to optimize the performance of machine learning models. It exports the graph as an ONNX format and then imports it back into the model, which helps to reduce the computational complexity of the model.\n        opt.info(self.name + \": fold constants\")\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#info().: This code defines a function called `info` that prints information about the graph, including the number of nodes, tensors, inputs, and outputs.\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#name.: undefined\n        \"\"\"\n        opt.infer_shapes() #This code is a method that infers the shapes of a graph, which is an important step in deploying machine learning models. It exports the graph to ONNX format, checks the size of the model, and then imports it back into the graph. The method returns the ONNX graph if requested.\n        opt.info(self.name + \": shape inference\")\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#info().: This code defines a function called `info` that prints information about the graph, including the number of nodes, tensors, inputs, and outputs.\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#name.: undefined\n        \"\"\"\n        opt.select_outputs([0], names=[\"text_embeddings\"])  # rename network output #This code is a function that takes two arguments, \"keep\" and \"names\". It modifies the graph's outputs by selecting only the ones specified in the \"keep\" argument, and then assigns names to the remaining outputs if the \"names\" argument is provided.\n        opt.info(self.name + \": remove output[0]\")\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#info().: This code defines a function called `info` that prints information about the graph, including the number of nodes, tensors, inputs, and outputs.\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#name.: undefined\n        \"\"\"\n        opt_onnx_graph = opt.cleanup(return_onnx=True) #The code cleans up the graph and sorts it in a topological order. If return_onnx is set to True, it exports the graph as an ONNX model.\n        opt.info(self.name + \": finished\")\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#info().: This code defines a function called `info` that prints information about the graph, including the number of nodes, tensors, inputs, and outputs.\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#name.: undefined\n        \"\"\"\n        return opt_onnx_graph","documentation":"The code defines a function called `optimize` that takes an ONNX graph as input and optimizes it by cleaning it up, selecting outputs, folding constants, and inferring shapes. It also exports the optimized graph as an ONNX model if requested."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#__init__().","attributes":{"range":[222,4,241,26],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#__init__().","content":"def __init__(\n        self,\n        fp16=False,\n        device=\"cuda\",\n        max_batch_size=16,\n        min_batch_size=1,\n        embedding_dim=768,\n        text_maxlen=77,\n        unet_dim=4,\n    ):\n        super(UNet, self).__init__(\n            fp16=fp16,\n            device=device,\n            max_batch_size=max_batch_size,\n            min_batch_size=min_batch_size,\n            embedding_dim=embedding_dim,\n            text_maxlen=text_maxlen,\n        )\n        self.unet_dim = unet_dim\n        self.name = \"UNet\"","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"3f7d0a4bd3ed9d46002f03cbe9a941ce51ca2713c7bee0f839fcc6dfdeb34a2f","processedContent":"def __init__(\n        self,\n        fp16=False,\n        device=\"cuda\",\n        max_batch_size=16,\n        min_batch_size=1,\n        embedding_dim=768,\n        text_maxlen=77,\n        unet_dim=4,\n    ):\n        super(UNet, self).__init__(\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#: undefined\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#__init__().: This code defines a class called \"SD Model\" that is used for image-text matching. It takes in various parameters, such as the device to use, whether to use fp16 precision, and the maximum and minimum batch sizes. The model also has attributes for the embedding dimension, text max length, and the minimum and maximum latent shapes.\n        \"\"\"\n            fp16=fp16,\n            device=device,\n            max_batch_size=max_batch_size,\n            min_batch_size=min_batch_size,\n            embedding_dim=embedding_dim,\n            text_maxlen=text_maxlen,\n        )\n        self.unet_dim = unet_dim\n        self.name = \"UNet\"","documentation":"This code defines a class called \"SD Model\" that is used for image-text matching. It takes in various parameters and has attributes for the embedding dimension, text max length, and the minimum and maximum latent shapes."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_input_names().","attributes":{"range":[243,4,244,62],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_input_names().","content":"def get_input_names(self):\n        return [\"sample\", \"timestep\", \"encoder_hidden_states\"]","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"3b260c967cf75d0d2de7fac6a6b168befc0746a4772ab6396e95e86138e0018b","processedContent":"def get_input_names(self):\n        return [\"sample\", \"timestep\", \"encoder_hidden_states\"]","documentation":"This code defines a function called `get_input_names` that returns a list of strings representing the names of the input variables required by the model."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_output_names().","attributes":{"range":[246,4,247,25],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_output_names().","content":"def get_output_names(self):\n        return [\"latent\"]","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"e7187a56f0865bb1097aef558bb441bd0376a49d8935a5b552ed0cdb4ff936f3","processedContent":"def get_output_names(self):\n        return [\"latent\"]","documentation":"This code defines a function called `get_output_names` that returns a list of strings representing the names of the output variables for a machine learning model."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_dynamic_axes().","attributes":{"range":[249,4,255,9],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_dynamic_axes().","content":"def get_dynamic_axes(self):\n        return {\n            \"sample\": {0: \"2B\", 2: \"H\", 3: \"W\"},\n            \"timestep\": {0: \"2B\"},\n            \"encoder_hidden_states\": {0: \"2B\"},\n            \"latent\": {0: \"2B\", 2: \"H\", 3: \"W\"},\n        }","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"515991b8ebcec4e58c0150350a740411c9806a22cdd6aebb4d4d21fd0a4bfb4c","processedContent":"def get_dynamic_axes(self):\n        return {\n            \"sample\": {0: \"2B\", 2: \"H\", 3: \"W\"},\n            \"timestep\": {0: \"2B\"},\n            \"encoder_hidden_states\": {0: \"2B\"},\n            \"latent\": {0: \"2B\", 2: \"H\", 3: \"W\"},\n        }","documentation":"This code defines a function called `get_dynamic_axes` that returns a dictionary with dynamic axes for a neural network. The dictionary maps the axis names to their corresponding sizes, which can vary depending on the input data."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_input_profile().","attributes":{"range":[257,4,283,9],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_input_profile().","content":"def get_input_profile(self, batch_size, image_height, image_width, static_batch, static_shape):\n        latent_height, latent_width = self.check_dims(batch_size, image_height, image_width)\n        (\n            min_batch,\n            max_batch,\n            _,\n            _,\n            _,\n            _,\n            min_latent_height,\n            max_latent_height,\n            min_latent_width,\n            max_latent_width,\n        ) = self.get_minmax_dims(batch_size, image_height, image_width, static_batch, static_shape)\n        return {\n            \"sample\": [\n                (min_batch, self.unet_dim, min_latent_height, min_latent_width),\n                (batch_size, self.unet_dim, latent_height, latent_width),\n                (max_batch, self.unet_dim, max_latent_height, max_latent_width),\n            ],\n            \"timestep\": [(min_batch,), (batch_size,), (max_batch,)],\n            \"encoder_hidden_states\": [\n                (min_batch, self.text_maxlen, self.embedding_dim),\n                (batch_size, self.text_maxlen, self.embedding_dim),\n                (max_batch, self.text_maxlen, self.embedding_dim),\n            ],\n        }","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"e9a23a2d59be14c832d9d699ebdd597dfd380ee1e8ee885c970bd3f967f826e1","processedContent":"def get_input_profile(self, batch_size, image_height, image_width, static_batch, static_shape):\n        latent_height, latent_width = self.check_dims(batch_size, image_height, image_width) #This code checks the dimensions of an image and returns the latent height and width based on certain business rules.\n        (\n            min_batch,\n            max_batch,\n            _,\n            _,\n            _,\n            _,\n            min_latent_height,\n            max_latent_height,\n            min_latent_width,\n            max_latent_width,\n        ) = self.get_minmax_dims(batch_size, image_height, image_width, static_batch, static_shape) #This code defines a function that returns the minimum and maximum values for various dimensions in an image processing pipeline. It takes into account the batch size, image height and width, and whether the shapes are static or dynamic.\n        return {\n            \"sample\": [\n                (min_batch, self.unet_dim, min_latent_height, min_latent_width), #undefined\n                (batch_size, self.unet_dim, latent_height, latent_width), #undefined\n                (max_batch, self.unet_dim, max_latent_height, max_latent_width), #undefined\n            ],\n            \"timestep\": [(min_batch,), (batch_size,), (max_batch,)],\n            \"encoder_hidden_states\": [\n                (min_batch, self.text_maxlen, self.embedding_dim),\n                \"\"\"\n                scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#text_maxlen.: undefined\n                scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#embedding_dim.: undefined\n                \"\"\"\n                (batch_size, self.text_maxlen, self.embedding_dim),\n                \"\"\"\n                scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#text_maxlen.: undefined\n                scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#embedding_dim.: undefined\n                \"\"\"\n                (max_batch, self.text_maxlen, self.embedding_dim),\n                \"\"\"\n                scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#text_maxlen.: undefined\n                scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#embedding_dim.: undefined\n                \"\"\"\n            ],\n        }","documentation":"This code defines a function that returns a dictionary with various dimensions for an image processing pipeline. It takes into account the batch size, image height and width, and whether the shapes are static or dynamic. The function also defines a function that returns the minimum and maximum values for various dimensions in the pipeline."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_shape_dict().","attributes":{"range":[285,4,292,9],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_shape_dict().","content":"def get_shape_dict(self, batch_size, image_height, image_width):\n        latent_height, latent_width = self.check_dims(batch_size, image_height, image_width)\n        return {\n            \"sample\": (2 * batch_size, self.unet_dim, latent_height, latent_width),\n            \"timestep\": (2 * batch_size,),\n            \"encoder_hidden_states\": (2 * batch_size, self.text_maxlen, self.embedding_dim),\n            \"latent\": (2 * batch_size, 4, latent_height, latent_width),\n        }","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"ef78dd867f98c8f41e53b32c2a9db4900ea3e8bf7ac2ae9d67835a98bb22f1b2","processedContent":"def get_shape_dict(self, batch_size, image_height, image_width):\n        latent_height, latent_width = self.check_dims(batch_size, image_height, image_width) #This code checks the dimensions of an image and returns the latent height and width based on certain business rules.\n        return {\n            \"sample\": (2 * batch_size, self.unet_dim, latent_height, latent_width), #undefined\n            \"timestep\": (2 * batch_size,),\n            \"encoder_hidden_states\": (2 * batch_size, self.text_maxlen, self.embedding_dim),\n            \"\"\"\n            scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#text_maxlen.: undefined\n            scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#embedding_dim.: undefined\n            \"\"\"\n            \"latent\": (2 * batch_size, 4, latent_height, latent_width),\n        }","documentation":"This code defines a function that returns a dictionary with various shapes for different tensors used in a deep learning model. The function takes in three parameters: batch size, image height, and image width, and checks the dimensions of an image based on certain business rules to determine the latent height and width. The dictionary returned by the function contains shapes for the \"sample\", \"timestep\", \"encoder_hidden_states\", and \"latent\" tensors."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_sample_input().","attributes":{"range":[294,4,303,9],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_sample_input().","content":"def get_sample_input(self, batch_size, image_height, image_width):\n        latent_height, latent_width = self.check_dims(batch_size, image_height, image_width)\n        dtype = torch.float16 if self.fp16 else torch.float32\n        return (\n            torch.randn(\n                2 * batch_size, self.unet_dim, latent_height, latent_width, dtype=torch.float32, device=self.device\n            ),\n            torch.ones((2 * batch_size,), dtype=torch.float32, device=self.device),\n            torch.randn(2 * batch_size, self.text_maxlen, self.embedding_dim, dtype=dtype, device=self.device),\n        )","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"777ba99975aadf1b8d65d12bd4d7c2843b955bbf01f4db718ae992f490b0c263","processedContent":"def get_sample_input(self, batch_size, image_height, image_width):\n        latent_height, latent_width = self.check_dims(batch_size, image_height, image_width) #This code checks the dimensions of an image and returns the latent height and width based on certain business rules.\n        dtype = torch.float16 if self.fp16 else torch.float32 #undefined\n        return (\n            torch.randn(\n                2 * batch_size, self.unet_dim, latent_height, latent_width, dtype=torch.float32, device=self.device\n                \"\"\"\n                scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#unet_dim.: undefined\n                scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#device.: undefined\n                \"\"\"\n            ),\n            torch.ones((2 * batch_size,), dtype=torch.float32, device=self.device), #undefined\n            torch.randn(2 * batch_size, self.text_maxlen, self.embedding_dim, dtype=dtype, device=self.device),\n            \"\"\"\n            scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#device.: undefined\n            scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#text_maxlen.: undefined\n            scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#embedding_dim.: undefined\n            \"\"\"\n        )","documentation":"This code defines a function called `get_sample_input` that generates a set of input data for a machine learning model. The function takes in three parameters: `batch_size`, `image_height`, and `image_width`. It then checks the dimensions of the image and returns the latent height and width based on certain business rules. The function also creates two random tensors with the specified dimensions and data types."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#__init__().","attributes":{"range":[307,4,314,33],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#__init__().","content":"def __init__(self, device, max_batch_size, min_batch_size=1):\n        super(VAE, self).__init__(\n            device=device,\n            max_batch_size=max_batch_size,\n            min_batch_size=min_batch_size,\n            embedding_dim=None,\n        )\n        self.name = \"VAE decoder\"","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"2a64469d3077c6b9759ca2f1e11e81c97b7a6c9376fec572cb7ce031bac5535d","processedContent":"def __init__(self, device, max_batch_size, min_batch_size=1):\n        super(VAE, self).__init__(\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#: undefined\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#__init__().: This code defines a class called \"SD Model\" that is used for image-text matching. It takes in various parameters, such as the device to use, whether to use fp16 precision, and the maximum and minimum batch sizes. The model also has attributes for the embedding dimension, text max length, and the minimum and maximum latent shapes.\n        \"\"\"\n            device=device,\n            max_batch_size=max_batch_size,\n            min_batch_size=min_batch_size,\n            embedding_dim=None,\n        )\n        self.name = \"VAE decoder\"","documentation":"This code defines a class called \"VAE decoder\" that is used for image-text matching. It takes in various parameters, such as the device to use and the maximum and minimum batch sizes, and has attributes for the embedding dimension, text max length, and the minimum and maximum latent shapes."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#get_input_names().","attributes":{"range":[316,4,317,25],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#get_input_names().","content":"def get_input_names(self):\n        return [\"latent\"]","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"55c335eb2d67d6c310029d61862831b166abd6f7d93f14ee55f75d967a9f78ad","processedContent":"def get_input_names(self):\n        return [\"latent\"]","documentation":"This code defines a function called `get_input_names` that returns a list of strings representing the names of the input variables for a machine learning model."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#get_output_names().","attributes":{"range":[319,4,320,25],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#get_output_names().","content":"def get_output_names(self):\n        return [\"images\"]","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"8b5f7ecfc651d25d6e720aa6f05e2f0e32a492996eea9aee7d8d79b9769ed4e2","processedContent":"def get_output_names(self):\n        return [\"images\"]","documentation":"This code defines a function called `get_output_names` that returns a list of strings, where each string represents the name of an output. In this case, the only output is \"images\"."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#get_dynamic_axes().","attributes":{"range":[322,4,326,9],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#get_dynamic_axes().","content":"def get_dynamic_axes(self):\n        return {\n            \"latent\": {0: \"B\", 2: \"H\", 3: \"W\"},\n            \"images\": {0: \"B\", 2: \"8H\", 3: \"8W\"},\n        }","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"c7caf808474a80c9e4e22e980f981c219bb4b44f6b5d636924ed768d16435f0b","processedContent":"def get_dynamic_axes(self):\n        return {\n            \"latent\": {0: \"B\", 2: \"H\", 3: \"W\"},\n            \"images\": {0: \"B\", 2: \"8H\", 3: \"8W\"},\n        }","documentation":"This code defines a function called `get_dynamic_axes` that returns a dictionary with two keys: \"latent\" and \"images\". The values of these keys are dictionaries that map the indices of the input tensors to their corresponding batch, height, and width dimensions."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#get_input_profile().","attributes":{"range":[328,4,348,9],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#get_input_profile().","content":"def get_input_profile(self, batch_size, image_height, image_width, static_batch, static_shape):\n        latent_height, latent_width = self.check_dims(batch_size, image_height, image_width)\n        (\n            min_batch,\n            max_batch,\n            _,\n            _,\n            _,\n            _,\n            min_latent_height,\n            max_latent_height,\n            min_latent_width,\n            max_latent_width,\n        ) = self.get_minmax_dims(batch_size, image_height, image_width, static_batch, static_shape)\n        return {\n            \"latent\": [\n                (min_batch, 4, min_latent_height, min_latent_width),\n                (batch_size, 4, latent_height, latent_width),\n                (max_batch, 4, max_latent_height, max_latent_width),\n            ]\n        }","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"a1b694358ddaf65be9346b42ef9e4457a9760f44494b0b19af797e36ce92f8e4","processedContent":"def get_input_profile(self, batch_size, image_height, image_width, static_batch, static_shape):\n        latent_height, latent_width = self.check_dims(batch_size, image_height, image_width) #This code checks the dimensions of an image and returns the latent height and width based on certain business rules.\n        (\n            min_batch,\n            max_batch,\n            _,\n            _,\n            _,\n            _,\n            min_latent_height,\n            max_latent_height,\n            min_latent_width,\n            max_latent_width,\n        ) = self.get_minmax_dims(batch_size, image_height, image_width, static_batch, static_shape) #This code defines a function that returns the minimum and maximum values for various dimensions in an image processing pipeline. It takes into account the batch size, image height and width, and whether the shapes are static or dynamic.\n        return {\n            \"latent\": [\n                (min_batch, 4, min_latent_height, min_latent_width),\n                (batch_size, 4, latent_height, latent_width),\n                (max_batch, 4, max_latent_height, max_latent_width),\n            ]\n        }","documentation":"This code defines a function that returns the minimum and maximum values for various dimensions in an image processing pipeline. It takes into account the batch size, image height and width, and whether the shapes are static or dynamic."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#get_shape_dict().","attributes":{"range":[350,4,355,9],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#get_shape_dict().","content":"def get_shape_dict(self, batch_size, image_height, image_width):\n        latent_height, latent_width = self.check_dims(batch_size, image_height, image_width)\n        return {\n            \"latent\": (batch_size, 4, latent_height, latent_width),\n            \"images\": (batch_size, 3, image_height, image_width),\n        }","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"9c671ae948ab21886f630a2effe990905e2c32e78c2c72e4aa66f06c7403b8f5","processedContent":"def get_shape_dict(self, batch_size, image_height, image_width):\n        latent_height, latent_width = self.check_dims(batch_size, image_height, image_width) #This code checks the dimensions of an image and returns the latent height and width based on certain business rules.\n        return {\n            \"latent\": (batch_size, 4, latent_height, latent_width),\n            \"images\": (batch_size, 3, image_height, image_width),\n        }","documentation":"This code defines a function that returns a dictionary with two keys: \"latent\" and \"images\". The \"latent\" key has a shape of (batch_size, 4, latent_height, latent_width), while the \"images\" key has a shape of (batch_size, 3, image_height, image_width). The function checks the dimensions of an image and returns the latent height and width based on certain business rules."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#get_sample_input().","attributes":{"range":[357,4,366,9],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#get_sample_input().","content":"def get_sample_input(self, batch_size, image_height, image_width):\n        latent_height, latent_width = self.check_dims(batch_size, image_height, image_width)\n        return torch.randn(\n            batch_size,\n            4,\n            latent_height,\n            latent_width,\n            dtype=torch.float32,\n            device=self.device,\n        )","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"73b2cf04ad242ac007cbfc33406d5f1da6b6acf584c25397786bc5faae8d6ea7","processedContent":"def get_sample_input(self, batch_size, image_height, image_width):\n        latent_height, latent_width = self.check_dims(batch_size, image_height, image_width) #This code checks the dimensions of an image and returns the latent height and width based on certain business rules.\n        return torch.randn(\n            batch_size,\n            4,\n            latent_height,\n            latent_width,\n            dtype=torch.float32,\n            device=self.device, #undefined\n        )","documentation":"This code generates a random input tensor for a deep learning model, with the dimensions of the input determined by the business rules defined in the check_dims function."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#__init__().","attributes":{"range":[370,4,377,33],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#__init__().","content":"def __init__(self, device, max_batch_size, min_batch_size=1):\n        super(VAEEncoder, self).__init__(\n            device=device,\n            max_batch_size=max_batch_size,\n            min_batch_size=min_batch_size,\n            embedding_dim=None,\n        )\n        self.name = \"VAE encoder\"","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"7339337a82443fec0190a76a1fbaaa93f14a140b53c613bd537016ca77e5c574","processedContent":"def __init__(self, device, max_batch_size, min_batch_size=1):\n        super(VAEEncoder, self).__init__(\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#: undefined\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#__init__().: This code defines a class called \"SD Model\" that is used for image-text matching. It takes in various parameters, such as the device to use, whether to use fp16 precision, and the maximum and minimum batch sizes. The model also has attributes for the embedding dimension, text max length, and the minimum and maximum latent shapes.\n        \"\"\"\n            device=device,\n            max_batch_size=max_batch_size,\n            min_batch_size=min_batch_size,\n            embedding_dim=None,\n        )\n        self.name = \"VAE encoder\"","documentation":"This code defines a class called \"VAEEncoder\" that is used for image-text matching. It takes in various parameters, such as the device to use and the maximum and minimum batch sizes, and has attributes for the embedding dimension, text max length, and the minimum and maximum latent shapes."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#get_input_names().","attributes":{"range":[379,4,380,25],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#get_input_names().","content":"def get_input_names(self):\n        return [\"images\"]","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"add31af8f2a76c27f6f0923bcebed7b45f655785f73f75a3b207f9c142744e22","processedContent":"def get_input_names(self):\n        return [\"images\"]","documentation":"This code defines a function called `get_input_names` that returns a list of strings representing the names of input data required for the model to run."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#get_output_names().","attributes":{"range":[382,4,383,25],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#get_output_names().","content":"def get_output_names(self):\n        return [\"latent\"]","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"e7187a56f0865bb1097aef558bb441bd0376a49d8935a5b552ed0cdb4ff936f3","processedContent":"def get_output_names(self):\n        return [\"latent\"]","documentation":"This code defines a function called `get_output_names` that returns a list of strings representing the names of the output variables for a machine learning model."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#get_dynamic_axes().","attributes":{"range":[385,4,389,9],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#get_dynamic_axes().","content":"def get_dynamic_axes(self):\n        return {\n            \"images\": {0: \"B\", 2: \"8H\", 3: \"8W\"},\n            \"latent\": {0: \"B\", 2: \"H\", 3: \"W\"},\n        }","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"fb6d7fbb65f0664fba629c45308b43a92cd693ede496d7ed6723d7612c8232c5","processedContent":"def get_dynamic_axes(self):\n        return {\n            \"images\": {0: \"B\", 2: \"8H\", 3: \"8W\"},\n            \"latent\": {0: \"B\", 2: \"H\", 3: \"W\"},\n        }","documentation":"This code defines a function called `get_dynamic_axes` that returns a dictionary with two keys: \"images\" and \"latent\". The values of these keys are also dictionaries that map the indices of the input tensors to their corresponding batch, height, and width dimensions."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#get_input_profile().","attributes":{"range":[391,4,415,9],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#get_input_profile().","content":"def get_input_profile(self, batch_size, image_height, image_width, static_batch, static_shape):\n        assert batch_size >= self.min_batch and batch_size <= self.max_batch\n        min_batch = batch_size if static_batch else self.min_batch\n        max_batch = batch_size if static_batch else self.max_batch\n        self.check_dims(batch_size, image_height, image_width)\n        (\n            min_batch,\n            max_batch,\n            min_image_height,\n            max_image_height,\n            min_image_width,\n            max_image_width,\n            _,\n            _,\n            _,\n            _,\n        ) = self.get_minmax_dims(batch_size, image_height, image_width, static_batch, static_shape)\n\n        return {\n            \"images\": [\n                (min_batch, 3, min_image_height, min_image_width),\n                (batch_size, 3, image_height, image_width),\n                (max_batch, 3, max_image_height, max_image_width),\n            ],\n        }","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"d4abeeea8776edb47cb12e013dacdeeb85d1b9c395f2eaf3d5cc4d5b2775fb3a","processedContent":"def get_input_profile(self, batch_size, image_height, image_width, static_batch, static_shape):\n        assert batch_size >= self.min_batch and batch_size <= self.max_batch\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#min_batch.: undefined\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#max_batch.: undefined\n        \"\"\"\n        min_batch = batch_size if static_batch else self.min_batch #undefined\n        max_batch = batch_size if static_batch else self.max_batch #undefined\n        self.check_dims(batch_size, image_height, image_width) #This code checks the dimensions of an image and returns the latent height and width based on certain business rules.\n        (\n            min_batch,\n            max_batch,\n            min_image_height,\n            max_image_height,\n            min_image_width,\n            max_image_width,\n            _,\n            _,\n            _,\n            _,\n        ) = self.get_minmax_dims(batch_size, image_height, image_width, static_batch, static_shape) #This code defines a function that returns the minimum and maximum values for various dimensions in an image processing pipeline. It takes into account the batch size, image height and width, and whether the shapes are static or dynamic.\n\n        return {\n            \"images\": [\n                (min_batch, 3, min_image_height, min_image_width),\n                (batch_size, 3, image_height, image_width),\n                (max_batch, 3, max_image_height, max_image_width),\n            ],\n        }","documentation":"This code defines a function that returns the minimum and maximum values for various dimensions in an image processing pipeline. It takes into account the batch size, image height and width, and whether the shapes are static or dynamic."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#get_shape_dict().","attributes":{"range":[417,4,422,9],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#get_shape_dict().","content":"def get_shape_dict(self, batch_size, image_height, image_width):\n        latent_height, latent_width = self.check_dims(batch_size, image_height, image_width)\n        return {\n            \"images\": (batch_size, 3, image_height, image_width),\n            \"latent\": (batch_size, 4, latent_height, latent_width),\n        }","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"f487a79d3a1e18c4fa4b5590591b749bc753553050f5e12386670b7c26d4ff67","processedContent":"def get_shape_dict(self, batch_size, image_height, image_width):\n        latent_height, latent_width = self.check_dims(batch_size, image_height, image_width) #This code checks the dimensions of an image and returns the latent height and width based on certain business rules.\n        return {\n            \"images\": (batch_size, 3, image_height, image_width),\n            \"latent\": (batch_size, 4, latent_height, latent_width),\n        }","documentation":"This code defines a function that returns a dictionary with two keys: \"images\" and \"latent\". The \"images\" key has a shape of (batch_size, 3, image_height, image_width), while the \"latent\" key has a shape of (batch_size, 4, latent_height, latent_width). The function checks the dimensions of an image and returns the latent height and width based on certain business rules."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#get_sample_input().","attributes":{"range":[424,4,433,9],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#get_sample_input().","content":"def get_sample_input(self, batch_size, image_height, image_width):\n        self.check_dims(batch_size, image_height, image_width)\n        return torch.randn(\n            batch_size,\n            3,\n            image_height,\n            image_width,\n            dtype=torch.float32,\n            device=self.device,\n        )","file":"/src/streamdiffusion/acceleration/tensorrt/models.py","language":"python","fileHash":"fb132cb02163615390b206c1f82b12bb08114d37ad8fd2de1069a272a6f01d68","hash":"e69ebce79489801ebdb972732b8c9954e4400ae5e7be0e4820f8d5b51b92af19","processedContent":"def get_sample_input(self, batch_size, image_height, image_width):\n        self.check_dims(batch_size, image_height, image_width) #This code checks the dimensions of an image and returns the latent height and width based on certain business rules.\n        return torch.randn(\n            batch_size,\n            3,\n            image_height,\n            image_width,\n            dtype=torch.float32,\n            device=self.device, #undefined\n        )","documentation":"This code defines a function called `get_sample_input` that generates a random input tensor for a deep learning model. The function takes three parameters: batch size, image height, and image width. It checks the dimensions of the input based on certain business rules and returns a tensor with the specified dimensions."}},{"key":"scip-python python temp indexer onnx/__init__:","attributes":{"symbol":"scip-python python temp indexer onnx/__init__:","language":"python"}},{"key":"scip-python python temp indexer `polygraphy.backend.onnx.loader`/__init__:","attributes":{"symbol":"scip-python python temp indexer `polygraphy.backend.onnx.loader`/__init__:","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#verbose.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#verbose.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#graph.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#graph.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#min_image_shape.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#min_image_shape.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#max_image_shape.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#max_image_shape.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#verbose.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#verbose.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#name.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#name.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#min_batch.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#min_batch.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#max_batch.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#max_batch.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#text_maxlen.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#text_maxlen.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#embedding_dim.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#embedding_dim.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#device.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#device.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#name.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#name.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#unet_dim.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#unet_dim.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#fp16.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#fp16.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/CUASSERT().","attributes":{"range":[70,0,78,15],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/CUASSERT().","content":"def CUASSERT(cuda_ret):\n    err = cuda_ret[0]\n    if err != cudart.cudaError_t.cudaSuccess:\n        raise RuntimeError(\n            f\"CUDA ERROR: {err}, error code reference: https://nvidia.github.io/cuda-python/module/cudart.html#cuda.cudart.cudaError_t\"\n        )\n    if len(cuda_ret) > 1:\n        return cuda_ret[1]\n    return None","file":"/src/streamdiffusion/acceleration/tensorrt/utilities.py","language":"python","fileHash":"63a6abfeb186705edc5dba1605e60b5eaa1a86e848db1ddcffe555e16476075c","hash":"44752d286d7e664f6a77394671d20e5dc7b2d09b0e7afc34bf6bb5448329ce4d","processedContent":"def CUASSERT(cuda_ret):\n    err = cuda_ret[0]\n    if err != cudart.cudaError_t.cudaSuccess:\n        raise RuntimeError(\n            f\"CUDA ERROR: {err}, error code reference: https://nvidia.github.io/cuda-python/module/cudart.html#cuda.cudart.cudaError_t\"\n        )\n    if len(cuda_ret) > 1:\n        return cuda_ret[1]\n    return None","documentation":"This code defines a function called CUASSERT that checks the return value of a CUDA call and raises an error if it fails. It also returns the result of the CUDA call if it succeeds."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#__init__().","attributes":{"range":[82,4,91,39],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#__init__().","content":"def __init__(\n        self,\n        engine_path,\n    ):\n        self.engine_path = engine_path\n        self.engine = None\n        self.context = None\n        self.buffers = OrderedDict()\n        self.tensors = OrderedDict()\n        self.cuda_graph_instance = None","file":"/src/streamdiffusion/acceleration/tensorrt/utilities.py","language":"python","fileHash":"63a6abfeb186705edc5dba1605e60b5eaa1a86e848db1ddcffe555e16476075c","hash":"b5f22667df2f2d5d8b8d1053b1c59cd82284f971bca222227e7ba7256db14b0a","processedContent":"def __init__(\n        self,\n        engine_path,\n    ):\n        self.engine_path = engine_path\n        self.engine = None\n        self.context = None\n        self.buffers = OrderedDict() #undefined\n        self.tensors = OrderedDict() #undefined\n        self.cuda_graph_instance = None","documentation":"This code defines a class that initializes an object with various attributes, including engine_path, engine, context, buffers, tensors, and cuda_graph_instance."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#__del__().","attributes":{"range":[93,4,98,24],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#__del__().","content":"def __del__(self):\n        [buf.free() for buf in self.buffers.values() if isinstance(buf, cuda.DeviceArray)]\n        del self.engine\n        del self.context\n        del self.buffers\n        del self.tensors","file":"/src/streamdiffusion/acceleration/tensorrt/utilities.py","language":"python","fileHash":"63a6abfeb186705edc5dba1605e60b5eaa1a86e848db1ddcffe555e16476075c","hash":"8c0e5ddb1da8e7ba817d8ec9db477424d59a5dda6bb2f92fdc81037e1c677af6","processedContent":"def __del__(self):\n        [buf.free() for buf in self.buffers.values() if isinstance(buf, cuda.DeviceArray)]\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#buffers.: undefined\n        scip-python python python-stdlib 3.11 collections/OrderedDict#values().: undefined\n        \"\"\"\n        del self.engine #undefined\n        del self.context #undefined\n        del self.buffers #undefined\n        del self.tensors","documentation":"This code is a destructor method for an object that frees memory and deletes references to other objects when the object is no longer needed."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#refit().","attributes":{"range":[100,4,201,19],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#refit().","content":"def refit(self, onnx_path, onnx_refit_path):\n        def convert_int64(arr):\n            # TODO: smarter conversion\n            if len(arr.shape) == 0:\n                return np.int32(arr)\n            return arr\n\n        def add_to_map(refit_dict, name, values):\n            if name in refit_dict:\n                assert refit_dict[name] is None\n                if values.dtype == np.int64:\n                    values = convert_int64(values)\n                refit_dict[name] = values\n\n        print(f\"Refitting TensorRT engine with {onnx_refit_path} weights\")\n        refit_nodes = gs.import_onnx(onnx.load(onnx_refit_path)).toposort().nodes\n\n        # Construct mapping from weight names in refit model -> original model\n        name_map = {}\n        for n, node in enumerate(gs.import_onnx(onnx.load(onnx_path)).toposort().nodes):\n            refit_node = refit_nodes[n]\n            assert node.op == refit_node.op\n            # Constant nodes in ONNX do not have inputs but have a constant output\n            if node.op == \"Constant\":\n                name_map[refit_node.outputs[0].name] = node.outputs[0].name\n            # Handle scale and bias weights\n            elif node.op == \"Conv\":\n                if node.inputs[1].__class__ == gs.Constant:\n                    name_map[refit_node.name + \"_TRTKERNEL\"] = node.name + \"_TRTKERNEL\"\n                if node.inputs[2].__class__ == gs.Constant:\n                    name_map[refit_node.name + \"_TRTBIAS\"] = node.name + \"_TRTBIAS\"\n            # For all other nodes: find node inputs that are initializers (gs.Constant)\n            else:\n                for i, inp in enumerate(node.inputs):\n                    if inp.__class__ == gs.Constant:\n                        name_map[refit_node.inputs[i].name] = inp.name\n\n        def map_name(name):\n            if name in name_map:\n                return name_map[name]\n            return name\n\n        # Construct refit dictionary\n        refit_dict = {}\n        refitter = trt.Refitter(self.engine, TRT_LOGGER)\n        all_weights = refitter.get_all()\n        for layer_name, role in zip(all_weights[0], all_weights[1]):\n            # for speciailized roles, use a unique name in the map:\n            if role == trt.WeightsRole.KERNEL:\n                name = layer_name + \"_TRTKERNEL\"\n            elif role == trt.WeightsRole.BIAS:\n                name = layer_name + \"_TRTBIAS\"\n            else:\n                name = layer_name\n\n            assert name not in refit_dict, \"Found duplicate layer: \" + name\n            refit_dict[name] = None\n\n        for n in refit_nodes:\n            # Constant nodes in ONNX do not have inputs but have a constant output\n            if n.op == \"Constant\":\n                name = map_name(n.outputs[0].name)\n                print(f\"Add Constant {name}\\n\")\n                add_to_map(refit_dict, name, n.outputs[0].values)\n\n            # Handle scale and bias weights\n            elif n.op == \"Conv\":\n                if n.inputs[1].__class__ == gs.Constant:\n                    name = map_name(n.name + \"_TRTKERNEL\")\n                    add_to_map(refit_dict, name, n.inputs[1].values)\n\n                if n.inputs[2].__class__ == gs.Constant:\n                    name = map_name(n.name + \"_TRTBIAS\")\n                    add_to_map(refit_dict, name, n.inputs[2].values)\n\n            # For all other nodes: find node inputs that are initializers (AKA gs.Constant)\n            else:\n                for inp in n.inputs:\n                    name = map_name(inp.name)\n                    if inp.__class__ == gs.Constant:\n                        add_to_map(refit_dict, name, inp.values)\n\n        for layer_name, weights_role in zip(all_weights[0], all_weights[1]):\n            if weights_role == trt.WeightsRole.KERNEL:\n                custom_name = layer_name + \"_TRTKERNEL\"\n            elif weights_role == trt.WeightsRole.BIAS:\n                custom_name = layer_name + \"_TRTBIAS\"\n            else:\n                custom_name = layer_name\n\n            # Skip refitting Trilu for now; scalar weights of type int64 value 1 - for clip model\n            if layer_name.startswith(\"onnx::Trilu\"):\n                continue\n\n            if refit_dict[custom_name] is not None:\n                refitter.set_weights(layer_name, weights_role, refit_dict[custom_name])\n            else:\n                print(f\"[W] No refit weights for layer: {layer_name}\")\n\n        if not refitter.refit_cuda_engine():\n            print(\"Failed to refit!\")\n            exit(0)","file":"/src/streamdiffusion/acceleration/tensorrt/utilities.py","language":"python","fileHash":"63a6abfeb186705edc5dba1605e60b5eaa1a86e848db1ddcffe555e16476075c","hash":"2e50a173c2293ccfe620c83ce29d3cd043b36cd4df14f9a693d199fe33e59c29","processedContent":"def refit(self, onnx_path, onnx_refit_path):\n        def convert_int64(arr):\n            \"\"\"This code defines a function called `convert_int64` that takes an array as input and converts it to an integer. The function first checks the shape of the array, and if it is empty, it returns a 32-bit integer. Otherwise, it returns the original array.\"\"\"\n            pass\n\n        def add_to_map(refit_dict, name, values):\n            \"\"\"This code defines a function called `add_to_map` that takes two arguments: a dictionary and a value. The function checks if the key exists in the dictionary, and if it does, it asserts that the value is None. If the value is not None, it converts it to an integer using a custom function called `convert_int64`.\"\"\"\n            pass\n\n        print(f\"Refitting TensorRT engine with {onnx_refit_path} weights\")\n        refit_nodes = gs.import_onnx(onnx.load(onnx_refit_path)).toposort().nodes\n\n        # Construct mapping from weight names in refit model -> original model\n        name_map = {}\n        for n, node in enumerate(gs.import_onnx(onnx.load(onnx_path)).toposort().nodes):\n            refit_node = refit_nodes[n]\n            assert node.op == refit_node.op\n            # Constant nodes in ONNX do not have inputs but have a constant output\n            if node.op == \"Constant\":\n                name_map[refit_node.outputs[0].name] = node.outputs[0].name\n            # Handle scale and bias weights\n            elif node.op == \"Conv\":\n                if node.inputs[1].__class__ == gs.Constant:\n                    name_map[refit_node.name + \"_TRTKERNEL\"] = node.name + \"_TRTKERNEL\"\n                if node.inputs[2].__class__ == gs.Constant:\n                    name_map[refit_node.name + \"_TRTBIAS\"] = node.name + \"_TRTBIAS\"\n            # For all other nodes: find node inputs that are initializers (gs.Constant)\n            else:\n                for i, inp in enumerate(node.inputs):\n                    if inp.__class__ == gs.Constant:\n                        name_map[refit_node.inputs[i].name] = inp.name\n\n        def map_name(name):\n            \"\"\"This code defines a function called `map_name` that takes a name as an argument and returns the mapped name if it exists in a dictionary called `name_map`. If the name is not found in the map, it returns the original name.\"\"\"\n            pass\n\n        # Construct refit dictionary\n        refit_dict = {}\n        refitter = trt.Refitter(self.engine, TRT_LOGGER)\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#engine.: undefined\n        scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/TRT_LOGGER.: undefined\n        \"\"\"\n        all_weights = refitter.get_all()\n        for layer_name, role in zip(all_weights[0], all_weights[1]):\n            # for speciailized roles, use a unique name in the map:\n            if role == trt.WeightsRole.KERNEL:\n                name = layer_name + \"_TRTKERNEL\"\n            elif role == trt.WeightsRole.BIAS:\n                name = layer_name + \"_TRTBIAS\"\n            else:\n                name = layer_name\n\n            assert name not in refit_dict, \"Found duplicate layer: \" + name\n            refit_dict[name] = None\n\n        for n in refit_nodes:\n            # Constant nodes in ONNX do not have inputs but have a constant output\n            if n.op == \"Constant\":\n                name = map_name(n.outputs[0].name) #This code defines a function called `map_name` that takes a name as an argument and returns the mapped name if it exists in a dictionary called `name_map`. If the name is not found in the map, it returns the original name.\n                print(f\"Add Constant {name}\\n\")\n                add_to_map(refit_dict, name, n.outputs[0].values) #This code defines a function called `add_to_map` that takes two arguments: a dictionary and a value. The function checks if the key exists in the dictionary, and if it does, it asserts that the value is None. If the value is not None, it converts it to an integer using a custom function called `convert_int64`.\n\n            # Handle scale and bias weights\n            elif n.op == \"Conv\":\n                if n.inputs[1].__class__ == gs.Constant:\n                    name = map_name(n.name + \"_TRTKERNEL\") #This code defines a function called `map_name` that takes a name as an argument and returns the mapped name if it exists in a dictionary called `name_map`. If the name is not found in the map, it returns the original name.\n                    add_to_map(refit_dict, name, n.inputs[1].values) #This code defines a function called `add_to_map` that takes two arguments: a dictionary and a value. The function checks if the key exists in the dictionary, and if it does, it asserts that the value is None. If the value is not None, it converts it to an integer using a custom function called `convert_int64`.\n\n                if n.inputs[2].__class__ == gs.Constant:\n                    name = map_name(n.name + \"_TRTBIAS\") #This code defines a function called `map_name` that takes a name as an argument and returns the mapped name if it exists in a dictionary called `name_map`. If the name is not found in the map, it returns the original name.\n                    add_to_map(refit_dict, name, n.inputs[2].values) #This code defines a function called `add_to_map` that takes two arguments: a dictionary and a value. The function checks if the key exists in the dictionary, and if it does, it asserts that the value is None. If the value is not None, it converts it to an integer using a custom function called `convert_int64`.\n\n            # For all other nodes: find node inputs that are initializers (AKA gs.Constant)\n            else:\n                for inp in n.inputs:\n                    name = map_name(inp.name) #This code defines a function called `map_name` that takes a name as an argument and returns the mapped name if it exists in a dictionary called `name_map`. If the name is not found in the map, it returns the original name.\n                    if inp.__class__ == gs.Constant:\n                        add_to_map(refit_dict, name, inp.values) #This code defines a function called `add_to_map` that takes two arguments: a dictionary and a value. The function checks if the key exists in the dictionary, and if it does, it asserts that the value is None. If the value is not None, it converts it to an integer using a custom function called `convert_int64`.\n\n        for layer_name, weights_role in zip(all_weights[0], all_weights[1]):\n            if weights_role == trt.WeightsRole.KERNEL:\n                custom_name = layer_name + \"_TRTKERNEL\"\n            elif weights_role == trt.WeightsRole.BIAS:\n                custom_name = layer_name + \"_TRTBIAS\"\n            else:\n                custom_name = layer_name\n\n            # Skip refitting Trilu for now; scalar weights of type int64 value 1 - for clip model\n            if layer_name.startswith(\"onnx::Trilu\"):\n                continue\n\n            if refit_dict[custom_name] is not None:\n                refitter.set_weights(layer_name, weights_role, refit_dict[custom_name])\n            else:\n                print(f\"[W] No refit weights for layer: {layer_name}\")\n\n        if not refitter.refit_cuda_engine():\n            print(\"Failed to refit!\")\n            exit(0)","documentation":"The code is a Python script that refits a TensorRT engine with new weights from an ONNX model. It maps the original weights to the refit engine and sets the new weights for each layer based on the mapping."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#convert_int64().","attributes":{"range":[101,8,105,22],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#convert_int64().","content":"def convert_int64(arr):\n            # TODO: smarter conversion\n            if len(arr.shape) == 0:\n                return np.int32(arr)\n            return arr","file":"/src/streamdiffusion/acceleration/tensorrt/utilities.py","language":"python","fileHash":"63a6abfeb186705edc5dba1605e60b5eaa1a86e848db1ddcffe555e16476075c","hash":"33dc26ff2e226541294ae6769d85787ee405ac108cd54d272671dfb7f3672630","processedContent":"def convert_int64(arr):\n            # TODO: smarter conversion\n            if len(arr.shape) == 0:\n                return np.int32(arr) #undefined\n            return arr","documentation":"This code defines a function called `convert_int64` that takes an array as input and converts it to an integer. The function first checks the shape of the array, and if it is empty, it returns a 32-bit integer. Otherwise, it returns the original array."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#add_to_map().","attributes":{"range":[107,8,112,41],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#add_to_map().","content":"def add_to_map(refit_dict, name, values):\n            if name in refit_dict:\n                assert refit_dict[name] is None\n                if values.dtype == np.int64:\n                    values = convert_int64(values)\n                refit_dict[name] = values","file":"/src/streamdiffusion/acceleration/tensorrt/utilities.py","language":"python","fileHash":"63a6abfeb186705edc5dba1605e60b5eaa1a86e848db1ddcffe555e16476075c","hash":"0842b9b0e952ab6231ae769a867b227f3613b61e0a5ae92d238fda5d455feb90","processedContent":"def add_to_map(refit_dict, name, values):\n            if name in refit_dict:\n                assert refit_dict[name] is None\n                if values.dtype == np.int64: #undefined\n                    values = convert_int64(values) #This code defines a function called `convert_int64` that takes an array as input and converts it to an integer. The function first checks the shape of the array, and if it is empty, it returns a 32-bit integer. Otherwise, it returns the original array.\n                refit_dict[name] = values","documentation":"This code defines a function called `add_to_map` that takes two arguments: a dictionary and a value. The function checks if the key exists in the dictionary, and if it does, it asserts that the value is None. If the value is not None, it converts it to an integer using a custom function called `convert_int64`."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#map_name().","attributes":{"range":[137,8,140,23],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#map_name().","content":"def map_name(name):\n            if name in name_map:\n                return name_map[name]\n            return name","file":"/src/streamdiffusion/acceleration/tensorrt/utilities.py","language":"python","fileHash":"63a6abfeb186705edc5dba1605e60b5eaa1a86e848db1ddcffe555e16476075c","hash":"1c04a43f6bfb1d9764cf3a183939228e3a96d4e7ca725cb10881ca68ab31e508","processedContent":"def map_name(name):\n            if name in name_map:\n                return name_map[name]\n            return name","documentation":"This code defines a function called `map_name` that takes a name as an argument and returns the mapped name if it exists in a dictionary called `name_map`. If the name is not found in the map, it returns the original name."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#build().","attributes":{"range":[203,4,234,50],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#build().","content":"def build(\n        self,\n        onnx_path,\n        fp16,\n        input_profile=None,\n        enable_refit=False,\n        enable_all_tactics=False,\n        timing_cache=None,\n        workspace_size=0,\n    ):\n        print(f\"Building TensorRT engine for {onnx_path}: {self.engine_path}\")\n        p = Profile()\n        if input_profile:\n            for name, dims in input_profile.items():\n                assert len(dims) == 3\n                p.add(name, min=dims[0], opt=dims[1], max=dims[2])\n\n        config_kwargs = {}\n\n        if workspace_size > 0:\n            config_kwargs[\"memory_pool_limits\"] = {trt.MemoryPoolType.WORKSPACE: workspace_size}\n        if not enable_all_tactics:\n            config_kwargs[\"tactic_sources\"] = []\n\n        engine = engine_from_network(\n            network_from_onnx_path(onnx_path, flags=[trt.OnnxParserFlag.NATIVE_INSTANCENORM]),\n            config=CreateConfig(\n                fp16=fp16, refittable=enable_refit, profiles=[p], load_timing_cache=timing_cache, **config_kwargs\n            ),\n            save_timing_cache=timing_cache,\n        )\n        save_engine(engine, path=self.engine_path)","file":"/src/streamdiffusion/acceleration/tensorrt/utilities.py","language":"python","fileHash":"63a6abfeb186705edc5dba1605e60b5eaa1a86e848db1ddcffe555e16476075c","hash":"2bf0a956df9778c3caba21b92cda92eacab1f085fd22dbccedcb6016567a66d4","processedContent":"def build(\n        self,\n        onnx_path,\n        fp16,\n        input_profile=None,\n        enable_refit=False,\n        enable_all_tactics=False,\n        timing_cache=None,\n        workspace_size=0,\n    ):\n        print(f\"Building TensorRT engine for {onnx_path}: {self.engine_path}\") #undefined\n        p = Profile()\n        if input_profile:\n            for name, dims in input_profile.items():\n                assert len(dims) == 3\n                p.add(name, min=dims[0], opt=dims[1], max=dims[2])\n\n        config_kwargs = {}\n\n        if workspace_size > 0:\n            config_kwargs[\"memory_pool_limits\"] = {trt.MemoryPoolType.WORKSPACE: workspace_size}\n        if not enable_all_tactics:\n            config_kwargs[\"tactic_sources\"] = []\n\n        engine = engine_from_network(\n            network_from_onnx_path(onnx_path, flags=[trt.OnnxParserFlag.NATIVE_INSTANCENORM]),\n            config=CreateConfig(\n                fp16=fp16, refittable=enable_refit, profiles=[p], load_timing_cache=timing_cache, **config_kwargs\n            ),\n            save_timing_cache=timing_cache,\n        )\n        save_engine(engine, path=self.engine_path)","documentation":"This code builds a TensorRT engine from an ONNX model, allowing for customization of the engine's performance through various parameters."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/decode_images().","attributes":{"range":[291,0,295,47],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/decode_images().","content":"def decode_images(images: torch.Tensor):\n    images = (\n        ((images + 1) * 255 / 2).clamp(0, 255).detach().permute(0, 2, 3, 1).round().type(torch.uint8).cpu().numpy()\n    )\n    return [Image.fromarray(x) for x in images]","file":"/src/streamdiffusion/acceleration/tensorrt/utilities.py","language":"python","fileHash":"63a6abfeb186705edc5dba1605e60b5eaa1a86e848db1ddcffe555e16476075c","hash":"dca01c7e18f1ac60ac43268c0b5bf2738d4f9a1ee6f7fd5822a073cd7dd29654","processedContent":"def decode_images(images: torch.Tensor):\n    images = (\n        ((images + 1) * 255 / 2).clamp(0, 255).detach().permute(0, 2, 3, 1).round().type(torch.uint8).cpu().numpy()\n    )\n    return [Image.fromarray(x) for x in images]","documentation":"This code takes a tensor of images and converts it into a list of PIL Image objects, each representing a single image."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/preprocess_image().","attributes":{"range":[298,0,305,33],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/preprocess_image().","content":"def preprocess_image(image: Image.Image):\n    w, h = image.size\n    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n    image = image.resize((w, h))\n    init_image = np.array(image).astype(np.float32) / 255.0\n    init_image = init_image[None].transpose(0, 3, 1, 2)\n    init_image = torch.from_numpy(init_image).contiguous()\n    return 2.0 * init_image - 1.0","file":"/src/streamdiffusion/acceleration/tensorrt/utilities.py","language":"python","fileHash":"63a6abfeb186705edc5dba1605e60b5eaa1a86e848db1ddcffe555e16476075c","hash":"3ece39482c9f07c87974f3c3e181024de19167482a3d3c1799ed5650497cb03b","processedContent":"def preprocess_image(image: Image.Image):\n\"\"\"\nscip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\nscip-python python Pillow 10.0.0 `pil.image`/Image#: undefined\n\"\"\"\n    w, h = image.size #undefined\n    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n    image = image.resize((w, h)) #undefined\n    init_image = np.array(image).astype(np.float32) / 255.0\n    \"\"\"\n    scip-python python numpy 1.25.2 `numpy.core.multiarray`/array().: undefined\n    scip-python python numpy 1.25.2 numpy/ndarray#astype().: undefined\n    scip-python python numpy 1.25.2 numpy/float32.: undefined\n    \"\"\"\n    init_image = init_image[None].transpose(0, 3, 1, 2) #undefined\n    init_image = torch.from_numpy(init_image).contiguous()\n    return 2.0 * init_image - 1.0","documentation":"This code preprocesses an image by resizing it to a multiple of 32, converting it to a numpy array, normalizing the values, and then converting it to a PyTorch tensor."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/prepare_mask_and_masked_image().","attributes":{"range":[308,0,323,29],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/prepare_mask_and_masked_image().","content":"def prepare_mask_and_masked_image(image: Image.Image, mask: Image.Image):\n    if isinstance(image, Image.Image):\n        image = np.array(image.convert(\"RGB\"))\n    image = image[None].transpose(0, 3, 1, 2)\n    image = torch.from_numpy(image).to(dtype=torch.float32).contiguous() / 127.5 - 1.0\n    if isinstance(mask, Image.Image):\n        mask = np.array(mask.convert(\"L\"))\n        mask = mask.astype(np.float32) / 255.0\n    mask = mask[None, None]\n    mask[mask < 0.5] = 0\n    mask[mask >= 0.5] = 1\n    mask = torch.from_numpy(mask).to(dtype=torch.float32).contiguous()\n\n    masked_image = image * (mask < 0.5)\n\n    return mask, masked_image","file":"/src/streamdiffusion/acceleration/tensorrt/utilities.py","language":"python","fileHash":"63a6abfeb186705edc5dba1605e60b5eaa1a86e848db1ddcffe555e16476075c","hash":"856e77277f745d13a05d155478a26f0ebd512d51219c807181446e4e03bc8227","processedContent":"def prepare_mask_and_masked_image(image: Image.Image, mask: Image.Image):\n\"\"\"\nscip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\nscip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\nscip-python python Pillow 10.0.0 `pil.image`/Image#: undefined\nscip-python python Pillow 10.0.0 `pil.image`/Image#: undefined\n\"\"\"\n    if isinstance(image, Image.Image):\n    \"\"\"\n    scip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\n    scip-python python Pillow 10.0.0 `pil.image`/Image#: undefined\n    \"\"\"\n        image = np.array(image.convert(\"RGB\"))\n        \"\"\"\n        scip-python python numpy 1.25.2 `numpy.core.multiarray`/array().: undefined\n        scip-python python Pillow 10.0.0 `pil.image`/Image#convert().: undefined\n        \"\"\"\n    image = image[None].transpose(0, 3, 1, 2)\n    image = torch.from_numpy(image).to(dtype=torch.float32).contiguous() / 127.5 - 1.0\n    if isinstance(mask, Image.Image):\n    \"\"\"\n    scip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\n    scip-python python Pillow 10.0.0 `pil.image`/Image#: undefined\n    \"\"\"\n        mask = np.array(mask.convert(\"L\"))\n        \"\"\"\n        scip-python python numpy 1.25.2 `numpy.core.multiarray`/array().: undefined\n        scip-python python Pillow 10.0.0 `pil.image`/Image#convert().: undefined\n        \"\"\"\n        mask = mask.astype(np.float32) / 255.0 #undefined\n    mask = mask[None, None]\n    mask[mask < 0.5] = 0\n    mask[mask >= 0.5] = 1\n    mask = torch.from_numpy(mask).to(dtype=torch.float32).contiguous()\n\n    masked_image = image * (mask < 0.5)\n\n    return mask, masked_image","documentation":"The code prepares an image and its mask for a deep learning model by converting the image to a numpy array, transposing it, normalizing it, and creating a binary mask from the original mask."}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/create_models().","attributes":{"range":[326,0,362,17],"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/create_models().","content":"def create_models(\n    model_id: str,\n    use_auth_token: Optional[str],\n    device: Union[str, torch.device],\n    max_batch_size: int,\n    unet_in_channels: int = 4,\n    embedding_dim: int = 768,\n):\n    models = {\n        \"clip\": CLIP(\n            hf_token=use_auth_token,\n            device=device,\n            max_batch_size=max_batch_size,\n            embedding_dim=embedding_dim,\n        ),\n        \"unet\": UNet(\n            hf_token=use_auth_token,\n            fp16=True,\n            device=device,\n            max_batch_size=max_batch_size,\n            embedding_dim=embedding_dim,\n            unet_dim=unet_in_channels,\n        ),\n        \"vae\": VAE(\n            hf_token=use_auth_token,\n            device=device,\n            max_batch_size=max_batch_size,\n            embedding_dim=embedding_dim,\n        ),\n        \"vae_encoder\": VAEEncoder(\n            hf_token=use_auth_token,\n            device=device,\n            max_batch_size=max_batch_size,\n            embedding_dim=embedding_dim,\n        ),\n    }\n    return models","file":"/src/streamdiffusion/acceleration/tensorrt/utilities.py","language":"python","fileHash":"63a6abfeb186705edc5dba1605e60b5eaa1a86e848db1ddcffe555e16476075c","hash":"f49a61cbfd4d556638e2cd73a37a81d61828e035cc07fd624b0292012187160d","processedContent":"def create_models(\n    model_id: str,\n    use_auth_token: Optional[str], #undefined\n    device: Union[str, torch.device], #undefined\n    max_batch_size: int,\n    unet_in_channels: int = 4,\n    embedding_dim: int = 768,\n):\n    models = {\n        \"clip\": CLIP( #This code defines a class called \"SD Model\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested.\n            hf_token=use_auth_token,\n            device=device,\n            max_batch_size=max_batch_size,\n            embedding_dim=embedding_dim,\n        ),\n        \"unet\": UNet( #This code defines a class called \"SD Model\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested.\n            hf_token=use_auth_token,\n            fp16=True,\n            device=device,\n            max_batch_size=max_batch_size,\n            embedding_dim=embedding_dim,\n            unet_dim=unet_in_channels,\n        ),\n        \"vae\": VAE( #This code defines a class called \"VAE decoder\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested.\n            hf_token=use_auth_token,\n            device=device,\n            max_batch_size=max_batch_size,\n            embedding_dim=embedding_dim,\n        ),\n        \"vae_encoder\": VAEEncoder( #This code defines a class called \"VAEEncoder\" that is used for image-text matching. It has various functions, including `get_model`, `get_input_names`, and `get_output_names`, which are used to retrieve information about the model and its inputs and outputs. Additionally, it has a function called `optimize` that optimizes an ONNX graph and exports it as an ONNX model if requested.\n            hf_token=use_auth_token,\n            device=device,\n            max_batch_size=max_batch_size,\n            embedding_dim=embedding_dim,\n        ),\n    }\n    return models","documentation":"This code defines a function called `create_models` that takes in several parameters and returns a dictionary of models. The models are defined as classes with various functions, including `get_model`, `get_input_names`, and `get_output_names`. Additionally, the function has an `optimize` function that optimizes an ONNX graph and exports it as an ONNX model if requested."}},{"key":"scip-python python python-stdlib 3.11 collections/__init__:","attributes":{"symbol":"scip-python python python-stdlib 3.11 collections/__init__:","language":"python"}},{"key":"scip-python python python-stdlib 3.11 collections/OrderedDict#","attributes":{"symbol":"scip-python python python-stdlib 3.11 collections/OrderedDict#","language":"python"}},{"key":"scip-python python temp indexer cuda/__init__:","attributes":{"symbol":"scip-python python temp indexer cuda/__init__:","language":"python"}},{"key":"scip-python python temp indexer `polygraphy.backend.common`/__init__:","attributes":{"symbol":"scip-python python temp indexer `polygraphy.backend.common`/__init__:","language":"python"}},{"key":"scip-python python temp indexer `polygraphy.backend.trt`/__init__:","attributes":{"symbol":"scip-python python temp indexer `polygraphy.backend.trt`/__init__:","language":"python"}},{"key":"scip-python python numpy 1.25.2 numpy/uint8.","attributes":{"symbol":"scip-python python numpy 1.25.2 numpy/uint8.","language":"python"}},{"key":"scip-python python numpy 1.25.2 numpy/int8.","attributes":{"symbol":"scip-python python numpy 1.25.2 numpy/int8.","language":"python"}},{"key":"scip-python python numpy 1.25.2 numpy/int16.","attributes":{"symbol":"scip-python python numpy 1.25.2 numpy/int16.","language":"python"}},{"key":"scip-python python numpy 1.25.2 numpy/int32.","attributes":{"symbol":"scip-python python numpy 1.25.2 numpy/int32.","language":"python"}},{"key":"scip-python python numpy 1.25.2 numpy/int64.","attributes":{"symbol":"scip-python python numpy 1.25.2 numpy/int64.","language":"python"}},{"key":"scip-python python numpy 1.25.2 numpy/float16.","attributes":{"symbol":"scip-python python numpy 1.25.2 numpy/float16.","language":"python"}},{"key":"scip-python python numpy 1.25.2 numpy/float32.","attributes":{"symbol":"scip-python python numpy 1.25.2 numpy/float32.","language":"python"}},{"key":"scip-python python numpy 1.25.2 numpy/float64.","attributes":{"symbol":"scip-python python numpy 1.25.2 numpy/float64.","language":"python"}},{"key":"scip-python python numpy 1.25.2 numpy/complex64.","attributes":{"symbol":"scip-python python numpy 1.25.2 numpy/complex64.","language":"python"}},{"key":"scip-python python numpy 1.25.2 numpy/complex128.","attributes":{"symbol":"scip-python python numpy 1.25.2 numpy/complex128.","language":"python"}},{"key":"scip-python python numpy 1.25.2 `numpy.version`/__init__:","attributes":{"symbol":"scip-python python numpy 1.25.2 `numpy.version`/__init__:","language":"python"}},{"key":"scip-python python numpy 1.25.2 `numpy.version`/full_version.","attributes":{"symbol":"scip-python python numpy 1.25.2 `numpy.version`/full_version.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/numpy_to_torch_dtype_dict.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/numpy_to_torch_dtype_dict.","language":"python"}},{"key":"scip-python python numpy 1.25.2 numpy/bool_#","attributes":{"symbol":"scip-python python numpy 1.25.2 numpy/bool_#","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#buffers.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#buffers.","language":"python"}},{"key":"scip-python python python-stdlib 3.11 collections/OrderedDict#values().","attributes":{"symbol":"scip-python python python-stdlib 3.11 collections/OrderedDict#values().","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#engine.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#engine.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#context.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#context.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#tensors.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#tensors.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/TRT_LOGGER.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/TRT_LOGGER.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#engine_path.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#engine_path.","language":"python"}},{"key":"scip-python python python-stdlib 3.11 collections/OrderedDict#items().","attributes":{"symbol":"scip-python python python-stdlib 3.11 collections/OrderedDict#items().","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#cuda_graph_instance.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#cuda_graph_instance.","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#graph.","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#graph.","language":"python"}},{"key":"scip-python python Pillow 10.0.0 `pil.image`/Image#size().","attributes":{"symbol":"scip-python python Pillow 10.0.0 `pil.image`/Image#size().","language":"python"}},{"key":"scip-python python numpy 1.25.2 numpy/ndarray#transpose().","attributes":{"symbol":"scip-python python numpy 1.25.2 numpy/ndarray#transpose().","language":"python"}},{"key":"scip-python python temp indexer `src.streamdiffusion.tools`/__init__:","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.tools`/__init__:","range":[0,0,0,0],"content":"","file":"/src/streamdiffusion/tools/__init__.py","language":"python","fileHash":"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855","hash":"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855","processedContent":""}},{"key":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/get_cuda_version_from_torch().","attributes":{"range":[8,0,14,43],"symbol":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/get_cuda_version_from_torch().","content":"def get_cuda_version_from_torch() -> Optional[Literal[\"11\", \"12\"]]:\n    try:\n        import torch\n    except ImportError:\n        return None\n\n    return torch.version.cuda.split(\".\")[0]","file":"/src/streamdiffusion/tools/install-tensorrt.py","language":"python","fileHash":"7c335b244ff920c2cfc6760e9ad16dc05dbbc0d9c8cc80c21862f679421c2faf","hash":"24abceb7130a407f95e476c41aec2e51f6ae0eefb8627a7742b477fd88bfc846","processedContent":"def get_cuda_version_from_torch() -> Optional[Literal[\"11\", \"12\"]]:\n\"\"\"\nscip-python python python-stdlib 3.11 typing/Optional.: undefined\nscip-python python python-stdlib 3.11 typing/Literal.: undefined\n\"\"\"\n    try:\n        import torch\n    except ImportError:\n        return None\n\n    return torch.version.cuda.split(\".\")[0]","documentation":"This code checks if the PyTorch library is installed and, if so, returns the major version of the CUDA driver that it uses."}},{"key":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/install().","attributes":{"range":[17,0,44,8],"symbol":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/install().","content":"def install(cu: Optional[Literal[\"11\", \"12\"]] = get_cuda_version_from_torch()):\n    if cu is None or cu not in [\"11\", \"12\"]:\n        print(\"Could not detect CUDA version. Please specify manually.\")\n        return\n    print(\"Installing TensorRT requirements...\")\n\n    if is_installed(\"tensorrt\"):\n        if version(\"tensorrt\") < Version(\"9.0.0\"):\n            run_pip(\"uninstall -y tensorrt\")\n\n    cudnn_name = f\"nvidia-cudnn-cu{cu}==8.9.4.25\"\n\n    if not is_installed(\"tensorrt\"):\n        run_pip(f\"install {cudnn_name} --no-cache-dir\")\n        run_pip(\n            \"install --pre --extra-index-url https://pypi.nvidia.com tensorrt==9.0.1.post11.dev4 --no-cache-dir\"\n        )\n\n    if not is_installed(\"polygraphy\"):\n        run_pip(\n            \"install polygraphy==0.47.1 --extra-index-url https://pypi.ngc.nvidia.com\"\n        )\n    if not is_installed(\"onnx_graphsurgeon\"):\n        run_pip(\n            \"install onnx-graphsurgeon==0.3.26 --extra-index-url https://pypi.ngc.nvidia.com\"\n        )\n\n    pass","file":"/src/streamdiffusion/tools/install-tensorrt.py","language":"python","fileHash":"7c335b244ff920c2cfc6760e9ad16dc05dbbc0d9c8cc80c21862f679421c2faf","hash":"5910b52bbe2608c2f763098226531ee67f56a434b61fed1821ac94ee8542dc79","processedContent":"def install(cu: Optional[Literal[\"11\", \"12\"]] = get_cuda_version_from_torch()):\n\"\"\"\nscip-python python python-stdlib 3.11 typing/Optional.: undefined\nscip-python python python-stdlib 3.11 typing/Literal.: undefined\nscip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/get_cuda_version_from_torch().: This code checks if the PyTorch library is installed and, if so, returns the major version of the CUDA driver that it uses.\n\"\"\"\n    if cu is None or cu not in [\"11\", \"12\"]:\n        print(\"Could not detect CUDA version. Please specify manually.\")\n        return\n    print(\"Installing TensorRT requirements...\")\n\n    if is_installed(\"tensorrt\"): #This code checks if a package is installed by attempting to import it and returning True if the import succeeds, False otherwise.\n        if version(\"tensorrt\") < Version(\"9.0.0\"):\n        \"\"\"\n        scip-python python temp indexer `src.streamdiffusion.pip_utils`/version().: This code defines a function called `version` that takes a string argument representing a package name and returns the version of that package if it is installed, or None if it is not.\n        scip-python python packaging 23.1 `packaging.version`/Version#: undefined\n        \"\"\"\n            run_pip(\"uninstall -y tensorrt\") #This code defines a function called `run_pip` that runs the Python package manager (pip) with a given command and environment variables.\n\n    cudnn_name = f\"nvidia-cudnn-cu{cu}==8.9.4.25\"\n\n    if not is_installed(\"tensorrt\"): #This code checks if a package is installed by attempting to import it and returning True if the import succeeds, False otherwise.\n        run_pip(f\"install {cudnn_name} --no-cache-dir\") #This code defines a function called `run_pip` that runs the Python package manager (pip) with a given command and environment variables.\n        run_pip( #This code defines a function called `run_pip` that runs the Python package manager (pip) with a given command and environment variables.\n            \"install --pre --extra-index-url https://pypi.nvidia.com tensorrt==9.0.1.post11.dev4 --no-cache-dir\"\n        )\n\n    if not is_installed(\"polygraphy\"): #This code checks if a package is installed by attempting to import it and returning True if the import succeeds, False otherwise.\n        run_pip( #This code defines a function called `run_pip` that runs the Python package manager (pip) with a given command and environment variables.\n            \"install polygraphy==0.47.1 --extra-index-url https://pypi.ngc.nvidia.com\"\n        )\n    if not is_installed(\"onnx_graphsurgeon\"): #This code checks if a package is installed by attempting to import it and returning True if the import succeeds, False otherwise.\n        run_pip( #This code defines a function called `run_pip` that runs the Python package manager (pip) with a given command and environment variables.\n            \"install onnx-graphsurgeon==0.3.26 --extra-index-url https://pypi.ngc.nvidia.com\"\n        )\n\n    pass","documentation":"This code installs the necessary packages for TensorRT, including CUDA and cuDNN, using pip. It also checks if the packages are already installed and updates them if necessary."}},{"key":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/__init__:","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/__init__:","range":[0,0,49,0],"content":"from typing import Literal, Optional\n\nimport fire\nfrom packaging.version import Version\n\nfrom ..pip_utils import is_installed, run_pip, version\n\n\ndef get_cuda_version_from_torch() -> Optional[Literal[\"11\", \"12\"]]:\n    try:\n        import torch\n    except ImportError:\n        return None\n\n    return torch.version.cuda.split(\".\")[0]\n\n\ndef install(cu: Optional[Literal[\"11\", \"12\"]] = get_cuda_version_from_torch()):\n    if cu is None or cu not in [\"11\", \"12\"]:\n        print(\"Could not detect CUDA version. Please specify manually.\")\n        return\n    print(\"Installing TensorRT requirements...\")\n\n    if is_installed(\"tensorrt\"):\n        if version(\"tensorrt\") < Version(\"9.0.0\"):\n            run_pip(\"uninstall -y tensorrt\")\n\n    cudnn_name = f\"nvidia-cudnn-cu{cu}==8.9.4.25\"\n\n    if not is_installed(\"tensorrt\"):\n        run_pip(f\"install {cudnn_name} --no-cache-dir\")\n        run_pip(\n            \"install --pre --extra-index-url https://pypi.nvidia.com tensorrt==9.0.1.post11.dev4 --no-cache-dir\"\n        )\n\n    if not is_installed(\"polygraphy\"):\n        run_pip(\n            \"install polygraphy==0.47.1 --extra-index-url https://pypi.ngc.nvidia.com\"\n        )\n    if not is_installed(\"onnx_graphsurgeon\"):\n        run_pip(\n            \"install onnx-graphsurgeon==0.3.26 --extra-index-url https://pypi.ngc.nvidia.com\"\n        )\n\n    pass\n\n\nif __name__ == \"__main__\":\n    fire.Fire(install)\n","file":"/src/streamdiffusion/tools/install-tensorrt.py","language":"python","fileHash":"7c335b244ff920c2cfc6760e9ad16dc05dbbc0d9c8cc80c21862f679421c2faf","hash":"7c335b244ff920c2cfc6760e9ad16dc05dbbc0d9c8cc80c21862f679421c2faf","processedContent":"from typing import Literal, Optional\n\"\"\"\nscip-python python python-stdlib 3.11 typing/__init__:: undefined\nscip-python python python-stdlib 3.11 typing/Literal.: undefined\nscip-python python python-stdlib 3.11 typing/Optional.: undefined\n\"\"\"\n\nimport fire\nfrom packaging.version import Version\n\"\"\"\nscip-python python packaging 23.1 `packaging.version`/__init__:: undefined\nscip-python python packaging 23.1 `packaging.version`/Version#: undefined\n\"\"\"\n\nfrom ..pip_utils import is_installed, run_pip, version\n\"\"\"\nscip-python python temp indexer `src.streamdiffusion.pip_utils`/__init__:: The code defines a set of functions for working with Python packages, including `version`, `is_installed`, and `run_python` and `run_pip`. These functions use the `subprocess` module to run commands in a subprocess and capture their output.\nscip-python python temp indexer `src.streamdiffusion.pip_utils`/is_installed().: This code checks if a package is installed by attempting to import it and returning True if the import succeeds, False otherwise.\nscip-python python temp indexer `src.streamdiffusion.pip_utils`/run_pip().: This code defines a function called `run_pip` that runs the Python package manager (pip) with a given command and environment variables.\nscip-python python temp indexer `src.streamdiffusion.pip_utils`/version().: This code defines a function called `version` that takes a string argument representing a package name and returns the version of that package if it is installed, or None if it is not.\n\"\"\"\n\n\ndef get_cuda_version_from_torch() -> Optional[Literal[\"11\", \"12\"]]:\n    \"\"\"This code checks if the PyTorch library is installed and, if so, returns the major version of the CUDA driver that it uses.\"\"\"\n    pass\n\n\ndef install(cu: Optional[Literal[\"11\", \"12\"]] = get_cuda_version_from_torch()):\n    \"\"\"This code installs the necessary packages for TensorRT, including CUDA and cuDNN, using pip. It also checks if the packages are already installed and updates them if necessary.\"\"\"\n    pass\n\n\nif __name__ == \"__main__\": #undefined\n    fire.Fire(install) #This code installs the necessary packages for TensorRT, including CUDA and cuDNN, using pip. It also checks if the packages are already installed and updates them if necessary.\n","documentation":"This code defines a function called `install` that installs the necessary packages for TensorRT, including CUDA and cuDNN, using pip. It also checks if the packages are already installed and updates them if necessary."}},{"key":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/","attributes":{"symbol":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/","language":"python"}},{"key":"scip-python python temp indexer utils/__init__:","attributes":{"symbol":"scip-python python temp indexer utils/__init__:","range":[0,0,0,0],"content":"","file":"/utils/__init__.py","language":"python","fileHash":"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855","hash":"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855","processedContent":""}},{"key":"scip-python python temp indexer `utils.viewer`/update_image().","attributes":{"range":[13,0,28,26],"symbol":"scip-python python temp indexer `utils.viewer`/update_image().","content":"def update_image(image_data: Image.Image, label: tk.Label) -> None:\n    \"\"\"\n    Update the image displayed on a Tkinter label.\n\n    Parameters\n    ----------\n    image_data : Image.Image\n        The image to be displayed.\n    label : tk.Label\n        The labels where the image will be updated.\n    \"\"\"\n    width = 512\n    height = 512\n    tk_image = ImageTk.PhotoImage(image_data, size=width)\n    label.configure(image=tk_image, width=width, height=height)\n    label.image = tk_image","file":"/utils/viewer.py","language":"python","fileHash":"52fc96aae078917f44a38142f6c441efbc4f0a1da0fd6d812edcb0e01602eec5","hash":"deca042ea0440b117c39fcb61b9e703c86c5d2107e053f7ffb37c39a36f5ce71","processedContent":"def update_image(image_data: Image.Image, label: tk.Label) -> None:\n\"\"\"\nscip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\nscip-python python Pillow 10.0.0 `pil.image`/Image#: undefined\nscip-python python python-stdlib 3.11 tkinter/Label#: undefined\n\"\"\"\n    \"\"\"\n    Update the image displayed on a Tkinter label.\n\n    Parameters\n    ----------\n    image_data : Image.Image\n        The image to be displayed.\n    label : tk.Label\n        The labels where the image will be updated.\n    \"\"\"\n    width = 512\n    height = 512\n    tk_image = ImageTk.PhotoImage(image_data, size=width)\n    \"\"\"\n    scip-python python Pillow 10.0.0 `PIL.ImageTk`/__init__:: undefined\n    scip-python python Pillow 10.0.0 `pil.imagetk`/PhotoImage#: undefined\n    \"\"\"\n    label.configure(image=tk_image, width=width, height=height) #undefined\n    label.image = tk_image","documentation":"This code updates an image displayed on a Tkinter label by creating a new PhotoImage object from the input image data and configuring the label to display it."}},{"key":"scip-python python temp indexer `utils.viewer`/_receive_images().","attributes":{"range":[30,0,61,18],"symbol":"scip-python python temp indexer `utils.viewer`/_receive_images().","content":"def _receive_images(\n    queue: Queue, fps_queue: Queue, label: tk.Label, fps_label: tk.Label\n) -> None:\n    \"\"\"\n    Continuously receive images from a queue and update the labels.\n\n    Parameters\n    ----------\n    queue : Queue\n        The queue to receive images from.\n    fps_queue : Queue\n        The queue to put the calculated fps.\n    label : tk.Label\n        The label to update with images.\n    fps_label : tk.Label\n        The label to show fps.\n    \"\"\"\n    while True:\n        try:\n            if not queue.empty():\n                label.after(\n                    0,\n                    update_image,\n                    postprocess_image(queue.get(block=False), output_type=\"pil\")[0],\n                    label,\n                )\n            if not fps_queue.empty():\n                fps_label.config(text=f\"FPS: {fps_queue.get(block=False):.2f}\")\n\n            time.sleep(0.0005)\n        except KeyboardInterrupt:\n            return","file":"/utils/viewer.py","language":"python","fileHash":"52fc96aae078917f44a38142f6c441efbc4f0a1da0fd6d812edcb0e01602eec5","hash":"ed44e439521cea6bd3ab649e71c97a45d0b0985d24ca4bab9434506daddf034b","processedContent":"def _receive_images(\n    queue: Queue, fps_queue: Queue, label: tk.Label, fps_label: tk.Label\n    \"\"\"\n    scip-python python python-stdlib 3.11 tkinter/Label#: undefined\n    scip-python python python-stdlib 3.11 tkinter/Label#: undefined\n    \"\"\"\n) -> None:\n    \"\"\"\n    Continuously receive images from a queue and update the labels.\n\n    Parameters\n    ----------\n    queue : Queue\n        The queue to receive images from.\n    fps_queue : Queue\n        The queue to put the calculated fps.\n    label : tk.Label\n        The label to update with images.\n    fps_label : tk.Label\n        The label to show fps.\n    \"\"\"\n    while True:\n        try:\n            if not queue.empty(): #undefined\n                label.after( #undefined\n                    0,\n                    update_image, #This code updates an image displayed on a Tkinter label by creating a new PhotoImage object from the input image data and configuring the label to display it.\n                    postprocess_image(queue.get(block=False), output_type=\"pil\")[0],\n                    label,\n                )\n            if not fps_queue.empty(): #undefined\n                fps_label.config(text=f\"FPS: {fps_queue.get(block=False):.2f}\") #undefined\n\n            time.sleep(0.0005)\n            \"\"\"\n            scip-python python python-stdlib 3.11 time/__init__:: undefined\n            scip-python python python-stdlib 3.11 time/sleep().: undefined\n            \"\"\"\n        except KeyboardInterrupt:\n            return","documentation":"This code continuously receives images from a queue and updates a label with the received images. It also calculates the frames per second (FPS) and updates another label with the calculated FPS."}},{"key":"scip-python python temp indexer `utils.viewer`/receive_images().on_closing().","attributes":{"range":[82,4,85,14],"symbol":"scip-python python temp indexer `utils.viewer`/receive_images().on_closing().","content":"def on_closing():\n        print(\"window closed\")\n        root.quit()  # stop event loop\n        return","file":"/utils/viewer.py","language":"python","fileHash":"52fc96aae078917f44a38142f6c441efbc4f0a1da0fd6d812edcb0e01602eec5","hash":"25154956b2e5afba3ba3bd9e183867629a41efc931f79cfb60c0a49c8265ee1f","processedContent":"def on_closing():\n        print(\"window closed\")\n        root.quit()  # stop event loop\n        return","documentation":"This code defines a function called `on_closing` that is triggered when the user closes the window. It prints a message to the console and stops the event loop, effectively closing the application."}},{"key":"scip-python python python-stdlib 3.11 tkinter/Wm#protocol.","attributes":{"symbol":"scip-python python python-stdlib 3.11 tkinter/Wm#protocol.","language":"python"}},{"key":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__init__().","attributes":{"range":[21,4,171,120],"symbol":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__init__().","content":"def __init__(\n        self,\n        model_id_or_path: str,\n        t_index_list: List[int],\n        lora_dict: Optional[Dict[str, float]] = None,\n        mode: Literal[\"img2img\", \"txt2img\"] = \"img2img\",\n        output_type: Literal[\"pil\", \"pt\", \"np\", \"latent\"] = \"pil\",\n        lcm_lora_id: Optional[str] = None,\n        vae_id: Optional[str] = None,\n        device: Literal[\"cpu\", \"cuda\"] = \"cuda\",\n        dtype: torch.dtype = torch.float16,\n        frame_buffer_size: int = 1,\n        width: int = 512,\n        height: int = 512,\n        warmup: int = 10,\n        acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"tensorrt\",\n        do_add_noise: bool = True,\n        device_ids: Optional[List[int]] = None,\n        use_lcm_lora: bool = True,\n        use_tiny_vae: bool = True,\n        enable_similar_image_filter: bool = False,\n        similar_image_filter_threshold: float = 0.98,\n        similar_image_filter_max_skip_frame: int = 10,\n        use_denoising_batch: bool = True,\n        cfg_type: Literal[\"none\", \"full\", \"self\", \"initialize\"] = \"self\",\n        seed: int = 2,\n        use_safety_checker: bool = False,\n    ):\n        \"\"\"\n        Initializes the StreamDiffusionWrapper.\n\n        Parameters\n        ----------\n        model_id_or_path : str\n            The model id or path to load.\n        t_index_list : List[int]\n            The t_index_list to use for inference.\n        lora_dict : Optional[Dict[str, float]], optional\n            The lora_dict to load, by default None.\n            Keys are the LoRA names and values are the LoRA scales.\n            Example: {\"LoRA_1\" : 0.5 , \"LoRA_2\" : 0.7 ,...}\n        mode : Literal[\"img2img\", \"txt2img\"], optional\n            txt2img or img2img, by default \"img2img\".\n        output_type : Literal[\"pil\", \"pt\", \"np\", \"latent\"], optional\n            The output type of image, by default \"pil\".\n        lcm_lora_id : Optional[str], optional\n            The lcm_lora_id to load, by default None.\n            If None, the default LCM-LoRA\n            (\"latent-consistency/lcm-lora-sdv1-5\") will be used.\n        vae_id : Optional[str], optional\n            The vae_id to load, by default None.\n            If None, the default TinyVAE\n            (\"madebyollin/taesd\") will be used.\n        device : Literal[\"cpu\", \"cuda\"], optional\n            The device to use for inference, by default \"cuda\".\n        dtype : torch.dtype, optional\n            The dtype for inference, by default torch.float16.\n        frame_buffer_size : int, optional\n            The frame buffer size for denoising batch, by default 1.\n        width : int, optional\n            The width of the image, by default 512.\n        height : int, optional\n            The height of the image, by default 512.\n        warmup : int, optional\n            The number of warmup steps to perform, by default 10.\n        acceleration : Literal[\"none\", \"xformers\", \"tensorrt\"], optional\n            The acceleration method, by default \"tensorrt\".\n        do_add_noise : bool, optional\n            Whether to add noise for following denoising steps or not,\n            by default True.\n        device_ids : Optional[List[int]], optional\n            The device ids to use for DataParallel, by default None.\n        use_lcm_lora : bool, optional\n            Whether to use LCM-LoRA or not, by default True.\n        use_tiny_vae : bool, optional\n            Whether to use TinyVAE or not, by default True.\n        enable_similar_image_filter : bool, optional\n            Whether to enable similar image filter or not,\n            by default False.\n        similar_image_filter_threshold : float, optional\n            The threshold for similar image filter, by default 0.98.\n        similar_image_filter_max_skip_frame : int, optional\n            The max skip frame for similar image filter, by default 10.\n        use_denoising_batch : bool, optional\n            Whether to use denoising batch or not, by default True.\n        cfg_type : Literal[\"none\", \"full\", \"self\", \"initialize\"],\n        optional\n            The cfg_type for img2img mode, by default \"self\".\n            You cannot use anything other than \"none\" for txt2img mode.\n        seed : int, optional\n            The seed, by default 2.\n        use_safety_checker : bool, optional\n            Whether to use safety checker or not, by default False.\n        \"\"\"\n        self.sd_turbo = \"turbo\" in model_id_or_path\n\n        if mode == \"txt2img\":\n            if cfg_type != \"none\":\n                raise ValueError(\n                    f\"txt2img mode accepts only cfg_type = 'none', but got {cfg_type}\"\n                )\n            if use_denoising_batch and frame_buffer_size > 1:\n                if not self.sd_turbo:\n                    raise ValueError(\n                        \"txt2img mode cannot use denoising batch with frame_buffer_size > 1.\"\n                    )\n\n        if mode == \"img2img\":\n            if not use_denoising_batch:\n                raise NotImplementedError(\n                    \"img2img mode must use denoising batch for now.\"\n                )\n\n        self.device = device\n        self.dtype = dtype\n        self.width = width\n        self.height = height\n        self.mode = mode\n        self.output_type = output_type\n        self.frame_buffer_size = frame_buffer_size\n        self.batch_size = (\n            len(t_index_list) * frame_buffer_size\n            if use_denoising_batch\n            else frame_buffer_size\n        )\n\n        self.use_denoising_batch = use_denoising_batch\n        self.use_safety_checker = use_safety_checker\n\n        self.stream: StreamDiffusion = self._load_model(\n            model_id_or_path=model_id_or_path,\n            lora_dict=lora_dict,\n            lcm_lora_id=lcm_lora_id,\n            vae_id=vae_id,\n            t_index_list=t_index_list,\n            acceleration=acceleration,\n            warmup=warmup,\n            do_add_noise=do_add_noise,\n            use_lcm_lora=use_lcm_lora,\n            use_tiny_vae=use_tiny_vae,\n            cfg_type=cfg_type,\n            seed=seed,\n        )\n\n        if device_ids is not None:\n            self.stream.unet = torch.nn.DataParallel(\n                self.stream.unet, device_ids=device_ids\n            )\n\n        if enable_similar_image_filter:\n            self.stream.enable_similar_image_filter(similar_image_filter_threshold, similar_image_filter_max_skip_frame)","file":"/utils/wrapper.py","language":"python","fileHash":"4fc2c1f8d34f59a019136fa442db9ed0ff27859512221442ea1df929458d10e7","hash":"5df3d8c1256d39abef9620df62c66692050050d7e137880b6fe8afbd3b1a3a4a","processedContent":"def __init__(\n        self,\n        model_id_or_path: str,\n        t_index_list: List[int], #undefined\n        lora_dict: Optional[Dict[str, float]] = None,\n        \"\"\"\n        scip-python python python-stdlib 3.11 typing/Optional.: undefined\n        scip-python python python-stdlib 3.11 typing/Dict.: undefined\n        \"\"\"\n        mode: Literal[\"img2img\", \"txt2img\"] = \"img2img\", #undefined\n        output_type: Literal[\"pil\", \"pt\", \"np\", \"latent\"] = \"pil\", #undefined\n        lcm_lora_id: Optional[str] = None, #undefined\n        vae_id: Optional[str] = None, #undefined\n        device: Literal[\"cpu\", \"cuda\"] = \"cuda\", #undefined\n        dtype: torch.dtype = torch.float16,\n        frame_buffer_size: int = 1,\n        width: int = 512,\n        height: int = 512,\n        warmup: int = 10,\n        acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"tensorrt\", #undefined\n        do_add_noise: bool = True,\n        device_ids: Optional[List[int]] = None,\n        \"\"\"\n        scip-python python python-stdlib 3.11 typing/List.: undefined\n        scip-python python python-stdlib 3.11 typing/Optional.: undefined\n        \"\"\"\n        use_lcm_lora: bool = True,\n        use_tiny_vae: bool = True,\n        enable_similar_image_filter: bool = False,\n        similar_image_filter_threshold: float = 0.98,\n        similar_image_filter_max_skip_frame: int = 10,\n        use_denoising_batch: bool = True,\n        cfg_type: Literal[\"none\", \"full\", \"self\", \"initialize\"] = \"self\", #undefined\n        seed: int = 2,\n        use_safety_checker: bool = False,\n    ):\n        \"\"\"\n        Initializes the StreamDiffusionWrapper.\n\n        Parameters\n        ----------\n        model_id_or_path : str\n            The model id or path to load.\n        t_index_list : List[int]\n            The t_index_list to use for inference.\n        lora_dict : Optional[Dict[str, float]], optional\n            The lora_dict to load, by default None.\n            Keys are the LoRA names and values are the LoRA scales.\n            Example: {\"LoRA_1\" : 0.5 , \"LoRA_2\" : 0.7 ,...}\n        mode : Literal[\"img2img\", \"txt2img\"], optional\n            txt2img or img2img, by default \"img2img\".\n        output_type : Literal[\"pil\", \"pt\", \"np\", \"latent\"], optional\n            The output type of image, by default \"pil\".\n        lcm_lora_id : Optional[str], optional\n            The lcm_lora_id to load, by default None.\n            If None, the default LCM-LoRA\n            (\"latent-consistency/lcm-lora-sdv1-5\") will be used.\n        vae_id : Optional[str], optional\n            The vae_id to load, by default None.\n            If None, the default TinyVAE\n            (\"madebyollin/taesd\") will be used.\n        device : Literal[\"cpu\", \"cuda\"], optional\n            The device to use for inference, by default \"cuda\".\n        dtype : torch.dtype, optional\n            The dtype for inference, by default torch.float16.\n        frame_buffer_size : int, optional\n            The frame buffer size for denoising batch, by default 1.\n        width : int, optional\n            The width of the image, by default 512.\n        height : int, optional\n            The height of the image, by default 512.\n        warmup : int, optional\n            The number of warmup steps to perform, by default 10.\n        acceleration : Literal[\"none\", \"xformers\", \"tensorrt\"], optional\n            The acceleration method, by default \"tensorrt\".\n        do_add_noise : bool, optional\n            Whether to add noise for following denoising steps or not,\n            by default True.\n        device_ids : Optional[List[int]], optional\n            The device ids to use for DataParallel, by default None.\n        use_lcm_lora : bool, optional\n            Whether to use LCM-LoRA or not, by default True.\n        use_tiny_vae : bool, optional\n            Whether to use TinyVAE or not, by default True.\n        enable_similar_image_filter : bool, optional\n            Whether to enable similar image filter or not,\n            by default False.\n        similar_image_filter_threshold : float, optional\n            The threshold for similar image filter, by default 0.98.\n        similar_image_filter_max_skip_frame : int, optional\n            The max skip frame for similar image filter, by default 10.\n        use_denoising_batch : bool, optional\n            Whether to use denoising batch or not, by default True.\n        cfg_type : Literal[\"none\", \"full\", \"self\", \"initialize\"],\n        optional\n            The cfg_type for img2img mode, by default \"self\".\n            You cannot use anything other than \"none\" for txt2img mode.\n        seed : int, optional\n            The seed, by default 2.\n        use_safety_checker : bool, optional\n            Whether to use safety checker or not, by default False.\n        \"\"\"\n        self.sd_turbo = \"turbo\" in model_id_or_path\n\n        if mode == \"txt2img\":\n            if cfg_type != \"none\":\n                raise ValueError(\n                    f\"txt2img mode accepts only cfg_type = 'none', but got {cfg_type}\"\n                )\n            if use_denoising_batch and frame_buffer_size > 1:\n                if not self.sd_turbo: #undefined\n                    raise ValueError(\n                        \"txt2img mode cannot use denoising batch with frame_buffer_size > 1.\"\n                    )\n\n        if mode == \"img2img\":\n            if not use_denoising_batch:\n                raise NotImplementedError(\n                    \"img2img mode must use denoising batch for now.\"\n                )\n\n        self.device = device\n        self.dtype = dtype\n        self.width = width\n        self.height = height\n        self.mode = mode\n        self.output_type = output_type\n        self.frame_buffer_size = frame_buffer_size\n        self.batch_size = (\n            len(t_index_list) * frame_buffer_size\n            if use_denoising_batch\n            else frame_buffer_size\n        )\n\n        self.use_denoising_batch = use_denoising_batch\n        self.use_safety_checker = use_safety_checker\n\n        self.stream: StreamDiffusion = self._load_model( #This code defines a function called `_load_model` that loads a model from a given path or ID, and returns a `StreamDiffusion` object. It also performs various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference.\n            model_id_or_path=model_id_or_path,\n            lora_dict=lora_dict,\n            lcm_lora_id=lcm_lora_id,\n            vae_id=vae_id,\n            t_index_list=t_index_list,\n            acceleration=acceleration,\n            warmup=warmup,\n            do_add_noise=do_add_noise,\n            use_lcm_lora=use_lcm_lora,\n            use_tiny_vae=use_tiny_vae,\n            cfg_type=cfg_type,\n            seed=seed,\n        )\n\n        if device_ids is not None:\n            self.stream.unet = torch.nn.DataParallel( #undefined\n                self.stream.unet, device_ids=device_ids #undefined\n            )\n\n        if enable_similar_image_filter:\n            self.stream.enable_similar_image_filter(similar_image_filter_threshold, similar_image_filter_max_skip_frame)","documentation":"This code defines a class called `StreamDiffusionWrapper` that loads a model from a given path or ID, performs various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference. It also defines various methods for performing image generation and manipulation."}},{"key":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__call__().","attributes":{"range":[204,4,227,39],"symbol":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__call__().","content":"def __call__(\n        self,\n        image: Optional[Union[str, Image.Image, torch.Tensor]] = None,\n        prompt: Optional[str] = None,\n    ) -> Union[Image.Image, List[Image.Image]]:\n        \"\"\"\n        Performs img2img or txt2img based on the mode.\n\n        Parameters\n        ----------\n        image : Optional[Union[str, Image.Image, torch.Tensor]]\n            The image to generate from.\n        prompt : Optional[str]\n            The prompt to generate images from.\n\n        Returns\n        -------\n        Union[Image.Image, List[Image.Image]]\n            The generated image.\n        \"\"\"\n        if self.mode == \"img2img\":\n            return self.img2img(image)\n        else:\n            return self.txt2img(prompt)","file":"/utils/wrapper.py","language":"python","fileHash":"4fc2c1f8d34f59a019136fa442db9ed0ff27859512221442ea1df929458d10e7","hash":"c2a06a0d4d17eeb117625cb1e334ea9a8785cf537b9e82e6ab131a01f5ccad15","processedContent":"def __call__(\n        self,\n        image: Optional[Union[str, Image.Image, torch.Tensor]] = None,\n        \"\"\"\n        scip-python python python-stdlib 3.11 typing/Union.: undefined\n        scip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\n        scip-python python Pillow 10.0.0 `pil.image`/Image#: undefined\n        scip-python python python-stdlib 3.11 typing/Optional.: undefined\n        \"\"\"\n        prompt: Optional[str] = None, #undefined\n    ) -> Union[Image.Image, List[Image.Image]]:\n    \"\"\"\n    scip-python python python-stdlib 3.11 typing/Union.: undefined\n    scip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\n    scip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\n    scip-python python Pillow 10.0.0 `pil.image`/Image#: undefined\n    scip-python python Pillow 10.0.0 `pil.image`/Image#: undefined\n    scip-python python python-stdlib 3.11 typing/List.: undefined\n    \"\"\"\n        \"\"\"\n        Performs img2img or txt2img based on the mode.\n\n        Parameters\n        ----------\n        image : Optional[Union[str, Image.Image, torch.Tensor]]\n            The image to generate from.\n        prompt : Optional[str]\n            The prompt to generate images from.\n\n        Returns\n        -------\n        Union[Image.Image, List[Image.Image]]\n            The generated image.\n        \"\"\"\n        if self.mode == \"img2img\": #undefined\n            return self.img2img(image) #This code defines a function called `img2img` that takes an image as input and returns a postprocessed image. The function preprocesses the image, passes it through a neural network for further processing, and then returns the resulting image.\n        else:\n            return self.txt2img(prompt)","documentation":"This code defines a function called `__call__` that takes an image or prompt as input and returns a postprocessed image. Depending on the mode, it either calls the `img2img` function or the `txt2img` function to generate the output."}},{"key":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#txt2img().","attributes":{"range":[229,4,264,20],"symbol":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#txt2img().","content":"def txt2img(\n        self, prompt: Optional[str] = None\n    ) -> Union[Image.Image, List[Image.Image], torch.Tensor, np.ndarray]:\n        \"\"\"\n        Performs txt2img.\n\n        Parameters\n        ----------\n        prompt : Optional[str]\n            The prompt to generate images from.\n\n        Returns\n        -------\n        Union[Image.Image, List[Image.Image]]\n            The generated image.\n        \"\"\"\n        if prompt is not None:\n            self.stream.update_prompt(prompt)\n\n        if self.sd_turbo:\n            image_tensor = self.stream.txt2img_sd_turbo(self.batch_size)\n        else:\n            image_tensor = self.stream.txt2img(self.frame_buffer_size)\n        image = self.postprocess_image(image_tensor, output_type=self.output_type)\n\n        if self.use_safety_checker:\n            safety_checker_input = self.feature_extractor(\n                image, return_tensors=\"pt\"\n            ).to(self.device)\n            _, has_nsfw_concept = self.safety_checker(\n                images=image_tensor.to(self.dtype),\n                clip_input=safety_checker_input.pixel_values.to(self.dtype),\n            )\n            image = self.nsfw_fallback_img if has_nsfw_concept[0] else image\n\n        return image","file":"/utils/wrapper.py","language":"python","fileHash":"4fc2c1f8d34f59a019136fa442db9ed0ff27859512221442ea1df929458d10e7","hash":"1701a1487e1a59cec9f7fee52cdae39c6596ce1189ed9f2cc3561d1cf45edcfc","processedContent":"def txt2img(\n        self, prompt: Optional[str] = None #undefined\n    ) -> Union[Image.Image, List[Image.Image], torch.Tensor, np.ndarray]:\n    \"\"\"\n    scip-python python python-stdlib 3.11 typing/Union.: undefined\n    scip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\n    scip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\n    scip-python python Pillow 10.0.0 `pil.image`/Image#: undefined\n    scip-python python Pillow 10.0.0 `pil.image`/Image#: undefined\n    scip-python python python-stdlib 3.11 typing/List.: undefined\n    scip-python python numpy 1.25.2 numpy/ndarray#: undefined\n    \"\"\"\n        \"\"\"\n        Performs txt2img.\n\n        Parameters\n        ----------\n        prompt : Optional[str]\n            The prompt to generate images from.\n\n        Returns\n        -------\n        Union[Image.Image, List[Image.Image]]\n            The generated image.\n        \"\"\"\n        if prompt is not None:\n            self.stream.update_prompt(prompt) #undefined\n\n        if self.sd_turbo: #undefined\n            image_tensor = self.stream.txt2img_sd_turbo(self.batch_size)\n            \"\"\"\n            scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#stream.: undefined\n            scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#batch_size.: undefined\n            \"\"\"\n        else:\n            image_tensor = self.stream.txt2img(self.frame_buffer_size)\n            \"\"\"\n            scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#stream.: undefined\n            scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#frame_buffer_size.: undefined\n            \"\"\"\n        image = self.postprocess_image(image_tensor, output_type=self.output_type)\n        \"\"\"\n        scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#postprocess_image().: This code defines a function called `postprocess_image` that takes an image tensor and returns a postprocessed image. The function checks if the frame buffer size is greater than 1, and if so, it calls itself recursively with the same input. Otherwise, it returns the first element of the output of the recursive call.\n        scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#output_type.: undefined\n        \"\"\"\n\n        if self.use_safety_checker: #undefined\n            safety_checker_input = self.feature_extractor( #undefined\n                image, return_tensors=\"pt\"\n            ).to(self.device) #undefined\n            _, has_nsfw_concept = self.safety_checker( #undefined\n                images=image_tensor.to(self.dtype), #undefined\n                clip_input=safety_checker_input.pixel_values.to(self.dtype), #undefined\n            )\n            image = self.nsfw_fallback_img if has_nsfw_concept[0] else image #undefined\n\n        return image","documentation":"The code defines a function called `txt2img` that takes a prompt as input and generates an image based on the prompt. It uses a StreamDiffusion model to generate the image, and then applies post-processing to the output."}},{"key":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#img2img().","attributes":{"range":[266,4,298,20],"symbol":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#img2img().","content":"def img2img(\n        self, image: Union[str, Image.Image, torch.Tensor]\n    ) -> Union[Image.Image, List[Image.Image], torch.Tensor, np.ndarray]:\n        \"\"\"\n        Performs img2img.\n\n        Parameters\n        ----------\n        image : Union[str, Image.Image, torch.Tensor]\n            The image to generate from.\n\n        Returns\n        -------\n        Image.Image\n            The generated image.\n        \"\"\"\n        if isinstance(image, str) or isinstance(image, Image.Image):\n            image = self.preprocess_image(image)\n\n        image_tensor = self.stream(image)\n        image = self.postprocess_image(image_tensor, output_type=self.output_type)\n\n        if self.use_safety_checker:\n            safety_checker_input = self.feature_extractor(\n                image, return_tensors=\"pt\"\n            ).to(self.device)\n            _, has_nsfw_concept = self.safety_checker(\n                images=image_tensor.to(self.dtype),\n                clip_input=safety_checker_input.pixel_values.to(self.dtype),\n            )\n            image = self.nsfw_fallback_img if has_nsfw_concept[0] else image\n\n        return image","file":"/utils/wrapper.py","language":"python","fileHash":"4fc2c1f8d34f59a019136fa442db9ed0ff27859512221442ea1df929458d10e7","hash":"8394914f1fd4ec17ba04315b031118bd7515d27f2efb9c382edb5dbf190e4dc6","processedContent":"def img2img(\n        self, image: Union[str, Image.Image, torch.Tensor]\n        \"\"\"\n        scip-python python python-stdlib 3.11 typing/Union.: undefined\n        scip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\n        scip-python python Pillow 10.0.0 `pil.image`/Image#: undefined\n        \"\"\"\n    ) -> Union[Image.Image, List[Image.Image], torch.Tensor, np.ndarray]:\n    \"\"\"\n    scip-python python python-stdlib 3.11 typing/Union.: undefined\n    scip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\n    scip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\n    scip-python python Pillow 10.0.0 `pil.image`/Image#: undefined\n    scip-python python Pillow 10.0.0 `pil.image`/Image#: undefined\n    scip-python python python-stdlib 3.11 typing/List.: undefined\n    scip-python python numpy 1.25.2 numpy/ndarray#: undefined\n    \"\"\"\n        \"\"\"\n        Performs img2img.\n\n        Parameters\n        ----------\n        image : Union[str, Image.Image, torch.Tensor]\n            The image to generate from.\n\n        Returns\n        -------\n        Image.Image\n            The generated image.\n        \"\"\"\n        if isinstance(image, str) or isinstance(image, Image.Image):\n        \"\"\"\n        scip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\n        scip-python python Pillow 10.0.0 `pil.image`/Image#: undefined\n        \"\"\"\n            image = self.preprocess_image(image) #This code preprocesses an image by converting it to RGB and resizing it to a specific width and height. It then passes the preprocessed image through a neural network for further processing.\n\n        image_tensor = self.stream(image) #undefined\n        image = self.postprocess_image(image_tensor, output_type=self.output_type)\n        \"\"\"\n        scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#postprocess_image().: This code defines a function called `postprocess_image` that takes an image tensor and returns a postprocessed image. The function checks if the frame buffer size is greater than 1, and if so, it calls itself recursively with the same input. Otherwise, it returns the first element of the output of the recursive call.\n        scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#output_type.: undefined\n        \"\"\"\n\n        if self.use_safety_checker: #undefined\n            safety_checker_input = self.feature_extractor( #undefined\n                image, return_tensors=\"pt\"\n            ).to(self.device) #undefined\n            _, has_nsfw_concept = self.safety_checker( #undefined\n                images=image_tensor.to(self.dtype), #undefined\n                clip_input=safety_checker_input.pixel_values.to(self.dtype), #undefined\n            )\n            image = self.nsfw_fallback_img if has_nsfw_concept[0] else image #undefined\n\n        return image","documentation":"This code defines a function called `img2img` that takes an image as input and returns a postprocessed image. The function preprocesses the image, passes it through a neural network for further processing, and then returns the resulting image."}},{"key":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#postprocess_image().","attributes":{"range":[323,4,342,84],"symbol":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#postprocess_image().","content":"def postprocess_image(\n        self, image_tensor: torch.Tensor, output_type: str = \"pil\"\n    ) -> Union[Image.Image, List[Image.Image], torch.Tensor, np.ndarray]:\n        \"\"\"\n        Postprocesses the image.\n\n        Parameters\n        ----------\n        image_tensor : torch.Tensor\n            The image tensor to postprocess.\n\n        Returns\n        -------\n        Union[Image.Image, List[Image.Image]]\n            The postprocessed image.\n        \"\"\"\n        if self.frame_buffer_size > 1:\n            return postprocess_image(image_tensor.cpu(), output_type=output_type)\n        else:\n            return postprocess_image(image_tensor.cpu(), output_type=output_type)[0]","file":"/utils/wrapper.py","language":"python","fileHash":"4fc2c1f8d34f59a019136fa442db9ed0ff27859512221442ea1df929458d10e7","hash":"6e454da471c365d8346f41bc10cde8860487e8c06261fc8e581dc3e0a677f11a","processedContent":"def postprocess_image(\n        self, image_tensor: torch.Tensor, output_type: str = \"pil\"\n    ) -> Union[Image.Image, List[Image.Image], torch.Tensor, np.ndarray]:\n    \"\"\"\n    scip-python python python-stdlib 3.11 typing/Union.: undefined\n    scip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\n    scip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\n    scip-python python Pillow 10.0.0 `pil.image`/Image#: undefined\n    scip-python python Pillow 10.0.0 `pil.image`/Image#: undefined\n    scip-python python python-stdlib 3.11 typing/List.: undefined\n    scip-python python numpy 1.25.2 numpy/ndarray#: undefined\n    \"\"\"\n        \"\"\"\n        Postprocesses the image.\n\n        Parameters\n        ----------\n        image_tensor : torch.Tensor\n            The image tensor to postprocess.\n\n        Returns\n        -------\n        Union[Image.Image, List[Image.Image]]\n            The postprocessed image.\n        \"\"\"\n        if self.frame_buffer_size > 1: #undefined\n            return postprocess_image(image_tensor.cpu(), output_type=output_type)\n        else:\n            return postprocess_image(image_tensor.cpu(), output_type=output_type)[0]","documentation":"This code defines a function called `postprocess_image` that takes an image tensor and returns a postprocessed image. The function checks if the frame buffer size is greater than 1, and if so, it calls itself recursively with the same input. Otherwise, it returns the first element of the output of the recursive call."}},{"key":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","attributes":{"range":[344,4,656,21],"symbol":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","content":"def _load_model(\n        self,\n        model_id_or_path: str,\n        t_index_list: List[int],\n        lora_dict: Optional[Dict[str, float]] = None,\n        lcm_lora_id: Optional[str] = None,\n        vae_id: Optional[str] = None,\n        acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"tensorrt\",\n        warmup: int = 10,\n        do_add_noise: bool = True,\n        use_lcm_lora: bool = True,\n        use_tiny_vae: bool = True,\n        cfg_type: Literal[\"none\", \"full\", \"self\", \"initialize\"] = \"self\",\n        seed: int = 2,\n    ) -> StreamDiffusion:\n        \"\"\"\n        Loads the model.\n\n        This method does the following:\n\n        1. Loads the model from the model_id_or_path.\n        2. Loads and fuses the LCM-LoRA model from the lcm_lora_id if needed.\n        3. Loads the VAE model from the vae_id if needed.\n        4. Enables acceleration if needed.\n        5. Prepares the model for inference.\n        6. Load the safety checker if needed.\n\n        Parameters\n        ----------\n        model_id_or_path : str\n            The model id or path to load.\n        t_index_list : List[int]\n            The t_index_list to use for inference.\n        lora_dict : Optional[Dict[str, float]], optional\n            The lora_dict to load, by default None.\n            Keys are the LoRA names and values are the LoRA scales.\n            Example: {\"LoRA_1\" : 0.5 , \"LoRA_2\" : 0.7 ,...}\n        lcm_lora_id : Optional[str], optional\n            The lcm_lora_id to load, by default None.\n        vae_id : Optional[str], optional\n            The vae_id to load, by default None.\n        acceleration : Literal[\"none\", \"xfomers\", \"sfast\", \"tensorrt\"], optional\n            The acceleration method, by default \"tensorrt\".\n        warmup : int, optional\n            The number of warmup steps to perform, by default 10.\n        do_add_noise : bool, optional\n            Whether to add noise for following denoising steps or not,\n            by default True.\n        use_lcm_lora : bool, optional\n            Whether to use LCM-LoRA or not, by default True.\n        use_tiny_vae : bool, optional\n            Whether to use TinyVAE or not, by default True.\n        cfg_type : Literal[\"none\", \"full\", \"self\", \"initialize\"],\n        optional\n            The cfg_type for img2img mode, by default \"self\".\n            You cannot use anything other than \"none\" for txt2img mode.\n        seed : int, optional\n            The seed, by default 2.\n\n        Returns\n        -------\n        StreamDiffusion\n            The loaded model.\n        \"\"\"\n\n        try:  # Load from local directory\n            pipe: StableDiffusionPipeline = StableDiffusionPipeline.from_pretrained(\n                model_id_or_path,\n            ).to(device=self.device, dtype=self.dtype)\n\n        except ValueError:  # Load from huggingface\n            pipe: StableDiffusionPipeline = StableDiffusionPipeline.from_single_file(\n                model_id_or_path,\n            ).to(device=self.device, dtype=self.dtype)\n        except Exception:  # No model found\n            traceback.print_exc()\n            print(\"Model load has failed. Doesn't exist.\")\n            exit()\n\n        stream = StreamDiffusion(\n            pipe=pipe,\n            t_index_list=t_index_list,\n            torch_dtype=self.dtype,\n            width=self.width,\n            height=self.height,\n            do_add_noise=do_add_noise,\n            frame_buffer_size=self.frame_buffer_size,\n            use_denoising_batch=self.use_denoising_batch,\n            cfg_type=cfg_type,\n        )\n        if not self.sd_turbo:\n            if use_lcm_lora:\n                if lcm_lora_id is not None:\n                    stream.load_lcm_lora(\n                        pretrained_model_name_or_path_or_dict=lcm_lora_id\n                    )\n                else:\n                    stream.load_lcm_lora()\n                stream.fuse_lora()\n\n            if lora_dict is not None:\n                for lora_name, lora_scale in lora_dict.items():\n                    stream.load_lora(lora_name)\n                    stream.fuse_lora(lora_scale=lora_scale)\n                    print(f\"Use LoRA: {lora_name} in weights {lora_scale}\")\n\n        if use_tiny_vae:\n            if vae_id is not None:\n                stream.vae = AutoencoderTiny.from_pretrained(vae_id).to(\n                    device=pipe.device, dtype=pipe.dtype\n                )\n            else:\n                stream.vae = AutoencoderTiny.from_pretrained(\"madebyollin/taesd\").to(\n                    device=pipe.device, dtype=pipe.dtype\n                )\n\n        try:\n            if acceleration == \"xformers\":\n                stream.pipe.enable_xformers_memory_efficient_attention()\n            if acceleration == \"tensorrt\":\n                from polygraphy import cuda\n                from streamdiffusion.acceleration.tensorrt import (\n                    TorchVAEEncoder,\n                    compile_unet,\n                    compile_vae_decoder,\n                    compile_vae_encoder,\n                )\n                from streamdiffusion.acceleration.tensorrt.engine import (\n                    AutoencoderKLEngine,\n                    UNet2DConditionModelEngine,\n                )\n                from streamdiffusion.acceleration.tensorrt.models import (\n                    VAE,\n                    UNet,\n                    VAEEncoder,\n                )\n\n                def create_prefix(\n                    model_id_or_path: str,\n                    max_batch_size: int,\n                    min_batch_size: int,\n                ):\n                    maybe_path = Path(model_id_or_path)\n                    if maybe_path.exists():\n                        return f\"{maybe_path.stem}--lcm_lora-{use_lcm_lora}--tiny_vae-{use_tiny_vae}--max_batch-{max_batch_size}--min_batch-{min_batch_size}--mode-{self.mode}\"\n                    else:\n                        return f\"{model_id_or_path}--lcm_lora-{use_lcm_lora}--tiny_vae-{use_tiny_vae}--max_batch-{max_batch_size}--min_batch-{min_batch_size}--mode-{self.mode}\"\n\n                engine_dir = os.path.join(\"engines\")\n                unet_path = os.path.join(\n                    engine_dir,\n                    create_prefix(\n                        model_id_or_path=model_id_or_path,\n                        max_batch_size=stream.trt_unet_batch_size,\n                        min_batch_size=stream.trt_unet_batch_size,\n                    ),\n                    \"unet.engine\",\n                )\n                vae_encoder_path = os.path.join(\n                    engine_dir,\n                    create_prefix(\n                        model_id_or_path=model_id_or_path,\n                        max_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                        min_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                    ),\n                    \"vae_encoder.engine\",\n                )\n                vae_decoder_path = os.path.join(\n                    engine_dir,\n                    create_prefix(\n                        model_id_or_path=model_id_or_path,\n                        max_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                        min_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                    ),\n                    \"vae_decoder.engine\",\n                )\n\n                if not os.path.exists(unet_path):\n                    os.makedirs(os.path.dirname(unet_path), exist_ok=True)\n                    unet_model = UNet(\n                        fp16=True,\n                        device=stream.device,\n                        max_batch_size=stream.trt_unet_batch_size,\n                        min_batch_size=stream.trt_unet_batch_size,\n                        embedding_dim=stream.text_encoder.config.hidden_size,\n                        unet_dim=stream.unet.config.in_channels,\n                    )\n                    compile_unet(\n                        stream.unet,\n                        unet_model,\n                        unet_path + \".onnx\",\n                        unet_path + \".opt.onnx\",\n                        unet_path,\n                        opt_batch_size=stream.trt_unet_batch_size,\n                    )\n\n                if not os.path.exists(vae_decoder_path):\n                    os.makedirs(os.path.dirname(vae_decoder_path), exist_ok=True)\n                    stream.vae.forward = stream.vae.decode\n                    vae_decoder_model = VAE(\n                        device=stream.device,\n                        max_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                        min_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                    )\n                    compile_vae_decoder(\n                        stream.vae,\n                        vae_decoder_model,\n                        vae_decoder_path + \".onnx\",\n                        vae_decoder_path + \".opt.onnx\",\n                        vae_decoder_path,\n                        opt_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                    )\n                    delattr(stream.vae, \"forward\")\n\n                if not os.path.exists(vae_encoder_path):\n                    os.makedirs(os.path.dirname(vae_encoder_path), exist_ok=True)\n                    vae_encoder = TorchVAEEncoder(stream.vae).to(torch.device(\"cuda\"))\n                    vae_encoder_model = VAEEncoder(\n                        device=stream.device,\n                        max_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                        min_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                    )\n                    compile_vae_encoder(\n                        vae_encoder,\n                        vae_encoder_model,\n                        vae_encoder_path + \".onnx\",\n                        vae_encoder_path + \".opt.onnx\",\n                        vae_encoder_path,\n                        opt_batch_size=self.batch_size\n                        if self.mode == \"txt2img\"\n                        else stream.frame_bff_size,\n                    )\n\n                cuda_steram = cuda.Stream()\n\n                vae_config = stream.vae.config\n                vae_dtype = stream.vae.dtype\n\n                stream.unet = UNet2DConditionModelEngine(\n                    unet_path, cuda_steram, use_cuda_graph=False\n                )\n                stream.vae = AutoencoderKLEngine(\n                    vae_encoder_path,\n                    vae_decoder_path,\n                    cuda_steram,\n                    stream.pipe.vae_scale_factor,\n                    use_cuda_graph=False,\n                )\n                setattr(stream.vae, \"config\", vae_config)\n                setattr(stream.vae, \"dtype\", vae_dtype)\n\n                gc.collect()\n                torch.cuda.empty_cache()\n\n                print(\"TensorRT acceleration enabled.\")\n            if acceleration == \"sfast\":\n                from streamdiffusion.acceleration.sfast import (\n                    accelerate_with_stable_fast,\n                )\n\n                stream = accelerate_with_stable_fast(stream)\n                print(\"StableFast acceleration enabled.\")\n        except Exception:\n            traceback.print_exc()\n            print(\"Acceleration has failed. Falling back to normal mode.\")\n\n        if seed < 0: # Random seed\n            seed = np.random.randint(0, 1000000)\n\n        stream.prepare(\n            \"\",\n            \"\",\n            num_inference_steps=50,\n            guidance_scale=1.1\n            if stream.cfg_type in [\"full\", \"self\", \"initialize\"]\n            else 1.0,\n            generator=torch.manual_seed(seed),\n            seed=seed,\n        )\n\n        if self.use_safety_checker:\n            from transformers import CLIPFeatureExtractor\n            from diffusers.pipelines.stable_diffusion.safety_checker import (\n                StableDiffusionSafetyChecker,\n            )\n\n            self.safety_checker = StableDiffusionSafetyChecker.from_pretrained(\n                \"CompVis/stable-diffusion-safety-checker\"\n            ).to(pipe.device)\n            self.feature_extractor = CLIPFeatureExtractor.from_pretrained(\n                \"openai/clip-vit-base-patch32\"\n            )\n            self.nsfw_fallback_img = Image.new(\"RGB\", (512, 512), (0, 0, 0))\n\n        return stream","file":"/utils/wrapper.py","language":"python","fileHash":"4fc2c1f8d34f59a019136fa442db9ed0ff27859512221442ea1df929458d10e7","hash":"f9d35d4a52fcd7cf05f979195a8bf5f657f2a588b418be7c10783d32914dc271","processedContent":"def _load_model(\n        self,\n        model_id_or_path: str,\n        t_index_list: List[int], #undefined\n        lora_dict: Optional[Dict[str, float]] = None,\n        \"\"\"\n        scip-python python python-stdlib 3.11 typing/Optional.: undefined\n        scip-python python python-stdlib 3.11 typing/Dict.: undefined\n        \"\"\"\n        lcm_lora_id: Optional[str] = None, #undefined\n        vae_id: Optional[str] = None, #undefined\n        acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"tensorrt\", #undefined\n        warmup: int = 10,\n        do_add_noise: bool = True,\n        use_lcm_lora: bool = True,\n        use_tiny_vae: bool = True,\n        cfg_type: Literal[\"none\", \"full\", \"self\", \"initialize\"] = \"self\", #undefined\n        seed: int = 2,\n    ) -> StreamDiffusion:\n        \"\"\"\n        Loads the model.\n\n        This method does the following:\n\n        1. Loads the model from the model_id_or_path.\n        2. Loads and fuses the LCM-LoRA model from the lcm_lora_id if needed.\n        3. Loads the VAE model from the vae_id if needed.\n        4. Enables acceleration if needed.\n        5. Prepares the model for inference.\n        6. Load the safety checker if needed.\n\n        Parameters\n        ----------\n        model_id_or_path : str\n            The model id or path to load.\n        t_index_list : List[int]\n            The t_index_list to use for inference.\n        lora_dict : Optional[Dict[str, float]], optional\n            The lora_dict to load, by default None.\n            Keys are the LoRA names and values are the LoRA scales.\n            Example: {\"LoRA_1\" : 0.5 , \"LoRA_2\" : 0.7 ,...}\n        lcm_lora_id : Optional[str], optional\n            The lcm_lora_id to load, by default None.\n        vae_id : Optional[str], optional\n            The vae_id to load, by default None.\n        acceleration : Literal[\"none\", \"xfomers\", \"sfast\", \"tensorrt\"], optional\n            The acceleration method, by default \"tensorrt\".\n        warmup : int, optional\n            The number of warmup steps to perform, by default 10.\n        do_add_noise : bool, optional\n            Whether to add noise for following denoising steps or not,\n            by default True.\n        use_lcm_lora : bool, optional\n            Whether to use LCM-LoRA or not, by default True.\n        use_tiny_vae : bool, optional\n            Whether to use TinyVAE or not, by default True.\n        cfg_type : Literal[\"none\", \"full\", \"self\", \"initialize\"],\n        optional\n            The cfg_type for img2img mode, by default \"self\".\n            You cannot use anything other than \"none\" for txt2img mode.\n        seed : int, optional\n            The seed, by default 2.\n\n        Returns\n        -------\n        StreamDiffusion\n            The loaded model.\n        \"\"\"\n\n        try:  # Load from local directory\n            pipe: StableDiffusionPipeline = StableDiffusionPipeline.from_pretrained(\n                model_id_or_path,\n            ).to(device=self.device, dtype=self.dtype)\n            \"\"\"\n            scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#device.: undefined\n            scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#dtype.: undefined\n            \"\"\"\n            \"\"\"\n            scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#device.: undefined\n            scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#dtype.: undefined\n            \"\"\"\n\n        except ValueError:  # Load from huggingface\n            pipe: StableDiffusionPipeline = StableDiffusionPipeline.from_single_file(\n                model_id_or_path,\n            ).to(device=self.device, dtype=self.dtype)\n        except Exception:  # No model found\n            traceback.print_exc()\n            \"\"\"\n            scip-python python python-stdlib 3.11 traceback/__init__:: undefined\n            scip-python python python-stdlib 3.11 traceback/print_exc().: undefined\n            \"\"\"\n            \"\"\"\n            scip-python python python-stdlib 3.11 traceback/__init__:: undefined\n            scip-python python python-stdlib 3.11 traceback/print_exc().: undefined\n            \"\"\"\n            print(\"Model load has failed. Doesn't exist.\")\n            exit()\n\n        stream = StreamDiffusion(\n            pipe=pipe,\n            t_index_list=t_index_list,\n            torch_dtype=self.dtype, #undefined\n            width=self.width, #undefined\n            height=self.height, #undefined\n            do_add_noise=do_add_noise,\n            frame_buffer_size=self.frame_buffer_size, #undefined\n            use_denoising_batch=self.use_denoising_batch, #undefined\n            cfg_type=cfg_type,\n        )\n        if not self.sd_turbo: #undefined\n            if use_lcm_lora:\n                if lcm_lora_id is not None:\n                    stream.load_lcm_lora(\n                        pretrained_model_name_or_path_or_dict=lcm_lora_id\n                    )\n                else:\n                    stream.load_lcm_lora()\n                stream.fuse_lora()\n\n            if lora_dict is not None:\n                for lora_name, lora_scale in lora_dict.items():\n                    stream.load_lora(lora_name)\n                    stream.fuse_lora(lora_scale=lora_scale)\n                    print(f\"Use LoRA: {lora_name} in weights {lora_scale}\")\n\n        if use_tiny_vae:\n            if vae_id is not None:\n                stream.vae = AutoencoderTiny.from_pretrained(vae_id).to(\n                    device=pipe.device, dtype=pipe.dtype\n                )\n            else:\n                stream.vae = AutoencoderTiny.from_pretrained(\"madebyollin/taesd\").to(\n                    device=pipe.device, dtype=pipe.dtype\n                )\n\n        try:\n            if acceleration == \"xformers\":\n                stream.pipe.enable_xformers_memory_efficient_attention()\n            if acceleration == \"tensorrt\":\n                from polygraphy import cuda #undefined\n                from streamdiffusion.acceleration.tensorrt import ( #undefined\n                    TorchVAEEncoder,\n                    compile_unet,\n                    compile_vae_decoder,\n                    compile_vae_encoder,\n                )\n                from streamdiffusion.acceleration.tensorrt.engine import ( #undefined\n                    AutoencoderKLEngine,\n                    UNet2DConditionModelEngine,\n                )\n                from streamdiffusion.acceleration.tensorrt.models import ( #undefined\n                    VAE,\n                    UNet,\n                    VAEEncoder,\n                )\n\n                def create_prefix(\n                    \"\"\"This code defines a function called `create_prefix` that takes three parameters: `model_id_or_path`, `max_batch_size`, and `min_batch_size`. The function checks if the `model_id_or_path` is a valid path, and if so, it returns a string with the prefix of the path, along with some additional information about the model. If the `model_id_or_path` is not a valid path, the function returns a string with the original value of the parameter.\"\"\"\n                    pass\n\n                engine_dir = os.path.join(\"engines\")\n                \"\"\"\n                scip-python python python-stdlib 3.11 os/__init__:: undefined\n                scip-python python python-stdlib 3.11 os/path.: undefined\n                scip-python python python-stdlib 3.11 ntpath/join().: undefined\n                \"\"\"\n                unet_path = os.path.join(\n                \"\"\"\n                scip-python python python-stdlib 3.11 os/__init__:: undefined\n                scip-python python python-stdlib 3.11 os/path.: undefined\n                scip-python python python-stdlib 3.11 ntpath/join().: undefined\n                \"\"\"\n                    engine_dir,\n                    create_prefix( #This code defines a function called `create_prefix` that takes three parameters: `model_id_or_path`, `max_batch_size`, and `min_batch_size`. The function checks if the `model_id_or_path` is a valid path, and if so, it returns a string with the prefix of the path, along with some additional information about the model. If the `model_id_or_path` is not a valid path, the function returns a string with the original value of the parameter.\n                        model_id_or_path=model_id_or_path,\n                        max_batch_size=stream.trt_unet_batch_size,\n                        min_batch_size=stream.trt_unet_batch_size,\n                    ),\n                    \"unet.engine\",\n                )\n                vae_encoder_path = os.path.join(\n                \"\"\"\n                scip-python python python-stdlib 3.11 os/__init__:: undefined\n                scip-python python python-stdlib 3.11 os/path.: undefined\n                scip-python python python-stdlib 3.11 ntpath/join().: undefined\n                \"\"\"\n                    engine_dir,\n                    create_prefix( #This code defines a function called `create_prefix` that takes three parameters: `model_id_or_path`, `max_batch_size`, and `min_batch_size`. The function checks if the `model_id_or_path` is a valid path, and if so, it returns a string with the prefix of the path, along with some additional information about the model. If the `model_id_or_path` is not a valid path, the function returns a string with the original value of the parameter.\n                        model_id_or_path=model_id_or_path,\n                        max_batch_size=self.batch_size #undefined\n                        if self.mode == \"txt2img\" #undefined\n                        else stream.frame_bff_size,\n                        min_batch_size=self.batch_size #undefined\n                        if self.mode == \"txt2img\" #undefined\n                        else stream.frame_bff_size,\n                    ),\n                    \"vae_encoder.engine\",\n                )\n                vae_decoder_path = os.path.join(\n                \"\"\"\n                scip-python python python-stdlib 3.11 os/__init__:: undefined\n                scip-python python python-stdlib 3.11 os/path.: undefined\n                scip-python python python-stdlib 3.11 ntpath/join().: undefined\n                \"\"\"\n                    engine_dir,\n                    create_prefix( #This code defines a function called `create_prefix` that takes three parameters: `model_id_or_path`, `max_batch_size`, and `min_batch_size`. The function checks if the `model_id_or_path` is a valid path, and if so, it returns a string with the prefix of the path, along with some additional information about the model. If the `model_id_or_path` is not a valid path, the function returns a string with the original value of the parameter.\n                        model_id_or_path=model_id_or_path,\n                        max_batch_size=self.batch_size #undefined\n                        if self.mode == \"txt2img\" #undefined\n                        else stream.frame_bff_size,\n                        min_batch_size=self.batch_size #undefined\n                        if self.mode == \"txt2img\" #undefined\n                        else stream.frame_bff_size,\n                    ),\n                    \"vae_decoder.engine\",\n                )\n\n                if not os.path.exists(unet_path):\n                \"\"\"\n                scip-python python python-stdlib 3.11 os/__init__:: undefined\n                scip-python python python-stdlib 3.11 os/path.: undefined\n                scip-python python python-stdlib 3.11 ntpath/join().: undefined\n                \"\"\"\n                    os.makedirs(os.path.dirname(unet_path), exist_ok=True)\n                    \"\"\"\n                    scip-python python python-stdlib 3.11 os/__init__:: undefined\n                    scip-python python python-stdlib 3.11 os/__init__:: undefined\n                    scip-python python python-stdlib 3.11 os/path.: undefined\n                    scip-python python python-stdlib 3.11 ntpath/join().: undefined\n                    scip-python python python-stdlib 3.11 os/makedirs().: undefined\n                    \"\"\"\n                    unet_model = UNet(\n                        fp16=True,\n                        device=stream.device,\n                        max_batch_size=stream.trt_unet_batch_size,\n                        min_batch_size=stream.trt_unet_batch_size,\n                        embedding_dim=stream.text_encoder.config.hidden_size,\n                        unet_dim=stream.unet.config.in_channels,\n                    )\n                    compile_unet(\n                        stream.unet,\n                        unet_model,\n                        unet_path + \".onnx\",\n                        unet_path + \".opt.onnx\",\n                        unet_path,\n                        opt_batch_size=stream.trt_unet_batch_size,\n                    )\n\n                if not os.path.exists(vae_decoder_path):\n                \"\"\"\n                scip-python python python-stdlib 3.11 os/__init__:: undefined\n                scip-python python python-stdlib 3.11 os/path.: undefined\n                scip-python python python-stdlib 3.11 ntpath/join().: undefined\n                \"\"\"\n                    os.makedirs(os.path.dirname(vae_decoder_path), exist_ok=True)\n                    \"\"\"\n                    scip-python python python-stdlib 3.11 os/__init__:: undefined\n                    scip-python python python-stdlib 3.11 os/__init__:: undefined\n                    scip-python python python-stdlib 3.11 os/path.: undefined\n                    scip-python python python-stdlib 3.11 ntpath/join().: undefined\n                    scip-python python python-stdlib 3.11 os/makedirs().: undefined\n                    \"\"\"\n                    stream.vae.forward = stream.vae.decode\n                    vae_decoder_model = VAE(\n                        device=stream.device,\n                        max_batch_size=self.batch_size #undefined\n                        if self.mode == \"txt2img\" #undefined\n                        else stream.frame_bff_size,\n                        min_batch_size=self.batch_size #undefined\n                        if self.mode == \"txt2img\" #undefined\n                        else stream.frame_bff_size,\n                    )\n                    compile_vae_decoder(\n                        stream.vae,\n                        vae_decoder_model,\n                        vae_decoder_path + \".onnx\",\n                        vae_decoder_path + \".opt.onnx\",\n                        vae_decoder_path,\n                        opt_batch_size=self.batch_size #undefined\n                        if self.mode == \"txt2img\" #undefined\n                        else stream.frame_bff_size,\n                    )\n                    delattr(stream.vae, \"forward\")\n\n                if not os.path.exists(vae_encoder_path):\n                \"\"\"\n                scip-python python python-stdlib 3.11 os/__init__:: undefined\n                scip-python python python-stdlib 3.11 os/path.: undefined\n                scip-python python python-stdlib 3.11 ntpath/join().: undefined\n                \"\"\"\n                    os.makedirs(os.path.dirname(vae_encoder_path), exist_ok=True)\n                    \"\"\"\n                    scip-python python python-stdlib 3.11 os/__init__:: undefined\n                    scip-python python python-stdlib 3.11 os/__init__:: undefined\n                    scip-python python python-stdlib 3.11 os/path.: undefined\n                    scip-python python python-stdlib 3.11 ntpath/join().: undefined\n                    scip-python python python-stdlib 3.11 os/makedirs().: undefined\n                    \"\"\"\n                    vae_encoder = TorchVAEEncoder(stream.vae).to(torch.device(\"cuda\"))\n                    vae_encoder_model = VAEEncoder(\n                        device=stream.device,\n                        max_batch_size=self.batch_size #undefined\n                        if self.mode == \"txt2img\" #undefined\n                        else stream.frame_bff_size,\n                        min_batch_size=self.batch_size #undefined\n                        if self.mode == \"txt2img\" #undefined\n                        else stream.frame_bff_size,\n                    )\n                    compile_vae_encoder(\n                        vae_encoder,\n                        vae_encoder_model,\n                        vae_encoder_path + \".onnx\",\n                        vae_encoder_path + \".opt.onnx\",\n                        vae_encoder_path,\n                        opt_batch_size=self.batch_size #undefined\n                        if self.mode == \"txt2img\" #undefined\n                        else stream.frame_bff_size,\n                    )\n\n                cuda_steram = cuda.Stream()\n\n                vae_config = stream.vae.config\n                vae_dtype = stream.vae.dtype\n\n                stream.unet = UNet2DConditionModelEngine(\n                    unet_path, cuda_steram, use_cuda_graph=False\n                )\n                stream.vae = AutoencoderKLEngine(\n                    vae_encoder_path,\n                    vae_decoder_path,\n                    cuda_steram,\n                    stream.pipe.vae_scale_factor,\n                    use_cuda_graph=False,\n                )\n                setattr(stream.vae, \"config\", vae_config)\n                setattr(stream.vae, \"dtype\", vae_dtype)\n\n                gc.collect()\n                \"\"\"\n                scip-python python python-stdlib 3.11 gc/__init__:: undefined\n                scip-python python python-stdlib 3.11 gc/collect().: undefined\n                \"\"\"\n                torch.cuda.empty_cache()\n\n                print(\"TensorRT acceleration enabled.\")\n            if acceleration == \"sfast\":\n                from streamdiffusion.acceleration.sfast import ( #undefined\n                    accelerate_with_stable_fast,\n                )\n\n                stream = accelerate_with_stable_fast(stream)\n                print(\"StableFast acceleration enabled.\")\n        except Exception:\n            traceback.print_exc()\n            print(\"Acceleration has failed. Falling back to normal mode.\")\n\n        if seed < 0: # Random seed\n            seed = np.random.randint(0, 1000000)\n            \"\"\"\n            scip-python python numpy 1.25.2 `numpy.random`/__init__:: undefined\n            scip-python python numpy 1.25.2 `numpy.random.mtrand`/randint.: undefined\n            \"\"\"\n\n        stream.prepare(\n            \"\",\n            \"\",\n            num_inference_steps=50,\n            guidance_scale=1.1\n            if stream.cfg_type in [\"full\", \"self\", \"initialize\"]\n            else 1.0,\n            generator=torch.manual_seed(seed),\n            seed=seed,\n        )\n\n        if self.use_safety_checker: #undefined\n            from transformers import CLIPFeatureExtractor #undefined\n            from diffusers.pipelines.stable_diffusion.safety_checker import ( #undefined\n                StableDiffusionSafetyChecker,\n            )\n\n            self.safety_checker = StableDiffusionSafetyChecker.from_pretrained(\n                \"CompVis/stable-diffusion-safety-checker\"\n            ).to(pipe.device)\n            self.feature_extractor = CLIPFeatureExtractor.from_pretrained(\n                \"openai/clip-vit-base-patch32\"\n            )\n            self.nsfw_fallback_img = Image.new(\"RGB\", (512, 512), (0, 0, 0))\n            \"\"\"\n            scip-python python Pillow 10.0.0 `PIL.Image`/__init__:: undefined\n            scip-python python Pillow 10.0.0 `pil.image`/new().: undefined\n            \"\"\"\n\n        return stream","documentation":"This code defines a function called `_load_model` that loads a model from a given path or ID, and returns a `StreamDiffusion` object. It also performs various tasks such as loading LCM-LoRA and VAE models, enabling acceleration, and preparing the model for inference."}},{"key":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#create_prefix().","attributes":{"range":[481,16,490,176],"symbol":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#create_prefix().","content":"def create_prefix(\n                    model_id_or_path: str,\n                    max_batch_size: int,\n                    min_batch_size: int,\n                ):\n                    maybe_path = Path(model_id_or_path)\n                    if maybe_path.exists():\n                        return f\"{maybe_path.stem}--lcm_lora-{use_lcm_lora}--tiny_vae-{use_tiny_vae}--max_batch-{max_batch_size}--min_batch-{min_batch_size}--mode-{self.mode}\"\n                    else:\n                        return f\"{model_id_or_path}--lcm_lora-{use_lcm_lora}--tiny_vae-{use_tiny_vae}--max_batch-{max_batch_size}--min_batch-{min_batch_size}--mode-{self.mode}\"","file":"/utils/wrapper.py","language":"python","fileHash":"4fc2c1f8d34f59a019136fa442db9ed0ff27859512221442ea1df929458d10e7","hash":"1ca37f90d6d4718747b843fc244b626165bfdc438e4c10788249fb12eca4045f","processedContent":"def create_prefix(\n                    model_id_or_path: str,\n                    max_batch_size: int,\n                    min_batch_size: int,\n                ):\n                    maybe_path = Path(model_id_or_path) #undefined\n                    if maybe_path.exists(): #undefined\n                        return f\"{maybe_path.stem}--lcm_lora-{use_lcm_lora}--tiny_vae-{use_tiny_vae}--max_batch-{max_batch_size}--min_batch-{min_batch_size}--mode-{self.mode}\"\n                        \"\"\"\n                        scip-python python python-stdlib 3.11 pathlib/PurePath#stem().: undefined\n                        scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#mode.: undefined\n                        \"\"\"\n                    else:\n                        return f\"{model_id_or_path}--lcm_lora-{use_lcm_lora}--tiny_vae-{use_tiny_vae}--max_batch-{max_batch_size}--min_batch-{min_batch_size}--mode-{self.mode}\"","documentation":"This code defines a function called `create_prefix` that takes three parameters: `model_id_or_path`, `max_batch_size`, and `min_batch_size`. The function checks if the `model_id_or_path` is a valid path, and if so, it returns a string with the prefix of the path, along with some additional information about the model. If the `model_id_or_path` is not a valid path, the function returns a string with the original value of the parameter."}},{"key":"scip-python python python-stdlib 3.11 traceback/__init__:","attributes":{"symbol":"scip-python python python-stdlib 3.11 traceback/__init__:","language":"python"}},{"key":"scip-python python temp indexer streamdiffusion/__init__:","attributes":{"symbol":"scip-python python temp indexer streamdiffusion/__init__:","language":"python"}},{"key":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#sd_turbo.","attributes":{"symbol":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#sd_turbo.","language":"python"}},{"key":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#mode.","attributes":{"symbol":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#mode.","language":"python"}},{"key":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#frame_buffer_size.","attributes":{"symbol":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#frame_buffer_size.","language":"python"}},{"key":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#output_type.","attributes":{"symbol":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#output_type.","language":"python"}},{"key":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#use_safety_checker.","attributes":{"symbol":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#use_safety_checker.","language":"python"}},{"key":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#feature_extractor.","attributes":{"symbol":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#feature_extractor.","language":"python"}},{"key":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#safety_checker.","attributes":{"symbol":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#safety_checker.","language":"python"}},{"key":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#nsfw_fallback_img.","attributes":{"symbol":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#nsfw_fallback_img.","language":"python"}},{"key":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#width.","attributes":{"symbol":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#width.","language":"python"}},{"key":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#height.","attributes":{"symbol":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#height.","language":"python"}},{"key":"scip-python python python-stdlib 3.11 traceback/print_exc().","attributes":{"symbol":"scip-python python python-stdlib 3.11 traceback/print_exc().","language":"python"}},{"key":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#use_denoising_batch.","attributes":{"symbol":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#use_denoising_batch.","language":"python"}},{"key":"scip-python python temp indexer `streamdiffusion.acceleration.tensorrt`/__init__:","attributes":{"symbol":"scip-python python temp indexer `streamdiffusion.acceleration.tensorrt`/__init__:","language":"python"}},{"key":"scip-python python temp indexer `streamdiffusion.acceleration.tensorrt.engine`/__init__:","attributes":{"symbol":"scip-python python temp indexer `streamdiffusion.acceleration.tensorrt.engine`/__init__:","language":"python"}},{"key":"scip-python python temp indexer `streamdiffusion.acceleration.tensorrt.models`/__init__:","attributes":{"symbol":"scip-python python temp indexer `streamdiffusion.acceleration.tensorrt.models`/__init__:","language":"python"}},{"key":"scip-python python python-stdlib 3.11 pathlib/Path#exists().","attributes":{"symbol":"scip-python python python-stdlib 3.11 pathlib/Path#exists().","language":"python"}},{"key":"scip-python python python-stdlib 3.11 pathlib/PurePath#stem().","attributes":{"symbol":"scip-python python python-stdlib 3.11 pathlib/PurePath#stem().","language":"python"}},{"key":"scip-python python temp indexer `streamdiffusion.acceleration.sfast`/__init__:","attributes":{"symbol":"scip-python python temp indexer `streamdiffusion.acceleration.sfast`/__init__:","language":"python"}},{"key":"scip-python python numpy 1.25.2 `numpy.random`/__init__:","attributes":{"symbol":"scip-python python numpy 1.25.2 `numpy.random`/__init__:","language":"python"}},{"key":"scip-python python numpy 1.25.2 `numpy.random.mtrand`/randint.","attributes":{"symbol":"scip-python python numpy 1.25.2 `numpy.random.mtrand`/randint.","language":"python"}},{"key":"scip-python python temp indexer transformers/__init__:","attributes":{"symbol":"scip-python python temp indexer transformers/__init__:","language":"python"}},{"key":"scip-python python temp indexer `diffusers.pipelines.stable_diffusion.safety_checker`/__init__:","attributes":{"symbol":"scip-python python temp indexer `diffusers.pipelines.stable_diffusion.safety_checker`/__init__:","language":"python"}},{"key":"scip-python python Pillow 10.0.0 `pil.image`/new().","attributes":{"symbol":"scip-python python Pillow 10.0.0 `pil.image`/new().","language":"python"}}],"edges":[{"key":"geid_136_0","source":"scip-typescript npm view 0.1.0 src/`reportWebVitals.ts`/","target":"scip-typescript npm view 0.1.0 src/`reportWebVitals.ts`/reportWebVitals.","attributes":{"type":"defines","at":[2,24,12,1]}},{"key":"geid_136_1","source":"scip-typescript npm view 0.1.0 src/`reportWebVitals.ts`/","target":"scip-typescript npm view 0.1.0 src/`reportWebVitals.ts`/reportWebVitals.","attributes":{"type":"uses","at":[14,15,14,30]}},{"key":"geid_136_2","source":"scip-typescript npm view 0.1.0 src/`index.tsx`/","target":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/","attributes":{"type":"uses","at":[0,18,0,25]}},{"key":"geid_136_3","source":"scip-typescript npm view 0.1.0 src/`index.tsx`/","target":"scip-typescript npm @types/react-dom 18.2.17 `client.d.ts`/","attributes":{"type":"uses","at":[1,21,1,39]}},{"key":"geid_136_4","source":"scip-typescript npm view 0.1.0 src/`index.tsx`/","target":"scip-typescript npm view 0.1.0 src/`App.tsx`/App().","attributes":{"type":"uses","at":[3,7,3,10]}},{"key":"geid_136_5","source":"scip-typescript npm view 0.1.0 src/`index.tsx`/","target":"scip-typescript npm view 0.1.0 src/`App.tsx`/","attributes":{"type":"uses","at":[3,16,3,23]}},{"key":"geid_136_6","source":"scip-typescript npm view 0.1.0 src/`index.tsx`/","target":"scip-typescript npm view 0.1.0 src/`reportWebVitals.ts`/reportWebVitals.","attributes":{"type":"uses","at":[4,7,4,22]}},{"key":"geid_136_7","source":"scip-typescript npm view 0.1.0 src/`index.tsx`/","target":"scip-typescript npm view 0.1.0 src/`reportWebVitals.ts`/","attributes":{"type":"uses","at":[4,28,4,47]}},{"key":"geid_136_8","source":"scip-typescript npm view 0.1.0 src/`index.tsx`/","target":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`global.d.ts`/HTMLElement#","attributes":{"type":"uses","at":[7,37,7,48]}},{"key":"geid_136_9","source":"scip-typescript npm view 0.1.0 src/`index.tsx`/","target":"scip-typescript npm view 0.1.0 src/`index.tsx`/root.","attributes":{"type":"uses","at":[9,0,9,4]}},{"key":"geid_136_10","source":"scip-typescript npm view 0.1.0 src/`index.tsx`/","target":"scip-typescript npm view 0.1.0 src/`App.tsx`/App().","attributes":{"type":"uses","at":[11,5,11,8]}},{"key":"geid_136_11","source":"scip-typescript npm view 0.1.0 src/`index.tsx`/","target":"scip-typescript npm view 0.1.0 src/`reportWebVitals.ts`/reportWebVitals.","attributes":{"type":"uses","at":[18,0,18,15]}},{"key":"geid_136_12","source":"scip-typescript npm view 0.1.0 src/`App.tsx`/","target":"scip-typescript npm view 0.1.0 src/`App.tsx`/App().","attributes":{"type":"defines","at":[3,0,137,1]}},{"key":"geid_136_13","source":"scip-typescript npm view 0.1.0 src/`App.tsx`/","target":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/React/useCallback().","attributes":{"type":"uses","at":[0,16,0,27]}},{"key":"geid_136_14","source":"scip-typescript npm view 0.1.0 src/`App.tsx`/","target":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/React/useState().","attributes":{"type":"uses","at":[0,29,0,37]}},{"key":"geid_136_15","source":"scip-typescript npm view 0.1.0 src/`App.tsx`/","target":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/","attributes":{"type":"uses","at":[0,45,0,52]}},{"key":"geid_136_16","source":"scip-typescript npm view 0.1.0 src/`App.tsx`/App().","target":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/React/useState().","attributes":{"type":"uses","at":[4,40,4,48]}},{"key":"geid_136_17","source":"scip-typescript npm view 0.1.0 src/`App.tsx`/App().","target":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/React/useState().","attributes":{"type":"uses","at":[5,38,5,46]}},{"key":"geid_136_18","source":"scip-typescript npm view 0.1.0 src/`App.tsx`/App().","target":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/React/useState().","attributes":{"type":"uses","at":[6,30,6,38]}},{"key":"geid_136_19","source":"scip-typescript npm view 0.1.0 src/`App.tsx`/App().","target":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/React/useCallback().","attributes":{"type":"uses","at":[37,21,37,32]}},{"key":"geid_136_20","source":"scip-typescript npm view 0.1.0 src/`App.tsx`/App().","target":"scip-typescript npm @types/node 20.9.3 `ts4.8`/`globals.d.ts`/fetch().","attributes":{"type":"uses","at":[40,31,40,36]}},{"key":"geid_136_21","source":"scip-typescript npm view 0.1.0 src/`App.tsx`/App().","target":"scip-typescript npm @types/node 20.9.3 `ts4.8`/`globals.d.ts`/console.","attributes":{"type":"uses","at":[54,8,54,15]}},{"key":"geid_136_22","source":"scip-typescript npm view 0.1.0 src/`App.tsx`/App().","target":"scip-typescript npm @types/node 20.9.3 `ts4.8`/`console.d.ts`/`\"node:console\"`/global/console/","attributes":{"type":"uses","at":[54,8,54,15]}},{"key":"geid_136_23","source":"scip-typescript npm view 0.1.0 src/`App.tsx`/App().","target":"scip-typescript npm @types/node 20.9.3 `ts4.8`/`console.d.ts`/`\"node:console\"`/global/console.","attributes":{"type":"uses","at":[54,8,54,15]}},{"key":"geid_136_24","source":"scip-typescript npm view 0.1.0 src/`App.tsx`/App().","target":"scip-typescript npm @types/node 20.9.3 `ts4.8`/`console.d.ts`/`\"node:console\"`/global/Console#error().","attributes":{"type":"uses","at":[54,16,54,21]}},{"key":"geid_136_25","source":"scip-typescript npm view 0.1.0 src/`App.tsx`/App().","target":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`global.d.ts`/HTMLInputElement#","attributes":{"type":"uses","at":[60,55,60,71]}},{"key":"geid_136_26","source":"scip-typescript npm view 0.1.0 src/`App.tsx`/App().","target":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/global/JSX/IntrinsicElements#div.","attributes":{"type":"uses","at":[75,5,75,8]}},{"key":"geid_136_27","source":"scip-typescript npm view 0.1.0 src/`App.tsx`/App().","target":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/React/HTMLAttributes#className.","attributes":{"type":"uses","at":[76,6,76,15]}},{"key":"geid_136_28","source":"scip-typescript npm view 0.1.0 src/`App.tsx`/App().","target":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/React/HTMLAttributes#style.","attributes":{"type":"uses","at":[77,6,77,11]}},{"key":"geid_136_29","source":"scip-typescript npm view 0.1.0 src/`App.tsx`/App().","target":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/global/JSX/IntrinsicElements#div.","attributes":{"type":"uses","at":[88,7,88,10]}},{"key":"geid_136_30","source":"scip-typescript npm view 0.1.0 src/`App.tsx`/App().","target":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/React/HTMLAttributes#style.","attributes":{"type":"uses","at":[89,8,89,13]}},{"key":"geid_136_31","source":"scip-typescript npm view 0.1.0 src/`App.tsx`/App().","target":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/global/JSX/IntrinsicElements#img.","attributes":{"type":"uses","at":[104,15,104,18]}},{"key":"geid_136_32","source":"scip-typescript npm view 0.1.0 src/`App.tsx`/App().","target":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/React/ImgHTMLAttributes#src.","attributes":{"type":"uses","at":[105,16,105,19]}},{"key":"geid_136_33","source":"scip-typescript npm view 0.1.0 src/`App.tsx`/App().","target":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/React/ImgHTMLAttributes#alt.","attributes":{"type":"uses","at":[106,16,106,19]}},{"key":"geid_136_34","source":"scip-typescript npm view 0.1.0 src/`App.tsx`/App().","target":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/React/HTMLAttributes#style.","attributes":{"type":"uses","at":[107,16,107,21]}},{"key":"geid_136_35","source":"scip-typescript npm view 0.1.0 src/`App.tsx`/App().","target":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/global/JSX/IntrinsicElements#div.","attributes":{"type":"uses","at":[134,8,134,11]}},{"key":"geid_136_36","source":"scip-typescript npm view 0.1.0 src/`App.tsx`/App().","target":"scip-typescript npm @types/react 18.2.39 `ts5.0`/`index.d.ts`/global/JSX/IntrinsicElements#div.","attributes":{"type":"uses","at":[135,6,135,9]}},{"key":"geid_136_37","source":"scip-typescript npm view 0.1.0 src/`App.tsx`/","target":"scip-typescript npm view 0.1.0 src/`App.tsx`/App().","attributes":{"type":"uses","at":[139,15,139,18]}},{"key":"geid_138_0","source":"scip-python python temp indexer setup/__init__:","target":"scip-python python temp indexer setup/deps_list().","attributes":{"type":"defines","at":[25,0,26,38]}},{"key":"geid_138_1","source":"scip-python python temp indexer setup/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[0,7,0,9]}},{"key":"geid_138_2","source":"scip-python python temp indexer setup/__init__:","target":"scip-python python python-stdlib 3.11 re/__init__:","attributes":{"type":"uses","at":[1,7,1,9]}},{"key":"geid_138_3","source":"scip-python python temp indexer setup/__init__:","target":"scip-python python setuptools 68.1.2 setuptools/__init__:","attributes":{"type":"uses","at":[3,5,3,15]}},{"key":"geid_138_4","source":"scip-python python temp indexer setup/__init__:","target":"scip-python python python-stdlib 3.11 re/__init__:","attributes":{"type":"uses","at":[22,26,22,28]}},{"key":"geid_138_5","source":"scip-python python temp indexer setup/__init__:","target":"scip-python python python-stdlib 3.11 re/findall().","attributes":{"type":"uses","at":[22,29,22,36]}},{"key":"geid_138_6","source":"scip-python python temp indexer setup/__init__:","target":"scip-python python temp indexer setup/_deps.","attributes":{"type":"uses","at":[22,86,22,91]}},{"key":"geid_138_7","source":"scip-python python temp indexer setup/deps_list().","target":"scip-python python temp indexer setup/deps.","attributes":{"type":"uses","at":[26,12,26,16]}},{"key":"geid_138_8","source":"scip-python python temp indexer setup/__init__:","target":"scip-python python temp indexer setup/extras.","attributes":{"type":"uses","at":[30,0,30,6]}},{"key":"geid_138_9","source":"scip-python python temp indexer setup/__init__:","target":"scip-python python temp indexer setup/deps_list().","attributes":{"type":"uses","at":[30,21,30,30]}},{"key":"geid_138_10","source":"scip-python python temp indexer setup/__init__:","target":"scip-python python temp indexer setup/extras.","attributes":{"type":"uses","at":[31,0,31,6]}},{"key":"geid_138_11","source":"scip-python python temp indexer setup/__init__:","target":"scip-python python temp indexer setup/deps_list().","attributes":{"type":"uses","at":[31,18,31,27]}},{"key":"geid_138_12","source":"scip-python python temp indexer setup/__init__:","target":"scip-python python temp indexer setup/extras.","attributes":{"type":"uses","at":[32,0,32,6]}},{"key":"geid_138_13","source":"scip-python python temp indexer setup/__init__:","target":"scip-python python temp indexer setup/deps_list().","attributes":{"type":"uses","at":[32,21,32,30]}},{"key":"geid_138_14","source":"scip-python python temp indexer setup/__init__:","target":"scip-python python temp indexer setup/extras.","attributes":{"type":"uses","at":[34,0,34,6]}},{"key":"geid_138_15","source":"scip-python python temp indexer setup/__init__:","target":"scip-python python temp indexer setup/extras.","attributes":{"type":"uses","at":[34,16,34,22]}},{"key":"geid_138_16","source":"scip-python python temp indexer setup/__init__:","target":"scip-python python temp indexer setup/extras.","attributes":{"type":"uses","at":[34,37,34,43]}},{"key":"geid_138_17","source":"scip-python python temp indexer setup/__init__:","target":"scip-python python temp indexer setup/extras.","attributes":{"type":"uses","at":[34,55,34,61]}},{"key":"geid_138_18","source":"scip-python python temp indexer setup/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[36,3,36,5]}},{"key":"geid_138_19","source":"scip-python python temp indexer setup/__init__:","target":"scip-python python python-stdlib 3.11 os/name.name.","attributes":{"type":"uses","at":[36,6,36,10]}},{"key":"geid_138_20","source":"scip-python python temp indexer setup/__init__:","target":"scip-python python temp indexer setup/extras.","attributes":{"type":"uses","at":[37,4,37,10]}},{"key":"geid_138_21","source":"scip-python python temp indexer setup/__init__:","target":"scip-python python temp indexer setup/extras.","attributes":{"type":"uses","at":[37,25,37,31]}},{"key":"geid_138_22","source":"scip-python python temp indexer setup/__init__:","target":"scip-python python temp indexer setup/deps_list().","attributes":{"type":"uses","at":[37,46,37,55]}},{"key":"geid_138_23","source":"scip-python python temp indexer setup/__init__:","target":"scip-python python temp indexer setup/deps.","attributes":{"type":"uses","at":[40,4,40,8]}},{"key":"geid_138_24","source":"scip-python python temp indexer setup/__init__:","target":"scip-python python temp indexer setup/deps.","attributes":{"type":"uses","at":[41,4,41,8]}},{"key":"geid_138_25","source":"scip-python python temp indexer setup/__init__:","target":"scip-python python temp indexer setup/deps.","attributes":{"type":"uses","at":[42,4,42,8]}},{"key":"geid_138_26","source":"scip-python python temp indexer setup/__init__:","target":"scip-python python temp indexer setup/deps.","attributes":{"type":"uses","at":[43,4,43,8]}},{"key":"geid_138_27","source":"scip-python python temp indexer setup/__init__:","target":"scip-python python python-stdlib 3.11 io/TextIOBase#read().","attributes":{"type":"uses","at":[50,62,50,66]}},{"key":"geid_138_28","source":"scip-python python temp indexer setup/__init__:","target":"scip-python python temp indexer setup/install_requires.","attributes":{"type":"uses","at":[62,26,62,42]}},{"key":"geid_138_29","source":"scip-python python temp indexer setup/__init__:","target":"scip-python python temp indexer setup/extras.","attributes":{"type":"uses","at":[63,19,63,25]}},{"key":"geid_138_30","source":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/__init__:","target":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#","attributes":{"type":"defines","at":[9,0,47,45]}},{"key":"geid_138_31","source":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/__init__:","target":"scip-python python python-stdlib 3.11 dataclasses/__init__:","attributes":{"type":"uses","at":[0,5,0,16]}},{"key":"geid_138_32","source":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/__init__:","target":"scip-python python python-stdlib 3.11 dataclasses/dataclass().","attributes":{"type":"uses","at":[0,24,0,33]}},{"key":"geid_138_33","source":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/__init__:","target":"scip-python python python-stdlib 3.11 dataclasses/field().","attributes":{"type":"uses","at":[0,35,0,40]}},{"key":"geid_138_34","source":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/__init__:","target":"scip-python python python-stdlib 3.11 typing/__init__:","attributes":{"type":"uses","at":[1,5,1,11]}},{"key":"geid_138_35","source":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/__init__:","target":"scip-python python python-stdlib 3.11 typing/List.","attributes":{"type":"uses","at":[1,19,1,23]}},{"key":"geid_138_36","source":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[1,25,1,32]}},{"key":"geid_138_37","source":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[4,7,4,9]}},{"key":"geid_138_38","source":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[6,17,6,19]}},{"key":"geid_138_39","source":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/__init__:","target":"scip-python python python-stdlib 3.11 os/environ.environ.","attributes":{"type":"uses","at":[6,20,6,27]}},{"key":"geid_138_40","source":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Mapping#get().","attributes":{"type":"uses","at":[6,28,6,31]}},{"key":"geid_138_41","source":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#","target":"scip-python python python-stdlib 3.11 dataclasses/dataclass().","attributes":{"type":"uses","at":[9,1,9,10]}},{"key":"geid_138_42","source":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[26,10,26,17]}},{"key":"geid_138_43","source":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[38,18,38,25]}},{"key":"geid_138_44","source":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#","target":"scip-python python python-stdlib 3.11 typing/List.","attributes":{"type":"uses","at":[44,18,44,22]}},{"key":"geid_138_45","source":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#","target":"scip-python python python-stdlib 3.11 dataclasses/field().","attributes":{"type":"uses","at":[44,30,44,35]}},{"key":"geid_138_46","source":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#","target":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/SAFETY_CHECKER.","attributes":{"type":"uses","at":[47,31,47,45]}},{"key":"geid_138_47","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#","target":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_base64_to_pil().","attributes":{"type":"defines","at":[136,4,152,81]}},{"key":"geid_138_48","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#","target":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_pil_to_base64().","attributes":{"type":"defines","at":[115,4,134,68]}},{"key":"geid_138_49","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#","target":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_predict().","attributes":{"type":"defines","at":[94,4,113,13]}},{"key":"geid_138_50","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#","target":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#__init__().","attributes":{"type":"defines","at":[50,4,92,49]}},{"key":"geid_138_51","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#","attributes":{"type":"defines","at":[49,0,152,81]}},{"key":"geid_138_52","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/UpdatePromptResponseModel#","attributes":{"type":"defines","at":[41,0,46,15]}},{"key":"geid_138_53","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/PredictResponseModel#","attributes":{"type":"defines","at":[33,0,38,21]}},{"key":"geid_138_54","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/PredictInputModel#","attributes":{"type":"defines","at":[25,0,30,15]}},{"key":"geid_138_55","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python python-stdlib 3.11 sys/__init__:","attributes":{"type":"uses","at":[0,7,0,10]}},{"key":"geid_138_56","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[1,7,1,9]}},{"key":"geid_138_57","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python python-stdlib 3.11 asyncio/__init__:","attributes":{"type":"uses","at":[2,7,2,14]}},{"key":"geid_138_58","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python python-stdlib 3.11 base64/__init__:","attributes":{"type":"uses","at":[3,7,3,13]}},{"key":"geid_138_59","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python python-stdlib 3.11 logging/__init__:","attributes":{"type":"uses","at":[4,7,4,14]}},{"key":"geid_138_60","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python python-stdlib 3.11 io/__init__:","attributes":{"type":"uses","at":[5,5,5,7]}},{"key":"geid_138_61","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python python-stdlib 3.11 io/BytesIO#","attributes":{"type":"uses","at":[5,15,5,22]}},{"key":"geid_138_62","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python python-stdlib 3.11 pathlib/__init__:","attributes":{"type":"uses","at":[6,5,6,12]}},{"key":"geid_138_63","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python python-stdlib 3.11 pathlib/Path#","attributes":{"type":"uses","at":[6,20,6,24]}},{"key":"geid_138_64","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/__init__:","attributes":{"type":"uses","at":[9,5,9,11]}},{"key":"geid_138_65","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#","attributes":{"type":"uses","at":[9,19,9,25]}},{"key":"geid_138_66","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python temp indexer fastapi/__init__:","attributes":{"type":"uses","at":[10,5,10,12]}},{"key":"geid_138_67","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python temp indexer `fastapi.middleware.cors`/__init__:","attributes":{"type":"uses","at":[11,5,11,28]}},{"key":"geid_138_68","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python temp indexer `fastapi.staticfiles`/__init__:","attributes":{"type":"uses","at":[12,5,12,24]}},{"key":"geid_138_69","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python Pillow 10.0.0 PIL/__init__:","attributes":{"type":"uses","at":[14,5,14,8]}},{"key":"geid_138_70","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[14,16,14,21]}},{"key":"geid_138_71","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python temp indexer pydantic/__init__:","attributes":{"type":"uses","at":[15,5,15,13]}},{"key":"geid_138_72","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python python-stdlib 3.11 sys/__init__:","attributes":{"type":"uses","at":[17,0,17,3]}},{"key":"geid_138_73","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python python-stdlib 3.11 sys/path.path.","attributes":{"type":"uses","at":[17,4,17,8]}},{"key":"geid_138_74","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[17,16,17,18]}},{"key":"geid_138_75","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[17,19,17,23]}},{"key":"geid_138_76","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[17,24,17,28]}},{"key":"geid_138_77","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[17,29,17,31]}},{"key":"geid_138_78","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[17,32,17,36]}},{"key":"geid_138_79","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[17,37,17,44]}},{"key":"geid_138_80","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python temp indexer `utils.wrapper`/__init__:","attributes":{"type":"uses","at":[19,5,19,18]}},{"key":"geid_138_81","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#","attributes":{"type":"uses","at":[19,26,19,48]}},{"key":"geid_138_82","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python python-stdlib 3.11 logging/__init__:","attributes":{"type":"uses","at":[21,9,21,16]}},{"key":"geid_138_83","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python python-stdlib 3.11 logging/getLogger().","attributes":{"type":"uses","at":[21,17,21,26]}},{"key":"geid_138_84","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python python-stdlib 3.11 pathlib/Path#","attributes":{"type":"uses","at":[22,14,22,18]}},{"key":"geid_138_85","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/","attributes":{"type":"uses","at":[22,19,22,27]}},{"key":"geid_138_86","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python python-stdlib 3.11 pathlib/PurePath#parent().","attributes":{"type":"uses","at":[22,29,22,35]}},{"key":"geid_138_87","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python python-stdlib 3.11 pathlib/PurePath#parent().","attributes":{"type":"uses","at":[22,36,22,42]}},{"key":"geid_138_88","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#__init__().","target":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#","attributes":{"type":"uses","at":[50,31,50,37]}},{"key":"geid_138_89","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#__init__().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#","attributes":{"type":"uses","at":[60,32,60,54]}},{"key":"geid_138_90","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#__init__().","target":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#mode.","attributes":{"type":"uses","at":[61,24,61,28]}},{"key":"geid_138_91","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#__init__().","target":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#model_id_or_path.","attributes":{"type":"uses","at":[62,36,62,52]}},{"key":"geid_138_92","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#__init__().","target":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#lcm_lora_id.","attributes":{"type":"uses","at":[63,31,63,42]}},{"key":"geid_138_93","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#__init__().","target":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#vae_id.","attributes":{"type":"uses","at":[64,26,64,32]}},{"key":"geid_138_94","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#__init__().","target":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#device.","attributes":{"type":"uses","at":[65,26,65,32]}},{"key":"geid_138_95","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#__init__().","target":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#dtype.","attributes":{"type":"uses","at":[66,25,66,30]}},{"key":"geid_138_96","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#__init__().","target":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#acceleration.","attributes":{"type":"uses","at":[67,32,67,44]}},{"key":"geid_138_97","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#__init__().","target":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#t_index_list.","attributes":{"type":"uses","at":[68,32,68,44]}},{"key":"geid_138_98","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#__init__().","target":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#warmup.","attributes":{"type":"uses","at":[69,26,69,32]}},{"key":"geid_138_99","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#__init__().","target":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#use_safety_checker.","attributes":{"type":"uses","at":[70,38,70,56]}},{"key":"geid_138_100","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#__init__().","target":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#app.","attributes":{"type":"uses","at":[74,13,74,16]}},{"key":"geid_138_101","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#__init__().","target":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_predict().","attributes":{"type":"uses","at":[76,17,76,25]}},{"key":"geid_138_102","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#__init__().","target":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/PredictResponseModel#","attributes":{"type":"uses","at":[78,27,78,47]}},{"key":"geid_138_103","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#__init__().","target":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#app.","attributes":{"type":"uses","at":[80,13,80,16]}},{"key":"geid_138_104","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#__init__().","target":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#app.","attributes":{"type":"uses","at":[87,13,87,16]}},{"key":"geid_138_105","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#__init__().","target":"scip-python python python-stdlib 3.11 asyncio/__init__:","attributes":{"type":"uses","at":[91,29,91,36]}},{"key":"geid_138_106","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#__init__().","target":"scip-python python temp indexer `asyncio.locks`/Lock#","attributes":{"type":"uses","at":[91,37,91,41]}},{"key":"geid_138_107","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#__init__().","target":"scip-python python python-stdlib 3.11 asyncio/__init__:","attributes":{"type":"uses","at":[92,35,92,42]}},{"key":"geid_138_108","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#__init__().","target":"scip-python python temp indexer `asyncio.locks`/Lock#","attributes":{"type":"uses","at":[92,43,92,47]}},{"key":"geid_138_109","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_predict().","target":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/PredictResponseModel#","attributes":{"type":"uses","at":[94,56,94,76]}},{"key":"geid_138_110","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_predict().","target":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/PredictInputModel#","attributes":{"type":"uses","at":[94,34,94,51]}},{"key":"geid_138_111","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_predict().","target":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_predict_lock.","attributes":{"type":"uses","at":[108,24,108,37]}},{"key":"geid_138_112","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_predict().","target":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/PredictResponseModel#","attributes":{"type":"uses","at":[109,19,109,39]}},{"key":"geid_138_113","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_predict().","target":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_pil_to_base64().","attributes":{"type":"uses","at":[110,34,110,48]}},{"key":"geid_138_114","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_predict().","target":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#stream_diffusion.","attributes":{"type":"uses","at":[111,25,111,41]}},{"key":"geid_138_115","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_predict().","target":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/PredictInputModel#prompt.","attributes":{"type":"uses","at":[111,53,111,59]}},{"key":"geid_138_116","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_pil_to_base64().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[115,36,115,41]}},{"key":"geid_138_117","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_pil_to_base64().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#","attributes":{"type":"uses","at":[115,42,115,47]}},{"key":"geid_138_118","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_pil_to_base64().","target":"scip-python python python-stdlib 3.11 io/BytesIO#","attributes":{"type":"uses","at":[132,19,132,26]}},{"key":"geid_138_119","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_pil_to_base64().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#convert().","attributes":{"type":"uses","at":[133,14,133,21]}},{"key":"geid_138_120","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_pil_to_base64().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#save().","attributes":{"type":"uses","at":[133,29,133,33]}},{"key":"geid_138_121","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_pil_to_base64().","target":"scip-python python python-stdlib 3.11 base64/__init__:","attributes":{"type":"uses","at":[134,15,134,21]}},{"key":"geid_138_122","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_pil_to_base64().","target":"scip-python python python-stdlib 3.11 base64/b64encode().","attributes":{"type":"uses","at":[134,22,134,31]}},{"key":"geid_138_123","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_pil_to_base64().","target":"scip-python python python-stdlib 3.11 io/BytesIO#getvalue().","attributes":{"type":"uses","at":[134,41,134,49]}},{"key":"geid_138_124","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_base64_to_pil().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[136,51,136,56]}},{"key":"geid_138_125","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_base64_to_pil().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#","attributes":{"type":"uses","at":[136,57,136,62]}},{"key":"geid_138_126","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_base64_to_pil().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[152,15,152,20]}},{"key":"geid_138_127","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_base64_to_pil().","target":"scip-python python Pillow 10.0.0 `pil.image`/open().","attributes":{"type":"uses","at":[152,21,152,25]}},{"key":"geid_138_128","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_base64_to_pil().","target":"scip-python python python-stdlib 3.11 io/BytesIO#","attributes":{"type":"uses","at":[152,26,152,33]}},{"key":"geid_138_129","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_base64_to_pil().","target":"scip-python python python-stdlib 3.11 base64/__init__:","attributes":{"type":"uses","at":[152,34,152,40]}},{"key":"geid_138_130","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_base64_to_pil().","target":"scip-python python python-stdlib 3.11 base64/b64decode().","attributes":{"type":"uses","at":[152,41,152,50]}},{"key":"geid_138_131","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#_base64_to_pil().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#convert().","attributes":{"type":"uses","at":[152,67,152,74]}},{"key":"geid_138_132","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/","attributes":{"type":"uses","at":[155,3,155,11]}},{"key":"geid_138_133","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/__init__:","attributes":{"type":"uses","at":[156,9,156,15]}},{"key":"geid_138_134","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#","attributes":{"type":"uses","at":[156,23,156,29]}},{"key":"geid_138_135","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#","attributes":{"type":"uses","at":[158,13,158,19]}},{"key":"geid_138_136","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#","attributes":{"type":"uses","at":[161,8,161,11]}},{"key":"geid_138_137","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/config.","attributes":{"type":"uses","at":[161,12,161,18]}},{"key":"geid_138_138","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/Api#app.","attributes":{"type":"uses","at":[161,20,161,23]}},{"key":"geid_138_139","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/config.","attributes":{"type":"uses","at":[162,13,162,19]}},{"key":"geid_138_140","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#host.","attributes":{"type":"uses","at":[162,20,162,24]}},{"key":"geid_138_141","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/config.","attributes":{"type":"uses","at":[163,13,163,19]}},{"key":"geid_138_142","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#port.","attributes":{"type":"uses","at":[163,20,163,24]}},{"key":"geid_138_143","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/config.","attributes":{"type":"uses","at":[164,16,164,22]}},{"key":"geid_138_144","source":"scip-python python temp indexer `demo.realtime-txt2img.server.main`/__init__:","target":"scip-python python temp indexer `demo.realtime-txt2img.server.config`/Config#workers.","attributes":{"type":"uses","at":[164,23,164,30]}},{"key":"geid_138_145","source":"scip-python python temp indexer `examples.benchmark.multi`/__init__:","target":"scip-python python temp indexer `examples.benchmark.multi`/run().","attributes":{"type":"defines","at":[36,0,141,65]}},{"key":"geid_138_146","source":"scip-python python temp indexer `examples.benchmark.multi`/__init__:","target":"scip-python python temp indexer `examples.benchmark.multi`/download_image().","attributes":{"type":"defines","at":[30,0,33,16]}},{"key":"geid_138_147","source":"scip-python python temp indexer `examples.benchmark.multi`/__init__:","target":"scip-python python temp indexer `examples.benchmark.multi`/_postprocess_image().","attributes":{"type":"defines","at":[20,0,27,18]}},{"key":"geid_138_148","source":"scip-python python temp indexer `examples.benchmark.multi`/__init__:","target":"scip-python python python-stdlib 3.11 io/__init__:","attributes":{"type":"uses","at":[0,7,0,9]}},{"key":"geid_138_149","source":"scip-python python temp indexer `examples.benchmark.multi`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[1,7,1,9]}},{"key":"geid_138_150","source":"scip-python python temp indexer `examples.benchmark.multi`/__init__:","target":"scip-python python python-stdlib 3.11 sys/__init__:","attributes":{"type":"uses","at":[2,7,2,10]}},{"key":"geid_138_151","source":"scip-python python temp indexer `examples.benchmark.multi`/__init__:","target":"scip-python python python-stdlib 3.11 time/__init__:","attributes":{"type":"uses","at":[3,7,3,11]}},{"key":"geid_138_152","source":"scip-python python temp indexer `examples.benchmark.multi`/__init__:","target":"scip-python python python-stdlib 3.11 multiprocessing/__init__:","attributes":{"type":"uses","at":[4,5,4,20]}},{"key":"geid_138_153","source":"scip-python python temp indexer `examples.benchmark.multi`/__init__:","target":"scip-python python python-stdlib 3.11 typing/__init__:","attributes":{"type":"uses","at":[5,5,5,11]}},{"key":"geid_138_154","source":"scip-python python temp indexer `examples.benchmark.multi`/__init__:","target":"scip-python python python-stdlib 3.11 typing/List.","attributes":{"type":"uses","at":[5,19,5,23]}},{"key":"geid_138_155","source":"scip-python python temp indexer `examples.benchmark.multi`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[5,25,5,32]}},{"key":"geid_138_156","source":"scip-python python temp indexer `examples.benchmark.multi`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[5,34,5,42]}},{"key":"geid_138_157","source":"scip-python python temp indexer `examples.benchmark.multi`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Dict.","attributes":{"type":"uses","at":[5,44,5,48]}},{"key":"geid_138_158","source":"scip-python python temp indexer `examples.benchmark.multi`/__init__:","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[8,7,8,16]}},{"key":"geid_138_159","source":"scip-python python temp indexer `examples.benchmark.multi`/__init__:","target":"scip-python python requests 2.31.0 requests/__init__:","attributes":{"type":"uses","at":[9,7,9,15]}},{"key":"geid_138_160","source":"scip-python python temp indexer `examples.benchmark.multi`/__init__:","target":"scip-python python tqdm 4.66.1 tqdm/__init__:","attributes":{"type":"uses","at":[11,5,11,9]}},{"key":"geid_138_161","source":"scip-python python temp indexer `examples.benchmark.multi`/__init__:","target":"scip-python python tqdm 4.66.1 `tqdm.std`/tqdm#","attributes":{"type":"uses","at":[11,17,11,21]}},{"key":"geid_138_162","source":"scip-python python temp indexer `examples.benchmark.multi`/__init__:","target":"scip-python python temp indexer `streamdiffusion.image_utils`/__init__:","attributes":{"type":"uses","at":[13,5,13,32]}},{"key":"geid_138_163","source":"scip-python python temp indexer `examples.benchmark.multi`/__init__:","target":"scip-python python python-stdlib 3.11 sys/__init__:","attributes":{"type":"uses","at":[15,0,15,3]}},{"key":"geid_138_164","source":"scip-python python temp indexer `examples.benchmark.multi`/__init__:","target":"scip-python python python-stdlib 3.11 sys/path.path.","attributes":{"type":"uses","at":[15,4,15,8]}},{"key":"geid_138_165","source":"scip-python python temp indexer `examples.benchmark.multi`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[15,16,15,18]}},{"key":"geid_138_166","source":"scip-python python temp indexer `examples.benchmark.multi`/__init__:","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[15,19,15,23]}},{"key":"geid_138_167","source":"scip-python python temp indexer `examples.benchmark.multi`/__init__:","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[15,24,15,28]}},{"key":"geid_138_168","source":"scip-python python temp indexer `examples.benchmark.multi`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[15,29,15,31]}},{"key":"geid_138_169","source":"scip-python python temp indexer `examples.benchmark.multi`/__init__:","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[15,32,15,36]}},{"key":"geid_138_170","source":"scip-python python temp indexer `examples.benchmark.multi`/__init__:","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[15,37,15,44]}},{"key":"geid_138_171","source":"scip-python python temp indexer `examples.benchmark.multi`/__init__:","target":"scip-python python temp indexer `utils.wrapper`/__init__:","attributes":{"type":"uses","at":[17,5,17,18]}},{"key":"geid_138_172","source":"scip-python python temp indexer `examples.benchmark.multi`/__init__:","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#","attributes":{"type":"uses","at":[17,26,17,48]}},{"key":"geid_138_173","source":"scip-python python temp indexer `examples.benchmark.multi`/_postprocess_image().","target":"scip-python python python-stdlib 3.11 queue/Queue#empty().","attributes":{"type":"uses","at":[23,25,23,30]}},{"key":"geid_138_174","source":"scip-python python temp indexer `examples.benchmark.multi`/_postprocess_image().","target":"scip-python python python-stdlib 3.11 time/__init__:","attributes":{"type":"uses","at":[25,12,25,16]}},{"key":"geid_138_175","source":"scip-python python temp indexer `examples.benchmark.multi`/_postprocess_image().","target":"scip-python python python-stdlib 3.11 time/sleep().","attributes":{"type":"uses","at":[25,17,25,22]}},{"key":"geid_138_176","source":"scip-python python temp indexer `examples.benchmark.multi`/download_image().","target":"scip-python python requests 2.31.0 requests/__init__:","attributes":{"type":"uses","at":[31,15,31,23]}},{"key":"geid_138_177","source":"scip-python python temp indexer `examples.benchmark.multi`/download_image().","target":"scip-python python requests 2.31.0 `requests.api`/get().","attributes":{"type":"uses","at":[31,24,31,27]}},{"key":"geid_138_178","source":"scip-python python temp indexer `examples.benchmark.multi`/download_image().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[32,12,32,21]}},{"key":"geid_138_179","source":"scip-python python temp indexer `examples.benchmark.multi`/download_image().","target":"scip-python python Pillow 10.0.0 `pil.image`/open().","attributes":{"type":"uses","at":[32,22,32,26]}},{"key":"geid_138_180","source":"scip-python python temp indexer `examples.benchmark.multi`/download_image().","target":"scip-python python python-stdlib 3.11 io/__init__:","attributes":{"type":"uses","at":[32,27,32,29]}},{"key":"geid_138_181","source":"scip-python python temp indexer `examples.benchmark.multi`/download_image().","target":"scip-python python python-stdlib 3.11 io/BytesIO#","attributes":{"type":"uses","at":[32,30,32,37]}},{"key":"geid_138_182","source":"scip-python python temp indexer `examples.benchmark.multi`/download_image().","target":"scip-python python requests 2.31.0 `requests.models`/Response#content().","attributes":{"type":"uses","at":[32,47,32,54]}},{"key":"geid_138_183","source":"scip-python python temp indexer `examples.benchmark.multi`/run().","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[39,15,39,23]}},{"key":"geid_138_184","source":"scip-python python temp indexer `examples.benchmark.multi`/run().","target":"scip-python python python-stdlib 3.11 typing/Dict.","attributes":{"type":"uses","at":[39,24,39,28]}},{"key":"geid_138_185","source":"scip-python python temp indexer `examples.benchmark.multi`/run().","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[47,18,47,25]}},{"key":"geid_138_186","source":"scip-python python temp indexer `examples.benchmark.multi`/run().","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[48,16,48,24]}},{"key":"geid_138_187","source":"scip-python python temp indexer `examples.benchmark.multi`/run().","target":"scip-python python python-stdlib 3.11 typing/List.","attributes":{"type":"uses","at":[48,25,48,29]}},{"key":"geid_138_188","source":"scip-python python temp indexer `examples.benchmark.multi`/run().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#","attributes":{"type":"uses","at":[88,13,88,35]}},{"key":"geid_138_189","source":"scip-python python temp indexer `examples.benchmark.multi`/run().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#prepare().","attributes":{"type":"uses","at":[108,11,108,18]}},{"key":"geid_138_190","source":"scip-python python temp indexer `examples.benchmark.multi`/run().","target":"scip-python python temp indexer `examples.benchmark.multi`/download_image().","attributes":{"type":"uses","at":[116,12,116,26]}},{"key":"geid_138_191","source":"scip-python python temp indexer `examples.benchmark.multi`/run().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#resize().","attributes":{"type":"uses","at":[116,60,116,66]}},{"key":"geid_138_192","source":"scip-python python temp indexer `examples.benchmark.multi`/run().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#preprocess_image().","attributes":{"type":"uses","at":[117,26,117,42]}},{"key":"geid_138_193","source":"scip-python python temp indexer `examples.benchmark.multi`/run().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#stream.","attributes":{"type":"uses","at":[121,15,121,21]}},{"key":"geid_138_194","source":"scip-python python temp indexer `examples.benchmark.multi`/run().","target":"scip-python python temp indexer `examples.benchmark.multi`/_postprocess_image().","attributes":{"type":"uses","at":[124,23,124,41]}},{"key":"geid_138_195","source":"scip-python python temp indexer `examples.benchmark.multi`/run().","target":"scip-python python tqdm 4.66.1 `tqdm.std`/tqdm#","attributes":{"type":"uses","at":[131,13,131,17]}},{"key":"geid_138_196","source":"scip-python python temp indexer `examples.benchmark.multi`/run().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#stream.","attributes":{"type":"uses","at":[133,28,133,34]}},{"key":"geid_138_197","source":"scip-python python temp indexer `examples.benchmark.multi`/__init__:","target":"scip-python python temp indexer `examples.benchmark.multi`/","attributes":{"type":"uses","at":[144,3,144,11]}},{"key":"geid_138_198","source":"scip-python python temp indexer `examples.benchmark.multi`/__init__:","target":"scip-python python temp indexer `examples.benchmark.multi`/run().","attributes":{"type":"uses","at":[145,14,145,17]}},{"key":"geid_138_199","source":"scip-python python temp indexer `examples.benchmark.single`/__init__:","target":"scip-python python temp indexer `examples.benchmark.single`/run().","attributes":{"type":"defines","at":[23,0,133,36]}},{"key":"geid_138_200","source":"scip-python python temp indexer `examples.benchmark.single`/__init__:","target":"scip-python python temp indexer `examples.benchmark.single`/download_image().","attributes":{"type":"defines","at":[17,0,20,16]}},{"key":"geid_138_201","source":"scip-python python temp indexer `examples.benchmark.single`/__init__:","target":"scip-python python python-stdlib 3.11 io/__init__:","attributes":{"type":"uses","at":[0,7,0,9]}},{"key":"geid_138_202","source":"scip-python python temp indexer `examples.benchmark.single`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[1,7,1,9]}},{"key":"geid_138_203","source":"scip-python python temp indexer `examples.benchmark.single`/__init__:","target":"scip-python python python-stdlib 3.11 sys/__init__:","attributes":{"type":"uses","at":[2,7,2,10]}},{"key":"geid_138_204","source":"scip-python python temp indexer `examples.benchmark.single`/__init__:","target":"scip-python python python-stdlib 3.11 typing/__init__:","attributes":{"type":"uses","at":[3,5,3,11]}},{"key":"geid_138_205","source":"scip-python python temp indexer `examples.benchmark.single`/__init__:","target":"scip-python python python-stdlib 3.11 typing/List.","attributes":{"type":"uses","at":[3,19,3,23]}},{"key":"geid_138_206","source":"scip-python python temp indexer `examples.benchmark.single`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[3,25,3,32]}},{"key":"geid_138_207","source":"scip-python python temp indexer `examples.benchmark.single`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[3,34,3,42]}},{"key":"geid_138_208","source":"scip-python python temp indexer `examples.benchmark.single`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Dict.","attributes":{"type":"uses","at":[3,44,3,48]}},{"key":"geid_138_209","source":"scip-python python temp indexer `examples.benchmark.single`/__init__:","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[6,7,6,16]}},{"key":"geid_138_210","source":"scip-python python temp indexer `examples.benchmark.single`/__init__:","target":"scip-python python requests 2.31.0 requests/__init__:","attributes":{"type":"uses","at":[7,7,7,15]}},{"key":"geid_138_211","source":"scip-python python temp indexer `examples.benchmark.single`/__init__:","target":"scip-python python tqdm 4.66.1 tqdm/__init__:","attributes":{"type":"uses","at":[9,5,9,9]}},{"key":"geid_138_212","source":"scip-python python temp indexer `examples.benchmark.single`/__init__:","target":"scip-python python tqdm 4.66.1 `tqdm.std`/tqdm#","attributes":{"type":"uses","at":[9,17,9,21]}},{"key":"geid_138_213","source":"scip-python python temp indexer `examples.benchmark.single`/__init__:","target":"scip-python python python-stdlib 3.11 sys/__init__:","attributes":{"type":"uses","at":[12,0,12,3]}},{"key":"geid_138_214","source":"scip-python python temp indexer `examples.benchmark.single`/__init__:","target":"scip-python python python-stdlib 3.11 sys/path.path.","attributes":{"type":"uses","at":[12,4,12,8]}},{"key":"geid_138_215","source":"scip-python python temp indexer `examples.benchmark.single`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[12,16,12,18]}},{"key":"geid_138_216","source":"scip-python python temp indexer `examples.benchmark.single`/__init__:","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[12,19,12,23]}},{"key":"geid_138_217","source":"scip-python python temp indexer `examples.benchmark.single`/__init__:","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[12,24,12,28]}},{"key":"geid_138_218","source":"scip-python python temp indexer `examples.benchmark.single`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[12,29,12,31]}},{"key":"geid_138_219","source":"scip-python python temp indexer `examples.benchmark.single`/__init__:","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[12,32,12,36]}},{"key":"geid_138_220","source":"scip-python python temp indexer `examples.benchmark.single`/__init__:","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[12,37,12,44]}},{"key":"geid_138_221","source":"scip-python python temp indexer `examples.benchmark.single`/__init__:","target":"scip-python python temp indexer `utils.wrapper`/__init__:","attributes":{"type":"uses","at":[14,5,14,18]}},{"key":"geid_138_222","source":"scip-python python temp indexer `examples.benchmark.single`/__init__:","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#","attributes":{"type":"uses","at":[14,26,14,48]}},{"key":"geid_138_223","source":"scip-python python temp indexer `examples.benchmark.single`/download_image().","target":"scip-python python requests 2.31.0 requests/__init__:","attributes":{"type":"uses","at":[18,15,18,23]}},{"key":"geid_138_224","source":"scip-python python temp indexer `examples.benchmark.single`/download_image().","target":"scip-python python requests 2.31.0 `requests.api`/get().","attributes":{"type":"uses","at":[18,24,18,27]}},{"key":"geid_138_225","source":"scip-python python temp indexer `examples.benchmark.single`/download_image().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[19,12,19,21]}},{"key":"geid_138_226","source":"scip-python python temp indexer `examples.benchmark.single`/download_image().","target":"scip-python python Pillow 10.0.0 `pil.image`/open().","attributes":{"type":"uses","at":[19,22,19,26]}},{"key":"geid_138_227","source":"scip-python python temp indexer `examples.benchmark.single`/download_image().","target":"scip-python python python-stdlib 3.11 io/__init__:","attributes":{"type":"uses","at":[19,27,19,29]}},{"key":"geid_138_228","source":"scip-python python temp indexer `examples.benchmark.single`/download_image().","target":"scip-python python python-stdlib 3.11 io/BytesIO#","attributes":{"type":"uses","at":[19,30,19,37]}},{"key":"geid_138_229","source":"scip-python python temp indexer `examples.benchmark.single`/download_image().","target":"scip-python python requests 2.31.0 `requests.models`/Response#content().","attributes":{"type":"uses","at":[19,47,19,54]}},{"key":"geid_138_230","source":"scip-python python temp indexer `examples.benchmark.single`/run().","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[26,15,26,23]}},{"key":"geid_138_231","source":"scip-python python temp indexer `examples.benchmark.single`/run().","target":"scip-python python python-stdlib 3.11 typing/Dict.","attributes":{"type":"uses","at":[26,24,26,28]}},{"key":"geid_138_232","source":"scip-python python temp indexer `examples.benchmark.single`/run().","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[34,18,34,25]}},{"key":"geid_138_233","source":"scip-python python temp indexer `examples.benchmark.single`/run().","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[35,16,35,24]}},{"key":"geid_138_234","source":"scip-python python temp indexer `examples.benchmark.single`/run().","target":"scip-python python python-stdlib 3.11 typing/List.","attributes":{"type":"uses","at":[35,25,35,29]}},{"key":"geid_138_235","source":"scip-python python temp indexer `examples.benchmark.single`/run().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#","attributes":{"type":"uses","at":[75,13,75,35]}},{"key":"geid_138_236","source":"scip-python python temp indexer `examples.benchmark.single`/run().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#prepare().","attributes":{"type":"uses","at":[95,11,95,18]}},{"key":"geid_138_237","source":"scip-python python temp indexer `examples.benchmark.single`/run().","target":"scip-python python temp indexer `examples.benchmark.single`/download_image().","attributes":{"type":"uses","at":[103,23,103,37]}},{"key":"geid_138_238","source":"scip-python python temp indexer `examples.benchmark.single`/run().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#resize().","attributes":{"type":"uses","at":[103,71,103,77]}},{"key":"geid_138_239","source":"scip-python python temp indexer `examples.benchmark.single`/run().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#preprocess_image().","attributes":{"type":"uses","at":[109,30,109,46]}},{"key":"geid_138_240","source":"scip-python python temp indexer `examples.benchmark.single`/run().","target":"scip-python python tqdm 4.66.1 `tqdm.std`/tqdm#","attributes":{"type":"uses","at":[117,13,117,17]}},{"key":"geid_138_241","source":"scip-python python temp indexer `examples.benchmark.single`/run().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#preprocess_image().","attributes":{"type":"uses","at":[119,30,119,46]}},{"key":"geid_138_242","source":"scip-python python temp indexer `examples.benchmark.single`/run().","target":"scip-python python numpy 1.25.2 numpy/__init__:","attributes":{"type":"uses","at":[128,11,128,16]}},{"key":"geid_138_243","source":"scip-python python temp indexer `examples.benchmark.single`/run().","target":"scip-python python numpy 1.25.2 `numpy.core.multiarray`/array().","attributes":{"type":"uses","at":[130,24,130,29]}},{"key":"geid_138_244","source":"scip-python python temp indexer `examples.benchmark.single`/run().","target":"scip-python python numpy 1.25.2 `numpy.core.fromnumeric`/max.","attributes":{"type":"uses","at":[131,25,131,28]}},{"key":"geid_138_245","source":"scip-python python temp indexer `examples.benchmark.single`/run().","target":"scip-python python numpy 1.25.2 `numpy.core.fromnumeric`/min.","attributes":{"type":"uses","at":[132,25,132,28]}},{"key":"geid_138_246","source":"scip-python python temp indexer `examples.benchmark.single`/run().","target":"scip-python python numpy 1.25.2 `numpy.core.fromnumeric`/std().","attributes":{"type":"uses","at":[133,21,133,24]}},{"key":"geid_138_247","source":"scip-python python temp indexer `examples.benchmark.single`/__init__:","target":"scip-python python temp indexer `examples.benchmark.single`/","attributes":{"type":"uses","at":[136,3,136,11]}},{"key":"geid_138_248","source":"scip-python python temp indexer `examples.benchmark.single`/__init__:","target":"scip-python python temp indexer `examples.benchmark.single`/run().","attributes":{"type":"uses","at":[137,14,137,17]}},{"key":"geid_138_249","source":"scip-python python temp indexer `examples.img2img.multi`/__init__:","target":"scip-python python temp indexer `examples.img2img.multi`/main().","attributes":{"type":"defines","at":[15,0,118,66]}},{"key":"geid_138_250","source":"scip-python python temp indexer `examples.img2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 glob/__init__:","attributes":{"type":"uses","at":[0,7,0,11]}},{"key":"geid_138_251","source":"scip-python python temp indexer `examples.img2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[1,7,1,9]}},{"key":"geid_138_252","source":"scip-python python temp indexer `examples.img2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 sys/__init__:","attributes":{"type":"uses","at":[2,7,2,10]}},{"key":"geid_138_253","source":"scip-python python temp indexer `examples.img2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 typing/__init__:","attributes":{"type":"uses","at":[3,5,3,11]}},{"key":"geid_138_254","source":"scip-python python temp indexer `examples.img2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[3,19,3,26]}},{"key":"geid_138_255","source":"scip-python python temp indexer `examples.img2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Dict.","attributes":{"type":"uses","at":[3,28,3,32]}},{"key":"geid_138_256","source":"scip-python python temp indexer `examples.img2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[3,34,3,42]}},{"key":"geid_138_257","source":"scip-python python temp indexer `examples.img2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 sys/__init__:","attributes":{"type":"uses","at":[8,0,8,3]}},{"key":"geid_138_258","source":"scip-python python temp indexer `examples.img2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 sys/path.path.","attributes":{"type":"uses","at":[8,4,8,8]}},{"key":"geid_138_259","source":"scip-python python temp indexer `examples.img2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[8,16,8,18]}},{"key":"geid_138_260","source":"scip-python python temp indexer `examples.img2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[8,19,8,23]}},{"key":"geid_138_261","source":"scip-python python temp indexer `examples.img2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[8,24,8,28]}},{"key":"geid_138_262","source":"scip-python python temp indexer `examples.img2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[8,29,8,31]}},{"key":"geid_138_263","source":"scip-python python temp indexer `examples.img2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[8,32,8,36]}},{"key":"geid_138_264","source":"scip-python python temp indexer `examples.img2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[8,37,8,44]}},{"key":"geid_138_265","source":"scip-python python temp indexer `examples.img2img.multi`/__init__:","target":"scip-python python temp indexer `utils.wrapper`/__init__:","attributes":{"type":"uses","at":[10,5,10,18]}},{"key":"geid_138_266","source":"scip-python python temp indexer `examples.img2img.multi`/__init__:","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#","attributes":{"type":"uses","at":[10,26,10,48]}},{"key":"geid_138_267","source":"scip-python python temp indexer `examples.img2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[12,14,12,16]}},{"key":"geid_138_268","source":"scip-python python temp indexer `examples.img2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[12,17,12,21]}},{"key":"geid_138_269","source":"scip-python python temp indexer `examples.img2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[12,22,12,29]}},{"key":"geid_138_270","source":"scip-python python temp indexer `examples.img2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[12,30,12,32]}},{"key":"geid_138_271","source":"scip-python python temp indexer `examples.img2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[12,33,12,37]}},{"key":"geid_138_272","source":"scip-python python temp indexer `examples.img2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[12,38,12,45]}},{"key":"geid_138_273","source":"scip-python python temp indexer `examples.img2img.multi`/__init__:","target":"scip-python python temp indexer `examples.img2img.multi`/","attributes":{"type":"uses","at":[12,46,12,54]}},{"key":"geid_138_274","source":"scip-python python temp indexer `examples.img2img.multi`/main().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[16,17,16,19]}},{"key":"geid_138_275","source":"scip-python python temp indexer `examples.img2img.multi`/main().","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[16,20,16,24]}},{"key":"geid_138_276","source":"scip-python python temp indexer `examples.img2img.multi`/main().","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[16,25,16,29]}},{"key":"geid_138_277","source":"scip-python python temp indexer `examples.img2img.multi`/main().","target":"scip-python python temp indexer `examples.img2img.multi`/CURRENT_DIR.","attributes":{"type":"uses","at":[16,30,16,41]}},{"key":"geid_138_278","source":"scip-python python temp indexer `examples.img2img.multi`/main().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[17,18,17,20]}},{"key":"geid_138_279","source":"scip-python python temp indexer `examples.img2img.multi`/main().","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[17,21,17,25]}},{"key":"geid_138_280","source":"scip-python python temp indexer `examples.img2img.multi`/main().","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[17,26,17,30]}},{"key":"geid_138_281","source":"scip-python python temp indexer `examples.img2img.multi`/main().","target":"scip-python python temp indexer `examples.img2img.multi`/CURRENT_DIR.","attributes":{"type":"uses","at":[17,31,17,42]}},{"key":"geid_138_282","source":"scip-python python temp indexer `examples.img2img.multi`/main().","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[19,15,19,23]}},{"key":"geid_138_283","source":"scip-python python temp indexer `examples.img2img.multi`/main().","target":"scip-python python python-stdlib 3.11 typing/Dict.","attributes":{"type":"uses","at":[19,24,19,28]}},{"key":"geid_138_284","source":"scip-python python temp indexer `examples.img2img.multi`/main().","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[24,18,24,25]}},{"key":"geid_138_285","source":"scip-python python temp indexer `examples.img2img.multi`/main().","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[27,14,27,21]}},{"key":"geid_138_286","source":"scip-python python temp indexer `examples.img2img.multi`/main().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[71,11,71,13]}},{"key":"geid_138_287","source":"scip-python python temp indexer `examples.img2img.multi`/main().","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[71,14,71,18]}},{"key":"geid_138_288","source":"scip-python python temp indexer `examples.img2img.multi`/main().","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[71,19,71,25]}},{"key":"geid_138_289","source":"scip-python python temp indexer `examples.img2img.multi`/main().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[72,8,72,10]}},{"key":"geid_138_290","source":"scip-python python temp indexer `examples.img2img.multi`/main().","target":"scip-python python python-stdlib 3.11 os/makedirs().","attributes":{"type":"uses","at":[72,11,72,19]}},{"key":"geid_138_291","source":"scip-python python temp indexer `examples.img2img.multi`/main().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#","attributes":{"type":"uses","at":[77,13,77,35]}},{"key":"geid_138_292","source":"scip-python python temp indexer `examples.img2img.multi`/main().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#prepare().","attributes":{"type":"uses","at":[92,11,92,18]}},{"key":"geid_138_293","source":"scip-python python temp indexer `examples.img2img.multi`/main().","target":"scip-python python python-stdlib 3.11 glob/__init__:","attributes":{"type":"uses","at":[100,13,100,17]}},{"key":"geid_138_294","source":"scip-python python temp indexer `examples.img2img.multi`/main().","target":"scip-python python python-stdlib 3.11 glob/glob().","attributes":{"type":"uses","at":[100,18,100,22]}},{"key":"geid_138_295","source":"scip-python python temp indexer `examples.img2img.multi`/main().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[100,23,100,25]}},{"key":"geid_138_296","source":"scip-python python temp indexer `examples.img2img.multi`/main().","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[100,26,100,30]}},{"key":"geid_138_297","source":"scip-python python temp indexer `examples.img2img.multi`/main().","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[100,31,100,35]}},{"key":"geid_138_298","source":"scip-python python temp indexer `examples.img2img.multi`/main().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#batch_size.","attributes":{"type":"uses","at":[101,45,101,55]}},{"key":"geid_138_299","source":"scip-python python temp indexer `examples.img2img.multi`/main().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#batch_size.","attributes":{"type":"uses","at":[104,26,104,36]}},{"key":"geid_138_300","source":"scip-python python temp indexer `examples.img2img.multi`/main().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[117,19,117,21]}},{"key":"geid_138_301","source":"scip-python python temp indexer `examples.img2img.multi`/main().","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[117,22,117,26]}},{"key":"geid_138_302","source":"scip-python python temp indexer `examples.img2img.multi`/main().","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[117,27,117,35]}},{"key":"geid_138_303","source":"scip-python python temp indexer `examples.img2img.multi`/main().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[117,36,117,38]}},{"key":"geid_138_304","source":"scip-python python temp indexer `examples.img2img.multi`/main().","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[117,39,117,43]}},{"key":"geid_138_305","source":"scip-python python temp indexer `examples.img2img.multi`/main().","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[117,44,117,52]}},{"key":"geid_138_306","source":"scip-python python temp indexer `examples.img2img.multi`/main().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#save().","attributes":{"type":"uses","at":[118,21,118,25]}},{"key":"geid_138_307","source":"scip-python python temp indexer `examples.img2img.multi`/main().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[118,26,118,28]}},{"key":"geid_138_308","source":"scip-python python temp indexer `examples.img2img.multi`/main().","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[118,29,118,33]}},{"key":"geid_138_309","source":"scip-python python temp indexer `examples.img2img.multi`/main().","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[118,34,118,38]}},{"key":"geid_138_310","source":"scip-python python temp indexer `examples.img2img.multi`/__init__:","target":"scip-python python temp indexer `examples.img2img.multi`/","attributes":{"type":"uses","at":[121,3,121,11]}},{"key":"geid_138_311","source":"scip-python python temp indexer `examples.img2img.multi`/__init__:","target":"scip-python python temp indexer `examples.img2img.multi`/main().","attributes":{"type":"uses","at":[122,14,122,18]}},{"key":"geid_138_312","source":"scip-python python temp indexer `examples.img2img.single`/__init__:","target":"scip-python python temp indexer `examples.img2img.single`/main().","attributes":{"type":"defines","at":[14,0,102,29]}},{"key":"geid_138_313","source":"scip-python python temp indexer `examples.img2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[0,7,0,9]}},{"key":"geid_138_314","source":"scip-python python temp indexer `examples.img2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 sys/__init__:","attributes":{"type":"uses","at":[1,7,1,10]}},{"key":"geid_138_315","source":"scip-python python temp indexer `examples.img2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 typing/__init__:","attributes":{"type":"uses","at":[2,5,2,11]}},{"key":"geid_138_316","source":"scip-python python temp indexer `examples.img2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[2,19,2,26]}},{"key":"geid_138_317","source":"scip-python python temp indexer `examples.img2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Dict.","attributes":{"type":"uses","at":[2,28,2,32]}},{"key":"geid_138_318","source":"scip-python python temp indexer `examples.img2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[2,34,2,42]}},{"key":"geid_138_319","source":"scip-python python temp indexer `examples.img2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 sys/__init__:","attributes":{"type":"uses","at":[7,0,7,3]}},{"key":"geid_138_320","source":"scip-python python temp indexer `examples.img2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 sys/path.path.","attributes":{"type":"uses","at":[7,4,7,8]}},{"key":"geid_138_321","source":"scip-python python temp indexer `examples.img2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[7,16,7,18]}},{"key":"geid_138_322","source":"scip-python python temp indexer `examples.img2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[7,19,7,23]}},{"key":"geid_138_323","source":"scip-python python temp indexer `examples.img2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[7,24,7,28]}},{"key":"geid_138_324","source":"scip-python python temp indexer `examples.img2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[7,29,7,31]}},{"key":"geid_138_325","source":"scip-python python temp indexer `examples.img2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[7,32,7,36]}},{"key":"geid_138_326","source":"scip-python python temp indexer `examples.img2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[7,37,7,44]}},{"key":"geid_138_327","source":"scip-python python temp indexer `examples.img2img.single`/__init__:","target":"scip-python python temp indexer `utils.wrapper`/__init__:","attributes":{"type":"uses","at":[9,5,9,18]}},{"key":"geid_138_328","source":"scip-python python temp indexer `examples.img2img.single`/__init__:","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#","attributes":{"type":"uses","at":[9,26,9,48]}},{"key":"geid_138_329","source":"scip-python python temp indexer `examples.img2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[11,14,11,16]}},{"key":"geid_138_330","source":"scip-python python temp indexer `examples.img2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[11,17,11,21]}},{"key":"geid_138_331","source":"scip-python python temp indexer `examples.img2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[11,22,11,29]}},{"key":"geid_138_332","source":"scip-python python temp indexer `examples.img2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[11,30,11,32]}},{"key":"geid_138_333","source":"scip-python python temp indexer `examples.img2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[11,33,11,37]}},{"key":"geid_138_334","source":"scip-python python temp indexer `examples.img2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[11,38,11,45]}},{"key":"geid_138_335","source":"scip-python python temp indexer `examples.img2img.single`/__init__:","target":"scip-python python temp indexer `examples.img2img.single`/","attributes":{"type":"uses","at":[11,46,11,54]}},{"key":"geid_138_336","source":"scip-python python temp indexer `examples.img2img.single`/main().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[15,17,15,19]}},{"key":"geid_138_337","source":"scip-python python temp indexer `examples.img2img.single`/main().","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[15,20,15,24]}},{"key":"geid_138_338","source":"scip-python python temp indexer `examples.img2img.single`/main().","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[15,25,15,29]}},{"key":"geid_138_339","source":"scip-python python temp indexer `examples.img2img.single`/main().","target":"scip-python python temp indexer `examples.img2img.single`/CURRENT_DIR.","attributes":{"type":"uses","at":[15,30,15,41]}},{"key":"geid_138_340","source":"scip-python python temp indexer `examples.img2img.single`/main().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[16,18,16,20]}},{"key":"geid_138_341","source":"scip-python python temp indexer `examples.img2img.single`/main().","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[16,21,16,25]}},{"key":"geid_138_342","source":"scip-python python temp indexer `examples.img2img.single`/main().","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[16,26,16,30]}},{"key":"geid_138_343","source":"scip-python python temp indexer `examples.img2img.single`/main().","target":"scip-python python temp indexer `examples.img2img.single`/CURRENT_DIR.","attributes":{"type":"uses","at":[16,31,16,42]}},{"key":"geid_138_344","source":"scip-python python temp indexer `examples.img2img.single`/main().","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[18,15,18,23]}},{"key":"geid_138_345","source":"scip-python python temp indexer `examples.img2img.single`/main().","target":"scip-python python python-stdlib 3.11 typing/Dict.","attributes":{"type":"uses","at":[18,24,18,28]}},{"key":"geid_138_346","source":"scip-python python temp indexer `examples.img2img.single`/main().","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[23,18,23,25]}},{"key":"geid_138_347","source":"scip-python python temp indexer `examples.img2img.single`/main().","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[26,14,26,21]}},{"key":"geid_138_348","source":"scip-python python temp indexer `examples.img2img.single`/main().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#","attributes":{"type":"uses","at":[73,13,73,35]}},{"key":"geid_138_349","source":"scip-python python temp indexer `examples.img2img.single`/main().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#prepare().","attributes":{"type":"uses","at":[88,11,88,18]}},{"key":"geid_138_350","source":"scip-python python temp indexer `examples.img2img.single`/main().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#preprocess_image().","attributes":{"type":"uses","at":[96,26,96,42]}},{"key":"geid_138_351","source":"scip-python python temp indexer `examples.img2img.single`/main().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#batch_size.","attributes":{"type":"uses","at":[98,26,98,36]}},{"key":"geid_138_352","source":"scip-python python temp indexer `examples.img2img.single`/main().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#save().","attributes":{"type":"uses","at":[102,17,102,21]}},{"key":"geid_138_353","source":"scip-python python temp indexer `examples.img2img.single`/__init__:","target":"scip-python python temp indexer `examples.img2img.single`/","attributes":{"type":"uses","at":[105,3,105,11]}},{"key":"geid_138_354","source":"scip-python python temp indexer `examples.img2img.single`/__init__:","target":"scip-python python temp indexer `examples.img2img.single`/main().","attributes":{"type":"uses","at":[106,14,106,18]}},{"key":"geid_138_355","source":"scip-python python temp indexer `examples.optimal-performance.multi`/__init__:","target":"scip-python python temp indexer `examples.optimal-performance.multi`/main().","attributes":{"type":"defines","at":[167,0,185,20]}},{"key":"geid_138_356","source":"scip-python python temp indexer `examples.optimal-performance.multi`/__init__:","target":"scip-python python temp indexer `examples.optimal-performance.multi`/receive_images().","attributes":{"type":"defines","at":[135,0,164,14]}},{"key":"geid_138_357","source":"scip-python python temp indexer `examples.optimal-performance.multi`/__init__:","target":"scip-python python temp indexer `examples.optimal-performance.multi`/_receive_images().","attributes":{"type":"defines","at":[101,0,132,18]}},{"key":"geid_138_358","source":"scip-python python temp indexer `examples.optimal-performance.multi`/__init__:","target":"scip-python python temp indexer `examples.optimal-performance.multi`/image_generation_process().","attributes":{"type":"defines","at":[44,0,98,18]}},{"key":"geid_138_359","source":"scip-python python temp indexer `examples.optimal-performance.multi`/__init__:","target":"scip-python python temp indexer `examples.optimal-performance.multi`/update_image().","attributes":{"type":"defines","at":[22,0,41,26]}},{"key":"geid_138_360","source":"scip-python python temp indexer `examples.optimal-performance.multi`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[0,7,0,9]}},{"key":"geid_138_361","source":"scip-python python temp indexer `examples.optimal-performance.multi`/__init__:","target":"scip-python python python-stdlib 3.11 sys/__init__:","attributes":{"type":"uses","at":[1,7,1,10]}},{"key":"geid_138_362","source":"scip-python python temp indexer `examples.optimal-performance.multi`/__init__:","target":"scip-python python python-stdlib 3.11 threading/__init__:","attributes":{"type":"uses","at":[2,7,2,16]}},{"key":"geid_138_363","source":"scip-python python temp indexer `examples.optimal-performance.multi`/__init__:","target":"scip-python python python-stdlib 3.11 time/__init__:","attributes":{"type":"uses","at":[3,7,3,11]}},{"key":"geid_138_364","source":"scip-python python temp indexer `examples.optimal-performance.multi`/__init__:","target":"scip-python python python-stdlib 3.11 tkinter/__init__:","attributes":{"type":"uses","at":[4,7,4,14]}},{"key":"geid_138_365","source":"scip-python python temp indexer `examples.optimal-performance.multi`/__init__:","target":"scip-python python python-stdlib 3.11 multiprocessing/__init__:","attributes":{"type":"uses","at":[5,5,5,20]}},{"key":"geid_138_366","source":"scip-python python temp indexer `examples.optimal-performance.multi`/__init__:","target":"scip-python python python-stdlib 3.11 typing/__init__:","attributes":{"type":"uses","at":[6,5,6,11]}},{"key":"geid_138_367","source":"scip-python python temp indexer `examples.optimal-performance.multi`/__init__:","target":"scip-python python python-stdlib 3.11 typing/List.","attributes":{"type":"uses","at":[6,19,6,23]}},{"key":"geid_138_368","source":"scip-python python temp indexer `examples.optimal-performance.multi`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[6,25,6,32]}},{"key":"geid_138_369","source":"scip-python python temp indexer `examples.optimal-performance.multi`/__init__:","target":"scip-python python Pillow 10.0.0 PIL/__init__:","attributes":{"type":"uses","at":[9,5,9,8]}},{"key":"geid_138_370","source":"scip-python python temp indexer `examples.optimal-performance.multi`/__init__:","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[9,16,9,21]}},{"key":"geid_138_371","source":"scip-python python temp indexer `examples.optimal-performance.multi`/__init__:","target":"scip-python python Pillow 10.0.0 `PIL.ImageTk`/__init__:","attributes":{"type":"uses","at":[9,23,9,30]}},{"key":"geid_138_372","source":"scip-python python temp indexer `examples.optimal-performance.multi`/__init__:","target":"scip-python python temp indexer `streamdiffusion.image_utils`/__init__:","attributes":{"type":"uses","at":[11,5,11,32]}},{"key":"geid_138_373","source":"scip-python python temp indexer `examples.optimal-performance.multi`/__init__:","target":"scip-python python python-stdlib 3.11 sys/__init__:","attributes":{"type":"uses","at":[14,0,14,3]}},{"key":"geid_138_374","source":"scip-python python temp indexer `examples.optimal-performance.multi`/__init__:","target":"scip-python python python-stdlib 3.11 sys/path.path.","attributes":{"type":"uses","at":[14,4,14,8]}},{"key":"geid_138_375","source":"scip-python python temp indexer `examples.optimal-performance.multi`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[14,16,14,18]}},{"key":"geid_138_376","source":"scip-python python temp indexer `examples.optimal-performance.multi`/__init__:","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[14,19,14,23]}},{"key":"geid_138_377","source":"scip-python python temp indexer `examples.optimal-performance.multi`/__init__:","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[14,24,14,28]}},{"key":"geid_138_378","source":"scip-python python temp indexer `examples.optimal-performance.multi`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[14,29,14,31]}},{"key":"geid_138_379","source":"scip-python python temp indexer `examples.optimal-performance.multi`/__init__:","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[14,32,14,36]}},{"key":"geid_138_380","source":"scip-python python temp indexer `examples.optimal-performance.multi`/__init__:","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[14,37,14,44]}},{"key":"geid_138_381","source":"scip-python python temp indexer `examples.optimal-performance.multi`/__init__:","target":"scip-python python temp indexer `utils.wrapper`/__init__:","attributes":{"type":"uses","at":[16,5,16,18]}},{"key":"geid_138_382","source":"scip-python python temp indexer `examples.optimal-performance.multi`/__init__:","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#","attributes":{"type":"uses","at":[16,26,16,48]}},{"key":"geid_138_383","source":"scip-python python temp indexer `examples.optimal-performance.multi`/update_image().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[22,29,22,34]}},{"key":"geid_138_384","source":"scip-python python temp indexer `examples.optimal-performance.multi`/update_image().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#","attributes":{"type":"uses","at":[22,35,22,40]}},{"key":"geid_138_385","source":"scip-python python temp indexer `examples.optimal-performance.multi`/update_image().","target":"scip-python python python-stdlib 3.11 typing/List.","attributes":{"type":"uses","at":[22,50,22,54]}},{"key":"geid_138_386","source":"scip-python python temp indexer `examples.optimal-performance.multi`/update_image().","target":"scip-python python python-stdlib 3.11 tkinter/Label#","attributes":{"type":"uses","at":[22,58,22,63]}},{"key":"geid_138_387","source":"scip-python python temp indexer `examples.optimal-performance.multi`/update_image().","target":"scip-python python temp indexer `examples.optimal-performance.multi`/image_update_counter.","attributes":{"type":"uses","at":[33,11,33,31]}},{"key":"geid_138_388","source":"scip-python python temp indexer `examples.optimal-performance.multi`/update_image().","target":"scip-python python temp indexer `examples.optimal-performance.multi`/image_update_counter.","attributes":{"type":"uses","at":[34,19,34,39]}},{"key":"geid_138_389","source":"scip-python python temp indexer `examples.optimal-performance.multi`/update_image().","target":"scip-python python temp indexer `examples.optimal-performance.multi`/image_update_counter.","attributes":{"type":"uses","at":[35,4,35,24]}},{"key":"geid_138_390","source":"scip-python python temp indexer `examples.optimal-performance.multi`/update_image().","target":"scip-python python Pillow 10.0.0 `PIL.ImageTk`/__init__:","attributes":{"type":"uses","at":[39,15,39,22]}},{"key":"geid_138_391","source":"scip-python python temp indexer `examples.optimal-performance.multi`/update_image().","target":"scip-python python Pillow 10.0.0 `pil.imagetk`/PhotoImage#","attributes":{"type":"uses","at":[39,23,39,33]}},{"key":"geid_138_392","source":"scip-python python temp indexer `examples.optimal-performance.multi`/update_image().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#resize().","attributes":{"type":"uses","at":[39,45,39,51]}},{"key":"geid_138_393","source":"scip-python python temp indexer `examples.optimal-performance.multi`/update_image().","target":"scip-python python python-stdlib 3.11 tkinter/Label#configure().","attributes":{"type":"uses","at":[40,10,40,19]}},{"key":"geid_138_394","source":"scip-python python temp indexer `examples.optimal-performance.multi`/image_generation_process().","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[50,18,50,25]}},{"key":"geid_138_395","source":"scip-python python temp indexer `examples.optimal-performance.multi`/image_generation_process().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#","attributes":{"type":"uses","at":[70,13,70,35]}},{"key":"geid_138_396","source":"scip-python python temp indexer `examples.optimal-performance.multi`/image_generation_process().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#prepare().","attributes":{"type":"uses","at":[82,11,82,18]}},{"key":"geid_138_397","source":"scip-python python temp indexer `examples.optimal-performance.multi`/image_generation_process().","target":"scip-python python python-stdlib 3.11 time/__init__:","attributes":{"type":"uses","at":[89,25,89,29]}},{"key":"geid_138_398","source":"scip-python python temp indexer `examples.optimal-performance.multi`/image_generation_process().","target":"scip-python python python-stdlib 3.11 time/time().","attributes":{"type":"uses","at":[89,30,89,34]}},{"key":"geid_138_399","source":"scip-python python temp indexer `examples.optimal-performance.multi`/image_generation_process().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#stream.","attributes":{"type":"uses","at":[91,31,91,37]}},{"key":"geid_138_400","source":"scip-python python temp indexer `examples.optimal-performance.multi`/image_generation_process().","target":"scip-python python python-stdlib 3.11 time/__init__:","attributes":{"type":"uses","at":[94,23,94,27]}},{"key":"geid_138_401","source":"scip-python python temp indexer `examples.optimal-performance.multi`/image_generation_process().","target":"scip-python python python-stdlib 3.11 time/time().","attributes":{"type":"uses","at":[94,28,94,32]}},{"key":"geid_138_402","source":"scip-python python temp indexer `examples.optimal-performance.multi`/_receive_images().","target":"scip-python python python-stdlib 3.11 typing/List.","attributes":{"type":"uses","at":[102,44,102,48]}},{"key":"geid_138_403","source":"scip-python python temp indexer `examples.optimal-performance.multi`/_receive_images().","target":"scip-python python python-stdlib 3.11 tkinter/Label#","attributes":{"type":"uses","at":[102,52,102,57]}},{"key":"geid_138_404","source":"scip-python python temp indexer `examples.optimal-performance.multi`/_receive_images().","target":"scip-python python python-stdlib 3.11 tkinter/Label#","attributes":{"type":"uses","at":[102,74,102,79]}},{"key":"geid_138_405","source":"scip-python python temp indexer `examples.optimal-performance.multi`/_receive_images().","target":"scip-python python python-stdlib 3.11 queue/Queue#empty().","attributes":{"type":"uses","at":[120,25,120,30]}},{"key":"geid_138_406","source":"scip-python python temp indexer `examples.optimal-performance.multi`/_receive_images().","target":"scip-python python python-stdlib 3.11 tkinter/Misc#after().","attributes":{"type":"uses","at":[122,30,122,35]}},{"key":"geid_138_407","source":"scip-python python temp indexer `examples.optimal-performance.multi`/_receive_images().","target":"scip-python python temp indexer `examples.optimal-performance.multi`/update_image().","attributes":{"type":"uses","at":[122,39,122,51]}},{"key":"geid_138_408","source":"scip-python python temp indexer `examples.optimal-performance.multi`/_receive_images().","target":"scip-python python python-stdlib 3.11 queue/Queue#empty().","attributes":{"type":"uses","at":[127,29,127,34]}},{"key":"geid_138_409","source":"scip-python python temp indexer `examples.optimal-performance.multi`/_receive_images().","target":"scip-python python python-stdlib 3.11 tkinter/Label#config.","attributes":{"type":"uses","at":[128,26,128,32]}},{"key":"geid_138_410","source":"scip-python python temp indexer `examples.optimal-performance.multi`/_receive_images().","target":"scip-python python python-stdlib 3.11 time/__init__:","attributes":{"type":"uses","at":[130,12,130,16]}},{"key":"geid_138_411","source":"scip-python python temp indexer `examples.optimal-performance.multi`/_receive_images().","target":"scip-python python python-stdlib 3.11 time/sleep().","attributes":{"type":"uses","at":[130,17,130,22]}},{"key":"geid_138_412","source":"scip-python python temp indexer `examples.optimal-performance.multi`/receive_images().","target":"scip-python python python-stdlib 3.11 tkinter/Tk#","attributes":{"type":"uses","at":[146,14,146,16]}},{"key":"geid_138_413","source":"scip-python python temp indexer `examples.optimal-performance.multi`/receive_images().","target":"scip-python python python-stdlib 3.11 tkinter/Wm#title.","attributes":{"type":"uses","at":[147,9,147,14]}},{"key":"geid_138_414","source":"scip-python python temp indexer `examples.optimal-performance.multi`/receive_images().","target":"scip-python python python-stdlib 3.11 tkinter/Label#","attributes":{"type":"uses","at":[148,17,148,22]}},{"key":"geid_138_415","source":"scip-python python temp indexer `examples.optimal-performance.multi`/receive_images().","target":"scip-python python python-stdlib 3.11 tkinter/Grid#grid.","attributes":{"type":"uses","at":[149,14,149,18]}},{"key":"geid_138_416","source":"scip-python python temp indexer `examples.optimal-performance.multi`/receive_images().","target":"scip-python python python-stdlib 3.11 tkinter/Grid#grid.","attributes":{"type":"uses","at":[150,14,150,18]}},{"key":"geid_138_417","source":"scip-python python temp indexer `examples.optimal-performance.multi`/receive_images().","target":"scip-python python python-stdlib 3.11 tkinter/Grid#grid.","attributes":{"type":"uses","at":[151,14,151,18]}},{"key":"geid_138_418","source":"scip-python python temp indexer `examples.optimal-performance.multi`/receive_images().","target":"scip-python python python-stdlib 3.11 tkinter/Grid#grid.","attributes":{"type":"uses","at":[152,14,152,18]}},{"key":"geid_138_419","source":"scip-python python temp indexer `examples.optimal-performance.multi`/receive_images().","target":"scip-python python python-stdlib 3.11 tkinter/Label#","attributes":{"type":"uses","at":[153,19,153,24]}},{"key":"geid_138_420","source":"scip-python python temp indexer `examples.optimal-performance.multi`/receive_images().","target":"scip-python python python-stdlib 3.11 tkinter/Grid#grid.","attributes":{"type":"uses","at":[154,14,154,18]}},{"key":"geid_138_421","source":"scip-python python temp indexer `examples.optimal-performance.multi`/receive_images().","target":"scip-python python python-stdlib 3.11 threading/__init__:","attributes":{"type":"uses","at":[156,13,156,22]}},{"key":"geid_138_422","source":"scip-python python temp indexer `examples.optimal-performance.multi`/receive_images().","target":"scip-python python python-stdlib 3.11 threading/Thread#","attributes":{"type":"uses","at":[156,23,156,29]}},{"key":"geid_138_423","source":"scip-python python temp indexer `examples.optimal-performance.multi`/receive_images().","target":"scip-python python temp indexer `examples.optimal-performance.multi`/_receive_images().","attributes":{"type":"uses","at":[157,15,157,30]}},{"key":"geid_138_424","source":"scip-python python temp indexer `examples.optimal-performance.multi`/receive_images().","target":"scip-python python python-stdlib 3.11 threading/Thread#start().","attributes":{"type":"uses","at":[159,11,159,16]}},{"key":"geid_138_425","source":"scip-python python temp indexer `examples.optimal-performance.multi`/receive_images().","target":"scip-python python python-stdlib 3.11 tkinter/Misc#mainloop().","attributes":{"type":"uses","at":[162,13,162,21]}},{"key":"geid_138_426","source":"scip-python python temp indexer `examples.optimal-performance.multi`/main().","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[171,18,171,25]}},{"key":"geid_138_427","source":"scip-python python temp indexer `examples.optimal-performance.multi`/main().","target":"scip-python python temp indexer `examples.optimal-performance.multi`/image_generation_process().","attributes":{"type":"uses","at":[179,15,179,39]}},{"key":"geid_138_428","source":"scip-python python temp indexer `examples.optimal-performance.multi`/main().","target":"scip-python python temp indexer `examples.optimal-performance.multi`/receive_images().","attributes":{"type":"uses","at":[184,30,184,44]}},{"key":"geid_138_429","source":"scip-python python temp indexer `examples.optimal-performance.multi`/__init__:","target":"scip-python python temp indexer `examples.optimal-performance.multi`/","attributes":{"type":"uses","at":[188,3,188,11]}},{"key":"geid_138_430","source":"scip-python python temp indexer `examples.optimal-performance.multi`/__init__:","target":"scip-python python temp indexer `examples.optimal-performance.multi`/main().","attributes":{"type":"uses","at":[189,14,189,18]}},{"key":"geid_138_431","source":"scip-python python temp indexer `examples.optimal-performance.single`/__init__:","target":"scip-python python temp indexer `examples.optimal-performance.single`/main().","attributes":{"type":"defines","at":[66,0,83,20]}},{"key":"geid_138_432","source":"scip-python python temp indexer `examples.optimal-performance.single`/__init__:","target":"scip-python python temp indexer `examples.optimal-performance.single`/image_generation_process().","attributes":{"type":"defines","at":[13,0,64,18]}},{"key":"geid_138_433","source":"scip-python python temp indexer `examples.optimal-performance.single`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[0,7,0,9]}},{"key":"geid_138_434","source":"scip-python python temp indexer `examples.optimal-performance.single`/__init__:","target":"scip-python python python-stdlib 3.11 sys/__init__:","attributes":{"type":"uses","at":[1,7,1,10]}},{"key":"geid_138_435","source":"scip-python python temp indexer `examples.optimal-performance.single`/__init__:","target":"scip-python python python-stdlib 3.11 time/__init__:","attributes":{"type":"uses","at":[2,7,2,11]}},{"key":"geid_138_436","source":"scip-python python temp indexer `examples.optimal-performance.single`/__init__:","target":"scip-python python python-stdlib 3.11 multiprocessing/__init__:","attributes":{"type":"uses","at":[3,5,3,20]}},{"key":"geid_138_437","source":"scip-python python temp indexer `examples.optimal-performance.single`/__init__:","target":"scip-python python python-stdlib 3.11 typing/__init__:","attributes":{"type":"uses","at":[4,5,4,11]}},{"key":"geid_138_438","source":"scip-python python temp indexer `examples.optimal-performance.single`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[4,19,4,26]}},{"key":"geid_138_439","source":"scip-python python temp indexer `examples.optimal-performance.single`/__init__:","target":"scip-python python python-stdlib 3.11 sys/__init__:","attributes":{"type":"uses","at":[8,0,8,3]}},{"key":"geid_138_440","source":"scip-python python temp indexer `examples.optimal-performance.single`/__init__:","target":"scip-python python python-stdlib 3.11 sys/path.path.","attributes":{"type":"uses","at":[8,4,8,8]}},{"key":"geid_138_441","source":"scip-python python temp indexer `examples.optimal-performance.single`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[8,16,8,18]}},{"key":"geid_138_442","source":"scip-python python temp indexer `examples.optimal-performance.single`/__init__:","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[8,19,8,23]}},{"key":"geid_138_443","source":"scip-python python temp indexer `examples.optimal-performance.single`/__init__:","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[8,24,8,28]}},{"key":"geid_138_444","source":"scip-python python temp indexer `examples.optimal-performance.single`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[8,29,8,31]}},{"key":"geid_138_445","source":"scip-python python temp indexer `examples.optimal-performance.single`/__init__:","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[8,32,8,36]}},{"key":"geid_138_446","source":"scip-python python temp indexer `examples.optimal-performance.single`/__init__:","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[8,37,8,44]}},{"key":"geid_138_447","source":"scip-python python temp indexer `examples.optimal-performance.single`/__init__:","target":"scip-python python temp indexer `utils.viewer`/__init__:","attributes":{"type":"uses","at":[10,5,10,17]}},{"key":"geid_138_448","source":"scip-python python temp indexer `examples.optimal-performance.single`/__init__:","target":"scip-python python temp indexer `utils.viewer`/receive_images().","attributes":{"type":"uses","at":[10,25,10,39]}},{"key":"geid_138_449","source":"scip-python python temp indexer `examples.optimal-performance.single`/__init__:","target":"scip-python python temp indexer `utils.wrapper`/__init__:","attributes":{"type":"uses","at":[11,5,11,18]}},{"key":"geid_138_450","source":"scip-python python temp indexer `examples.optimal-performance.single`/__init__:","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#","attributes":{"type":"uses","at":[11,26,11,48]}},{"key":"geid_138_451","source":"scip-python python temp indexer `examples.optimal-performance.single`/image_generation_process().","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[18,18,18,25]}},{"key":"geid_138_452","source":"scip-python python temp indexer `examples.optimal-performance.single`/image_generation_process().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#","attributes":{"type":"uses","at":[36,13,36,35]}},{"key":"geid_138_453","source":"scip-python python temp indexer `examples.optimal-performance.single`/image_generation_process().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#prepare().","attributes":{"type":"uses","at":[48,11,48,18]}},{"key":"geid_138_454","source":"scip-python python temp indexer `examples.optimal-performance.single`/image_generation_process().","target":"scip-python python python-stdlib 3.11 time/__init__:","attributes":{"type":"uses","at":[55,25,55,29]}},{"key":"geid_138_455","source":"scip-python python temp indexer `examples.optimal-performance.single`/image_generation_process().","target":"scip-python python python-stdlib 3.11 time/time().","attributes":{"type":"uses","at":[55,30,55,34]}},{"key":"geid_138_456","source":"scip-python python temp indexer `examples.optimal-performance.single`/image_generation_process().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#stream.","attributes":{"type":"uses","at":[57,31,57,37]}},{"key":"geid_138_457","source":"scip-python python temp indexer `examples.optimal-performance.single`/image_generation_process().","target":"scip-python python python-stdlib 3.11 time/__init__:","attributes":{"type":"uses","at":[60,23,60,27]}},{"key":"geid_138_458","source":"scip-python python temp indexer `examples.optimal-performance.single`/image_generation_process().","target":"scip-python python python-stdlib 3.11 time/time().","attributes":{"type":"uses","at":[60,28,60,32]}},{"key":"geid_138_459","source":"scip-python python temp indexer `examples.optimal-performance.single`/main().","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[69,18,69,25]}},{"key":"geid_138_460","source":"scip-python python temp indexer `examples.optimal-performance.single`/main().","target":"scip-python python temp indexer `examples.optimal-performance.single`/image_generation_process().","attributes":{"type":"uses","at":[77,15,77,39]}},{"key":"geid_138_461","source":"scip-python python temp indexer `examples.optimal-performance.single`/main().","target":"scip-python python temp indexer `utils.viewer`/receive_images().","attributes":{"type":"uses","at":[82,30,82,44]}},{"key":"geid_138_462","source":"scip-python python temp indexer `examples.optimal-performance.single`/__init__:","target":"scip-python python temp indexer `examples.optimal-performance.single`/","attributes":{"type":"uses","at":[86,3,86,11]}},{"key":"geid_138_463","source":"scip-python python temp indexer `examples.optimal-performance.single`/__init__:","target":"scip-python python temp indexer `examples.optimal-performance.single`/main().","attributes":{"type":"uses","at":[87,14,87,18]}},{"key":"geid_138_464","source":"scip-python python temp indexer `examples.screen.main`/__init__:","target":"scip-python python temp indexer `examples.screen.main`/main().","attributes":{"type":"defines","at":[195,0,249,20]}},{"key":"geid_138_465","source":"scip-python python temp indexer `examples.screen.main`/__init__:","target":"scip-python python temp indexer `examples.screen.main`/image_generation_process().","attributes":{"type":"defines","at":[60,0,192,18]}},{"key":"geid_138_466","source":"scip-python python temp indexer `examples.screen.main`/dummy_screen().","target":"scip-python python temp indexer `examples.screen.main`/dummy_screen().update_geometry().","attributes":{"type":"defines","at":[52,4,55,29]}},{"key":"geid_138_467","source":"scip-python python temp indexer `examples.screen.main`/dummy_screen().","target":"scip-python python temp indexer `examples.screen.main`/dummy_screen().destroy().","attributes":{"type":"defines","at":[49,4,50,22]}},{"key":"geid_138_468","source":"scip-python python temp indexer `examples.screen.main`/__init__:","target":"scip-python python temp indexer `examples.screen.main`/dummy_screen().","attributes":{"type":"defines","at":[39,0,58,71]}},{"key":"geid_138_469","source":"scip-python python temp indexer `examples.screen.main`/__init__:","target":"scip-python python temp indexer `examples.screen.main`/screen().","attributes":{"type":"defines","at":[23,0,37,22]}},{"key":"geid_138_470","source":"scip-python python temp indexer `examples.screen.main`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[0,7,0,9]}},{"key":"geid_138_471","source":"scip-python python temp indexer `examples.screen.main`/__init__:","target":"scip-python python python-stdlib 3.11 sys/__init__:","attributes":{"type":"uses","at":[1,7,1,10]}},{"key":"geid_138_472","source":"scip-python python temp indexer `examples.screen.main`/__init__:","target":"scip-python python python-stdlib 3.11 time/__init__:","attributes":{"type":"uses","at":[2,7,2,11]}},{"key":"geid_138_473","source":"scip-python python temp indexer `examples.screen.main`/__init__:","target":"scip-python python python-stdlib 3.11 threading/__init__:","attributes":{"type":"uses","at":[3,7,3,16]}},{"key":"geid_138_474","source":"scip-python python temp indexer `examples.screen.main`/__init__:","target":"scip-python python python-stdlib 3.11 multiprocessing/__init__:","attributes":{"type":"uses","at":[4,5,4,20]}},{"key":"geid_138_475","source":"scip-python python temp indexer `examples.screen.main`/__init__:","target":"scip-python python python-stdlib 3.11 typing/__init__:","attributes":{"type":"uses","at":[5,5,5,11]}},{"key":"geid_138_476","source":"scip-python python temp indexer `examples.screen.main`/__init__:","target":"scip-python python python-stdlib 3.11 typing/List.","attributes":{"type":"uses","at":[5,19,5,23]}},{"key":"geid_138_477","source":"scip-python python temp indexer `examples.screen.main`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[5,25,5,32]}},{"key":"geid_138_478","source":"scip-python python temp indexer `examples.screen.main`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Dict.","attributes":{"type":"uses","at":[5,34,5,38]}},{"key":"geid_138_479","source":"scip-python python temp indexer `examples.screen.main`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[5,40,5,48]}},{"key":"geid_138_480","source":"scip-python python temp indexer `examples.screen.main`/__init__:","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[7,7,7,16]}},{"key":"geid_138_481","source":"scip-python python temp indexer `examples.screen.main`/__init__:","target":"scip-python python temp indexer `streamdiffusion.image_utils`/__init__:","attributes":{"type":"uses","at":[8,5,8,32]}},{"key":"geid_138_482","source":"scip-python python temp indexer `examples.screen.main`/__init__:","target":"scip-python python python-stdlib 3.11 tkinter/__init__:","attributes":{"type":"uses","at":[11,7,11,14]}},{"key":"geid_138_483","source":"scip-python python temp indexer `examples.screen.main`/__init__:","target":"scip-python python python-stdlib 3.11 sys/__init__:","attributes":{"type":"uses","at":[13,0,13,3]}},{"key":"geid_138_484","source":"scip-python python temp indexer `examples.screen.main`/__init__:","target":"scip-python python python-stdlib 3.11 sys/path.path.","attributes":{"type":"uses","at":[13,4,13,8]}},{"key":"geid_138_485","source":"scip-python python temp indexer `examples.screen.main`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[13,16,13,18]}},{"key":"geid_138_486","source":"scip-python python temp indexer `examples.screen.main`/__init__:","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[13,19,13,23]}},{"key":"geid_138_487","source":"scip-python python temp indexer `examples.screen.main`/__init__:","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[13,24,13,28]}},{"key":"geid_138_488","source":"scip-python python temp indexer `examples.screen.main`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[13,29,13,31]}},{"key":"geid_138_489","source":"scip-python python temp indexer `examples.screen.main`/__init__:","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[13,32,13,36]}},{"key":"geid_138_490","source":"scip-python python temp indexer `examples.screen.main`/__init__:","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[13,37,13,44]}},{"key":"geid_138_491","source":"scip-python python temp indexer `examples.screen.main`/__init__:","target":"scip-python python temp indexer `utils.viewer`/__init__:","attributes":{"type":"uses","at":[15,5,15,17]}},{"key":"geid_138_492","source":"scip-python python temp indexer `examples.screen.main`/__init__:","target":"scip-python python temp indexer `utils.viewer`/receive_images().","attributes":{"type":"uses","at":[15,25,15,39]}},{"key":"geid_138_493","source":"scip-python python temp indexer `examples.screen.main`/__init__:","target":"scip-python python temp indexer `utils.wrapper`/__init__:","attributes":{"type":"uses","at":[16,5,16,18]}},{"key":"geid_138_494","source":"scip-python python temp indexer `examples.screen.main`/__init__:","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#","attributes":{"type":"uses","at":[16,26,16,48]}},{"key":"geid_138_495","source":"scip-python python temp indexer `examples.screen.main`/screen().","target":"scip-python python python-stdlib 3.11 typing/Dict.","attributes":{"type":"uses","at":[26,13,26,17]}},{"key":"geid_138_496","source":"scip-python python temp indexer `examples.screen.main`/screen().","target":"scip-python python temp indexer `examples.screen.main`/inputs.","attributes":{"type":"uses","at":[28,11,28,17]}},{"key":"geid_138_497","source":"scip-python python temp indexer `examples.screen.main`/screen().","target":"scip-python python temp indexer `examples.screen.main`/stop_capture.","attributes":{"type":"uses","at":[29,11,29,23]}},{"key":"geid_138_498","source":"scip-python python temp indexer `examples.screen.main`/screen().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[33,18,33,27]}},{"key":"geid_138_499","source":"scip-python python temp indexer `examples.screen.main`/screen().","target":"scip-python python Pillow 10.0.0 `pil.image`/frombytes().","attributes":{"type":"uses","at":[33,28,33,37]}},{"key":"geid_138_500","source":"scip-python python temp indexer `examples.screen.main`/screen().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#resize().","attributes":{"type":"uses","at":[34,16,34,22]}},{"key":"geid_138_501","source":"scip-python python temp indexer `examples.screen.main`/screen().","target":"scip-python python temp indexer `examples.screen.main`/inputs.","attributes":{"type":"uses","at":[35,12,35,18]}},{"key":"geid_138_502","source":"scip-python python temp indexer `examples.screen.main`/screen().","target":"scip-python python temp indexer `examples.screen.main`/stop_capture.","attributes":{"type":"uses","at":[36,15,36,27]}},{"key":"geid_138_503","source":"scip-python python temp indexer `examples.screen.main`/dummy_screen().","target":"scip-python python python-stdlib 3.11 tkinter/Tk#","attributes":{"type":"uses","at":[43,14,43,16]}},{"key":"geid_138_504","source":"scip-python python temp indexer `examples.screen.main`/dummy_screen().","target":"scip-python python python-stdlib 3.11 tkinter/Wm#title.","attributes":{"type":"uses","at":[44,9,44,14]}},{"key":"geid_138_505","source":"scip-python python temp indexer `examples.screen.main`/dummy_screen().","target":"scip-python python python-stdlib 3.11 tkinter/Wm#geometry.","attributes":{"type":"uses","at":[45,9,45,17]}},{"key":"geid_138_506","source":"scip-python python temp indexer `examples.screen.main`/dummy_screen().","target":"scip-python python python-stdlib 3.11 tkinter/Wm#resizable.","attributes":{"type":"uses","at":[46,9,46,18]}},{"key":"geid_138_507","source":"scip-python python temp indexer `examples.screen.main`/dummy_screen().","target":"scip-python python python-stdlib 3.11 tkinter/Wm#attributes.","attributes":{"type":"uses","at":[47,9,47,19]}},{"key":"geid_138_508","source":"scip-python python temp indexer `examples.screen.main`/dummy_screen().","target":"scip-python python python-stdlib 3.11 tkinter/Tk#configure().","attributes":{"type":"uses","at":[48,9,48,18]}},{"key":"geid_138_509","source":"scip-python python temp indexer `examples.screen.main`/dummy_screen().destroy().","target":"scip-python python python-stdlib 3.11 tkinter/Tk#destroy().","attributes":{"type":"uses","at":[50,13,50,20]}},{"key":"geid_138_510","source":"scip-python python temp indexer `examples.screen.main`/dummy_screen().","target":"scip-python python python-stdlib 3.11 tkinter/Misc#bind().","attributes":{"type":"uses","at":[51,9,51,13]}},{"key":"geid_138_511","source":"scip-python python temp indexer `examples.screen.main`/dummy_screen().update_geometry().","target":"scip-python python temp indexer `examples.screen.main`/top.","attributes":{"type":"uses","at":[53,15,53,18]}},{"key":"geid_138_512","source":"scip-python python temp indexer `examples.screen.main`/dummy_screen().update_geometry().","target":"scip-python python temp indexer `examples.screen.main`/left.","attributes":{"type":"uses","at":[53,20,53,24]}},{"key":"geid_138_513","source":"scip-python python temp indexer `examples.screen.main`/dummy_screen().update_geometry().","target":"scip-python python temp indexer `examples.screen.main`/top.","attributes":{"type":"uses","at":[54,8,54,11]}},{"key":"geid_138_514","source":"scip-python python temp indexer `examples.screen.main`/dummy_screen().update_geometry().","target":"scip-python python python-stdlib 3.11 tkinter/Misc#winfo_y().","attributes":{"type":"uses","at":[54,19,54,26]}},{"key":"geid_138_515","source":"scip-python python temp indexer `examples.screen.main`/dummy_screen().update_geometry().","target":"scip-python python temp indexer `examples.screen.main`/left.","attributes":{"type":"uses","at":[55,8,55,12]}},{"key":"geid_138_516","source":"scip-python python temp indexer `examples.screen.main`/dummy_screen().update_geometry().","target":"scip-python python python-stdlib 3.11 tkinter/Misc#winfo_x().","attributes":{"type":"uses","at":[55,20,55,27]}},{"key":"geid_138_517","source":"scip-python python temp indexer `examples.screen.main`/dummy_screen().","target":"scip-python python python-stdlib 3.11 tkinter/Misc#bind().","attributes":{"type":"uses","at":[56,9,56,13]}},{"key":"geid_138_518","source":"scip-python python temp indexer `examples.screen.main`/dummy_screen().","target":"scip-python python python-stdlib 3.11 tkinter/Misc#mainloop().","attributes":{"type":"uses","at":[57,9,57,17]}},{"key":"geid_138_519","source":"scip-python python temp indexer `examples.screen.main`/dummy_screen().","target":"scip-python python temp indexer `examples.screen.main`/top.","attributes":{"type":"uses","at":[58,19,58,22]}},{"key":"geid_138_520","source":"scip-python python temp indexer `examples.screen.main`/dummy_screen().","target":"scip-python python temp indexer `examples.screen.main`/left.","attributes":{"type":"uses","at":[58,32,58,36]}},{"key":"geid_138_521","source":"scip-python python temp indexer `examples.screen.main`/image_generation_process().","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[64,15,64,23]}},{"key":"geid_138_522","source":"scip-python python temp indexer `examples.screen.main`/image_generation_process().","target":"scip-python python python-stdlib 3.11 typing/Dict.","attributes":{"type":"uses","at":[64,24,64,28]}},{"key":"geid_138_523","source":"scip-python python temp indexer `examples.screen.main`/image_generation_process().","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[70,18,70,25]}},{"key":"geid_138_524","source":"scip-python python temp indexer `examples.screen.main`/image_generation_process().","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[73,14,73,21]}},{"key":"geid_138_525","source":"scip-python python temp indexer `examples.screen.main`/image_generation_process().","target":"scip-python python python-stdlib 3.11 typing/Dict.","attributes":{"type":"uses","at":[80,13,80,17]}},{"key":"geid_138_526","source":"scip-python python temp indexer `examples.screen.main`/image_generation_process().","target":"scip-python python temp indexer `examples.screen.main`/inputs.","attributes":{"type":"uses","at":[134,11,134,17]}},{"key":"geid_138_527","source":"scip-python python temp indexer `examples.screen.main`/image_generation_process().","target":"scip-python python temp indexer `examples.screen.main`/stop_capture.","attributes":{"type":"uses","at":[135,11,135,23]}},{"key":"geid_138_528","source":"scip-python python temp indexer `examples.screen.main`/image_generation_process().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#","attributes":{"type":"uses","at":[136,13,136,35]}},{"key":"geid_138_529","source":"scip-python python temp indexer `examples.screen.main`/image_generation_process().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#prepare().","attributes":{"type":"uses","at":[155,11,155,18]}},{"key":"geid_138_530","source":"scip-python python temp indexer `examples.screen.main`/image_generation_process().","target":"scip-python python python-stdlib 3.11 threading/__init__:","attributes":{"type":"uses","at":[163,19,163,28]}},{"key":"geid_138_531","source":"scip-python python temp indexer `examples.screen.main`/image_generation_process().","target":"scip-python python python-stdlib 3.11 threading/Thread#","attributes":{"type":"uses","at":[163,29,163,35]}},{"key":"geid_138_532","source":"scip-python python temp indexer `examples.screen.main`/image_generation_process().","target":"scip-python python temp indexer `examples.screen.main`/screen().","attributes":{"type":"uses","at":[163,43,163,49]}},{"key":"geid_138_533","source":"scip-python python temp indexer `examples.screen.main`/image_generation_process().","target":"scip-python python python-stdlib 3.11 threading/Thread#start().","attributes":{"type":"uses","at":[164,17,164,22]}},{"key":"geid_138_534","source":"scip-python python temp indexer `examples.screen.main`/image_generation_process().","target":"scip-python python python-stdlib 3.11 time/__init__:","attributes":{"type":"uses","at":[165,4,165,8]}},{"key":"geid_138_535","source":"scip-python python temp indexer `examples.screen.main`/image_generation_process().","target":"scip-python python python-stdlib 3.11 time/sleep().","attributes":{"type":"uses","at":[165,9,165,14]}},{"key":"geid_138_536","source":"scip-python python temp indexer `examples.screen.main`/image_generation_process().","target":"scip-python python temp indexer `examples.screen.main`/inputs.","attributes":{"type":"uses","at":[169,19,169,25]}},{"key":"geid_138_537","source":"scip-python python temp indexer `examples.screen.main`/image_generation_process().","target":"scip-python python python-stdlib 3.11 time/__init__:","attributes":{"type":"uses","at":[170,16,170,20]}},{"key":"geid_138_538","source":"scip-python python temp indexer `examples.screen.main`/image_generation_process().","target":"scip-python python python-stdlib 3.11 time/sleep().","attributes":{"type":"uses","at":[170,21,170,26]}},{"key":"geid_138_539","source":"scip-python python temp indexer `examples.screen.main`/image_generation_process().","target":"scip-python python python-stdlib 3.11 time/__init__:","attributes":{"type":"uses","at":[172,25,172,29]}},{"key":"geid_138_540","source":"scip-python python temp indexer `examples.screen.main`/image_generation_process().","target":"scip-python python python-stdlib 3.11 time/time().","attributes":{"type":"uses","at":[172,30,172,34]}},{"key":"geid_138_541","source":"scip-python python temp indexer `examples.screen.main`/image_generation_process().","target":"scip-python python temp indexer `examples.screen.main`/inputs.","attributes":{"type":"uses","at":[175,29,175,35]}},{"key":"geid_138_542","source":"scip-python python temp indexer `examples.screen.main`/image_generation_process().","target":"scip-python python temp indexer `examples.screen.main`/inputs.","attributes":{"type":"uses","at":[176,38,176,44]}},{"key":"geid_138_543","source":"scip-python python temp indexer `examples.screen.main`/image_generation_process().","target":"scip-python python temp indexer `examples.screen.main`/inputs.","attributes":{"type":"uses","at":[176,49,176,55]}},{"key":"geid_138_544","source":"scip-python python temp indexer `examples.screen.main`/image_generation_process().","target":"scip-python python temp indexer `examples.screen.main`/inputs.","attributes":{"type":"uses","at":[178,12,178,18]}},{"key":"geid_138_545","source":"scip-python python temp indexer `examples.screen.main`/image_generation_process().","target":"scip-python python python-stdlib 3.11 typing/MutableSequence#clear().","attributes":{"type":"uses","at":[178,19,178,24]}},{"key":"geid_138_546","source":"scip-python python temp indexer `examples.screen.main`/image_generation_process().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#stream.","attributes":{"type":"uses","at":[179,35,179,41]}},{"key":"geid_138_547","source":"scip-python python temp indexer `examples.screen.main`/image_generation_process().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#device.","attributes":{"type":"uses","at":[180,45,180,51]}},{"key":"geid_138_548","source":"scip-python python temp indexer `examples.screen.main`/image_generation_process().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#dtype.","attributes":{"type":"uses","at":[180,66,180,71]}},{"key":"geid_138_549","source":"scip-python python temp indexer `examples.screen.main`/image_generation_process().","target":"scip-python python python-stdlib 3.11 time/__init__:","attributes":{"type":"uses","at":[187,23,187,27]}},{"key":"geid_138_550","source":"scip-python python temp indexer `examples.screen.main`/image_generation_process().","target":"scip-python python python-stdlib 3.11 time/time().","attributes":{"type":"uses","at":[187,28,187,32]}},{"key":"geid_138_551","source":"scip-python python temp indexer `examples.screen.main`/image_generation_process().","target":"scip-python python temp indexer `examples.screen.main`/stop_capture.","attributes":{"type":"uses","at":[190,12,190,24]}},{"key":"geid_138_552","source":"scip-python python temp indexer `examples.screen.main`/main().","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[197,15,197,23]}},{"key":"geid_138_553","source":"scip-python python temp indexer `examples.screen.main`/main().","target":"scip-python python python-stdlib 3.11 typing/Dict.","attributes":{"type":"uses","at":[197,24,197,28]}},{"key":"geid_138_554","source":"scip-python python temp indexer `examples.screen.main`/main().","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[203,18,203,25]}},{"key":"geid_138_555","source":"scip-python python temp indexer `examples.screen.main`/main().","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[206,14,206,21]}},{"key":"geid_138_556","source":"scip-python python temp indexer `examples.screen.main`/main().","target":"scip-python python temp indexer `examples.screen.main`/dummy_screen().","attributes":{"type":"uses","at":[217,14,217,26]}},{"key":"geid_138_557","source":"scip-python python temp indexer `examples.screen.main`/main().","target":"scip-python python temp indexer `examples.screen.main`/image_generation_process().","attributes":{"type":"uses","at":[222,15,222,39]}},{"key":"geid_138_558","source":"scip-python python temp indexer `examples.screen.main`/main().","target":"scip-python python temp indexer `utils.viewer`/receive_images().","attributes":{"type":"uses","at":[248,30,248,44]}},{"key":"geid_138_559","source":"scip-python python temp indexer `examples.screen.main`/__init__:","target":"scip-python python temp indexer `examples.screen.main`/","attributes":{"type":"uses","at":[252,3,252,11]}},{"key":"geid_138_560","source":"scip-python python temp indexer `examples.screen.main`/__init__:","target":"scip-python python temp indexer `examples.screen.main`/main().","attributes":{"type":"uses","at":[253,14,253,18]}},{"key":"geid_138_561","source":"scip-python python temp indexer `examples.txt2img.multi`/__init__:","target":"scip-python python temp indexer `examples.txt2img.multi`/main().","attributes":{"type":"defines","at":[13,0,76,62]}},{"key":"geid_138_562","source":"scip-python python temp indexer `examples.txt2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[0,7,0,9]}},{"key":"geid_138_563","source":"scip-python python temp indexer `examples.txt2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 sys/__init__:","attributes":{"type":"uses","at":[1,7,1,10]}},{"key":"geid_138_564","source":"scip-python python temp indexer `examples.txt2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 typing/__init__:","attributes":{"type":"uses","at":[2,5,2,11]}},{"key":"geid_138_565","source":"scip-python python temp indexer `examples.txt2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[2,19,2,26]}},{"key":"geid_138_566","source":"scip-python python temp indexer `examples.txt2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Dict.","attributes":{"type":"uses","at":[2,28,2,32]}},{"key":"geid_138_567","source":"scip-python python temp indexer `examples.txt2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[2,34,2,42]}},{"key":"geid_138_568","source":"scip-python python temp indexer `examples.txt2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 sys/__init__:","attributes":{"type":"uses","at":[6,0,6,3]}},{"key":"geid_138_569","source":"scip-python python temp indexer `examples.txt2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 sys/path.path.","attributes":{"type":"uses","at":[6,4,6,8]}},{"key":"geid_138_570","source":"scip-python python temp indexer `examples.txt2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[6,16,6,18]}},{"key":"geid_138_571","source":"scip-python python temp indexer `examples.txt2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[6,19,6,23]}},{"key":"geid_138_572","source":"scip-python python temp indexer `examples.txt2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[6,24,6,28]}},{"key":"geid_138_573","source":"scip-python python temp indexer `examples.txt2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[6,29,6,31]}},{"key":"geid_138_574","source":"scip-python python temp indexer `examples.txt2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[6,32,6,36]}},{"key":"geid_138_575","source":"scip-python python temp indexer `examples.txt2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[6,37,6,44]}},{"key":"geid_138_576","source":"scip-python python temp indexer `examples.txt2img.multi`/__init__:","target":"scip-python python temp indexer `utils.wrapper`/__init__:","attributes":{"type":"uses","at":[8,5,8,18]}},{"key":"geid_138_577","source":"scip-python python temp indexer `examples.txt2img.multi`/__init__:","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#","attributes":{"type":"uses","at":[8,26,8,48]}},{"key":"geid_138_578","source":"scip-python python temp indexer `examples.txt2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[10,14,10,16]}},{"key":"geid_138_579","source":"scip-python python temp indexer `examples.txt2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[10,17,10,21]}},{"key":"geid_138_580","source":"scip-python python temp indexer `examples.txt2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[10,22,10,29]}},{"key":"geid_138_581","source":"scip-python python temp indexer `examples.txt2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[10,30,10,32]}},{"key":"geid_138_582","source":"scip-python python temp indexer `examples.txt2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[10,33,10,37]}},{"key":"geid_138_583","source":"scip-python python temp indexer `examples.txt2img.multi`/__init__:","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[10,38,10,45]}},{"key":"geid_138_584","source":"scip-python python temp indexer `examples.txt2img.multi`/__init__:","target":"scip-python python temp indexer `examples.txt2img.multi`/","attributes":{"type":"uses","at":[10,46,10,54]}},{"key":"geid_138_585","source":"scip-python python temp indexer `examples.txt2img.multi`/main().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[14,18,14,20]}},{"key":"geid_138_586","source":"scip-python python temp indexer `examples.txt2img.multi`/main().","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[14,21,14,25]}},{"key":"geid_138_587","source":"scip-python python temp indexer `examples.txt2img.multi`/main().","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[14,26,14,30]}},{"key":"geid_138_588","source":"scip-python python temp indexer `examples.txt2img.multi`/main().","target":"scip-python python temp indexer `examples.txt2img.multi`/CURRENT_DIR.","attributes":{"type":"uses","at":[14,31,14,42]}},{"key":"geid_138_589","source":"scip-python python temp indexer `examples.txt2img.multi`/main().","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[16,15,16,23]}},{"key":"geid_138_590","source":"scip-python python temp indexer `examples.txt2img.multi`/main().","target":"scip-python python python-stdlib 3.11 typing/Dict.","attributes":{"type":"uses","at":[16,24,16,28]}},{"key":"geid_138_591","source":"scip-python python temp indexer `examples.txt2img.multi`/main().","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[21,18,21,25]}},{"key":"geid_138_592","source":"scip-python python temp indexer `examples.txt2img.multi`/main().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[52,4,52,6]}},{"key":"geid_138_593","source":"scip-python python temp indexer `examples.txt2img.multi`/main().","target":"scip-python python python-stdlib 3.11 os/makedirs().","attributes":{"type":"uses","at":[52,7,52,15]}},{"key":"geid_138_594","source":"scip-python python temp indexer `examples.txt2img.multi`/main().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#","attributes":{"type":"uses","at":[54,13,54,35]}},{"key":"geid_138_595","source":"scip-python python temp indexer `examples.txt2img.multi`/main().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#prepare().","attributes":{"type":"uses","at":[69,11,69,18]}},{"key":"geid_138_596","source":"scip-python python temp indexer `examples.txt2img.multi`/main().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[76,26,76,28]}},{"key":"geid_138_597","source":"scip-python python temp indexer `examples.txt2img.multi`/main().","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[76,29,76,33]}},{"key":"geid_138_598","source":"scip-python python temp indexer `examples.txt2img.multi`/main().","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[76,34,76,38]}},{"key":"geid_138_599","source":"scip-python python temp indexer `examples.txt2img.multi`/__init__:","target":"scip-python python temp indexer `examples.txt2img.multi`/","attributes":{"type":"uses","at":[79,3,79,11]}},{"key":"geid_138_600","source":"scip-python python temp indexer `examples.txt2img.multi`/__init__:","target":"scip-python python temp indexer `examples.txt2img.multi`/main().","attributes":{"type":"uses","at":[80,14,80,18]}},{"key":"geid_138_601","source":"scip-python python temp indexer `examples.txt2img.single`/__init__:","target":"scip-python python temp indexer `examples.txt2img.single`/main().","attributes":{"type":"defines","at":[14,0,77,29]}},{"key":"geid_138_602","source":"scip-python python temp indexer `examples.txt2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[0,7,0,9]}},{"key":"geid_138_603","source":"scip-python python temp indexer `examples.txt2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 sys/__init__:","attributes":{"type":"uses","at":[1,7,1,10]}},{"key":"geid_138_604","source":"scip-python python temp indexer `examples.txt2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 typing/__init__:","attributes":{"type":"uses","at":[2,5,2,11]}},{"key":"geid_138_605","source":"scip-python python temp indexer `examples.txt2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[2,19,2,26]}},{"key":"geid_138_606","source":"scip-python python temp indexer `examples.txt2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Dict.","attributes":{"type":"uses","at":[2,28,2,32]}},{"key":"geid_138_607","source":"scip-python python temp indexer `examples.txt2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[2,34,2,42]}},{"key":"geid_138_608","source":"scip-python python temp indexer `examples.txt2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 sys/__init__:","attributes":{"type":"uses","at":[7,0,7,3]}},{"key":"geid_138_609","source":"scip-python python temp indexer `examples.txt2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 sys/path.path.","attributes":{"type":"uses","at":[7,4,7,8]}},{"key":"geid_138_610","source":"scip-python python temp indexer `examples.txt2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[7,16,7,18]}},{"key":"geid_138_611","source":"scip-python python temp indexer `examples.txt2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[7,19,7,23]}},{"key":"geid_138_612","source":"scip-python python temp indexer `examples.txt2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[7,24,7,28]}},{"key":"geid_138_613","source":"scip-python python temp indexer `examples.txt2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[7,29,7,31]}},{"key":"geid_138_614","source":"scip-python python temp indexer `examples.txt2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[7,32,7,36]}},{"key":"geid_138_615","source":"scip-python python temp indexer `examples.txt2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[7,37,7,44]}},{"key":"geid_138_616","source":"scip-python python temp indexer `examples.txt2img.single`/__init__:","target":"scip-python python temp indexer `utils.wrapper`/__init__:","attributes":{"type":"uses","at":[9,5,9,18]}},{"key":"geid_138_617","source":"scip-python python temp indexer `examples.txt2img.single`/__init__:","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#","attributes":{"type":"uses","at":[9,26,9,48]}},{"key":"geid_138_618","source":"scip-python python temp indexer `examples.txt2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[11,14,11,16]}},{"key":"geid_138_619","source":"scip-python python temp indexer `examples.txt2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[11,17,11,21]}},{"key":"geid_138_620","source":"scip-python python temp indexer `examples.txt2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[11,22,11,29]}},{"key":"geid_138_621","source":"scip-python python temp indexer `examples.txt2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[11,30,11,32]}},{"key":"geid_138_622","source":"scip-python python temp indexer `examples.txt2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[11,33,11,37]}},{"key":"geid_138_623","source":"scip-python python temp indexer `examples.txt2img.single`/__init__:","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[11,38,11,45]}},{"key":"geid_138_624","source":"scip-python python temp indexer `examples.txt2img.single`/__init__:","target":"scip-python python temp indexer `examples.txt2img.single`/","attributes":{"type":"uses","at":[11,46,11,54]}},{"key":"geid_138_625","source":"scip-python python temp indexer `examples.txt2img.single`/main().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[15,18,15,20]}},{"key":"geid_138_626","source":"scip-python python temp indexer `examples.txt2img.single`/main().","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[15,21,15,25]}},{"key":"geid_138_627","source":"scip-python python temp indexer `examples.txt2img.single`/main().","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[15,26,15,30]}},{"key":"geid_138_628","source":"scip-python python temp indexer `examples.txt2img.single`/main().","target":"scip-python python temp indexer `examples.txt2img.single`/CURRENT_DIR.","attributes":{"type":"uses","at":[15,31,15,42]}},{"key":"geid_138_629","source":"scip-python python temp indexer `examples.txt2img.single`/main().","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[17,15,17,23]}},{"key":"geid_138_630","source":"scip-python python temp indexer `examples.txt2img.single`/main().","target":"scip-python python python-stdlib 3.11 typing/Dict.","attributes":{"type":"uses","at":[17,24,17,28]}},{"key":"geid_138_631","source":"scip-python python temp indexer `examples.txt2img.single`/main().","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[21,18,21,25]}},{"key":"geid_138_632","source":"scip-python python temp indexer `examples.txt2img.single`/main().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#","attributes":{"type":"uses","at":[53,13,53,35]}},{"key":"geid_138_633","source":"scip-python python temp indexer `examples.txt2img.single`/main().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#prepare().","attributes":{"type":"uses","at":[68,11,68,18]}},{"key":"geid_138_634","source":"scip-python python temp indexer `examples.txt2img.single`/main().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#batch_size.","attributes":{"type":"uses","at":[73,26,73,36]}},{"key":"geid_138_635","source":"scip-python python temp indexer `examples.txt2img.single`/main().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#save().","attributes":{"type":"uses","at":[77,17,77,21]}},{"key":"geid_138_636","source":"scip-python python temp indexer `examples.txt2img.single`/__init__:","target":"scip-python python temp indexer `examples.txt2img.single`/","attributes":{"type":"uses","at":[80,3,80,11]}},{"key":"geid_138_637","source":"scip-python python temp indexer `examples.txt2img.single`/__init__:","target":"scip-python python temp indexer `examples.txt2img.single`/main().","attributes":{"type":"uses","at":[81,14,81,18]}},{"key":"geid_138_638","source":"scip-python python temp indexer `examples.vid2vid.main`/__init__:","target":"scip-python python temp indexer `examples.vid2vid.main`/main().","attributes":{"type":"defines","at":[16,0,98,50]}},{"key":"geid_138_639","source":"scip-python python temp indexer `examples.vid2vid.main`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[0,7,0,9]}},{"key":"geid_138_640","source":"scip-python python temp indexer `examples.vid2vid.main`/__init__:","target":"scip-python python python-stdlib 3.11 sys/__init__:","attributes":{"type":"uses","at":[1,7,1,10]}},{"key":"geid_138_641","source":"scip-python python temp indexer `examples.vid2vid.main`/__init__:","target":"scip-python python python-stdlib 3.11 typing/__init__:","attributes":{"type":"uses","at":[2,5,2,11]}},{"key":"geid_138_642","source":"scip-python python temp indexer `examples.vid2vid.main`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[2,19,2,26]}},{"key":"geid_138_643","source":"scip-python python temp indexer `examples.vid2vid.main`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Dict.","attributes":{"type":"uses","at":[2,28,2,32]}},{"key":"geid_138_644","source":"scip-python python temp indexer `examples.vid2vid.main`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[2,34,2,42]}},{"key":"geid_138_645","source":"scip-python python temp indexer `examples.vid2vid.main`/__init__:","target":"scip-python python temp indexer `torchvision.io`/__init__:","attributes":{"type":"uses","at":[6,5,6,19]}},{"key":"geid_138_646","source":"scip-python python temp indexer `examples.vid2vid.main`/__init__:","target":"scip-python python tqdm 4.66.1 tqdm/__init__:","attributes":{"type":"uses","at":[7,5,7,9]}},{"key":"geid_138_647","source":"scip-python python temp indexer `examples.vid2vid.main`/__init__:","target":"scip-python python tqdm 4.66.1 `tqdm.std`/tqdm#","attributes":{"type":"uses","at":[7,17,7,21]}},{"key":"geid_138_648","source":"scip-python python temp indexer `examples.vid2vid.main`/__init__:","target":"scip-python python python-stdlib 3.11 sys/__init__:","attributes":{"type":"uses","at":[9,0,9,3]}},{"key":"geid_138_649","source":"scip-python python temp indexer `examples.vid2vid.main`/__init__:","target":"scip-python python python-stdlib 3.11 sys/path.path.","attributes":{"type":"uses","at":[9,4,9,8]}},{"key":"geid_138_650","source":"scip-python python temp indexer `examples.vid2vid.main`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[9,16,9,18]}},{"key":"geid_138_651","source":"scip-python python temp indexer `examples.vid2vid.main`/__init__:","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[9,19,9,23]}},{"key":"geid_138_652","source":"scip-python python temp indexer `examples.vid2vid.main`/__init__:","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[9,24,9,28]}},{"key":"geid_138_653","source":"scip-python python temp indexer `examples.vid2vid.main`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[9,29,9,31]}},{"key":"geid_138_654","source":"scip-python python temp indexer `examples.vid2vid.main`/__init__:","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[9,32,9,36]}},{"key":"geid_138_655","source":"scip-python python temp indexer `examples.vid2vid.main`/__init__:","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[9,37,9,44]}},{"key":"geid_138_656","source":"scip-python python temp indexer `examples.vid2vid.main`/__init__:","target":"scip-python python temp indexer `utils.wrapper`/__init__:","attributes":{"type":"uses","at":[11,5,11,18]}},{"key":"geid_138_657","source":"scip-python python temp indexer `examples.vid2vid.main`/__init__:","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#","attributes":{"type":"uses","at":[11,26,11,48]}},{"key":"geid_138_658","source":"scip-python python temp indexer `examples.vid2vid.main`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[13,14,13,16]}},{"key":"geid_138_659","source":"scip-python python temp indexer `examples.vid2vid.main`/__init__:","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[13,17,13,21]}},{"key":"geid_138_660","source":"scip-python python temp indexer `examples.vid2vid.main`/__init__:","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[13,22,13,29]}},{"key":"geid_138_661","source":"scip-python python temp indexer `examples.vid2vid.main`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[13,30,13,32]}},{"key":"geid_138_662","source":"scip-python python temp indexer `examples.vid2vid.main`/__init__:","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[13,33,13,37]}},{"key":"geid_138_663","source":"scip-python python temp indexer `examples.vid2vid.main`/__init__:","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[13,38,13,45]}},{"key":"geid_138_664","source":"scip-python python temp indexer `examples.vid2vid.main`/__init__:","target":"scip-python python temp indexer `examples.vid2vid.main`/","attributes":{"type":"uses","at":[13,46,13,54]}},{"key":"geid_138_665","source":"scip-python python temp indexer `examples.vid2vid.main`/main().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[18,18,18,20]}},{"key":"geid_138_666","source":"scip-python python temp indexer `examples.vid2vid.main`/main().","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[18,21,18,25]}},{"key":"geid_138_667","source":"scip-python python temp indexer `examples.vid2vid.main`/main().","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[18,26,18,30]}},{"key":"geid_138_668","source":"scip-python python temp indexer `examples.vid2vid.main`/main().","target":"scip-python python temp indexer `examples.vid2vid.main`/CURRENT_DIR.","attributes":{"type":"uses","at":[18,31,18,42]}},{"key":"geid_138_669","source":"scip-python python temp indexer `examples.vid2vid.main`/main().","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[20,15,20,23]}},{"key":"geid_138_670","source":"scip-python python temp indexer `examples.vid2vid.main`/main().","target":"scip-python python python-stdlib 3.11 typing/Dict.","attributes":{"type":"uses","at":[20,24,20,28]}},{"key":"geid_138_671","source":"scip-python python temp indexer `examples.vid2vid.main`/main().","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[23,18,23,25]}},{"key":"geid_138_672","source":"scip-python python temp indexer `examples.vid2vid.main`/main().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#","attributes":{"type":"uses","at":[65,13,65,35]}},{"key":"geid_138_673","source":"scip-python python temp indexer `examples.vid2vid.main`/main().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#prepare().","attributes":{"type":"uses","at":[83,11,83,18]}},{"key":"geid_138_674","source":"scip-python python temp indexer `examples.vid2vid.main`/main().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#batch_size.","attributes":{"type":"uses","at":[90,26,90,36]}},{"key":"geid_138_675","source":"scip-python python temp indexer `examples.vid2vid.main`/main().","target":"scip-python python tqdm 4.66.1 `tqdm.std`/tqdm#","attributes":{"type":"uses","at":[93,13,93,17]}},{"key":"geid_138_676","source":"scip-python python temp indexer `examples.vid2vid.main`/__init__:","target":"scip-python python temp indexer `examples.vid2vid.main`/","attributes":{"type":"uses","at":[101,3,101,11]}},{"key":"geid_138_677","source":"scip-python python temp indexer `examples.vid2vid.main`/__init__:","target":"scip-python python temp indexer `examples.vid2vid.main`/main().","attributes":{"type":"uses","at":[102,14,102,18]}},{"key":"geid_138_678","source":"scip-python python temp indexer `src.streamdiffusion`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/__init__:","attributes":{"type":"uses","at":[0,5,0,14]}},{"key":"geid_138_679","source":"scip-python python temp indexer `src.streamdiffusion`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#","attributes":{"type":"uses","at":[0,22,0,37]}},{"key":"geid_138_680","source":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#","target":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#set_max_skip_frame().","attributes":{"type":"defines","at":[43,4,44,44]}},{"key":"geid_138_681","source":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#","target":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#set_threshold().","attributes":{"type":"defines","at":[40,4,41,34]}},{"key":"geid_138_682","source":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#","target":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#__call__().","attributes":{"type":"defines","at":[14,4,38,31]}},{"key":"geid_138_683","source":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#","target":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#__init__().","attributes":{"type":"defines","at":[7,4,12,27]}},{"key":"geid_138_684","source":"scip-python python temp indexer `src.streamdiffusion.image_filter`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#","attributes":{"type":"defines","at":[6,0,44,44]}},{"key":"geid_138_685","source":"scip-python python temp indexer `src.streamdiffusion.image_filter`/__init__:","target":"scip-python python python-stdlib 3.11 typing/__init__:","attributes":{"type":"uses","at":[0,5,0,11]}},{"key":"geid_138_686","source":"scip-python python temp indexer `src.streamdiffusion.image_filter`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[0,19,0,27]}},{"key":"geid_138_687","source":"scip-python python temp indexer `src.streamdiffusion.image_filter`/__init__:","target":"scip-python python python-stdlib 3.11 random/__init__:","attributes":{"type":"uses","at":[1,7,1,13]}},{"key":"geid_138_688","source":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#__call__().","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[14,43,14,51]}},{"key":"geid_138_689","source":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#prev_tensor.","attributes":{"type":"uses","at":[15,16,15,27]}},{"key":"geid_138_690","source":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#prev_tensor.","attributes":{"type":"uses","at":[16,17,16,28]}},{"key":"geid_138_691","source":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#cos.","attributes":{"type":"uses","at":[19,27,19,30]}},{"key":"geid_138_692","source":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#prev_tensor.","attributes":{"type":"uses","at":[19,36,19,47]}},{"key":"geid_138_693","source":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#__call__().","target":"scip-python python python-stdlib 3.11 random/__init__:","attributes":{"type":"uses","at":[20,21,20,27]}},{"key":"geid_138_694","source":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#__call__().","target":"scip-python python python-stdlib 3.11 random/uniform.","attributes":{"type":"uses","at":[20,28,20,35]}},{"key":"geid_138_695","source":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#threshold.","attributes":{"type":"uses","at":[21,20,21,29]}},{"key":"geid_138_696","source":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#threshold.","attributes":{"type":"uses","at":[24,65,24,74]}},{"key":"geid_138_697","source":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#prev_tensor.","attributes":{"type":"uses","at":[28,21,28,32]}},{"key":"geid_138_698","source":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#skip_count.","attributes":{"type":"uses","at":[32,24,32,34]}},{"key":"geid_138_699","source":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#max_skip_frame.","attributes":{"type":"uses","at":[32,42,32,56]}},{"key":"geid_138_700","source":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#skip_count.","attributes":{"type":"uses","at":[33,25,33,35]}},{"key":"geid_138_701","source":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#prev_tensor.","attributes":{"type":"uses","at":[34,25,34,36]}},{"key":"geid_138_702","source":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#skip_count.","attributes":{"type":"uses","at":[37,25,37,35]}},{"key":"geid_138_703","source":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#set_threshold().","target":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#threshold.","attributes":{"type":"uses","at":[41,13,41,22]}},{"key":"geid_138_704","source":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#set_max_skip_frame().","target":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#max_skip_frame.","attributes":{"type":"uses","at":[44,13,44,27]}},{"key":"geid_138_705","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.image_utils`/pil2tensor().","attributes":{"type":"defines","at":[86,0,97,24]}},{"key":"geid_138_706","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.image_utils`/process_image().","attributes":{"type":"defines","at":[77,0,83,38]}},{"key":"geid_138_707","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.image_utils`/postprocess_image().","attributes":{"type":"defines","at":[41,0,74,34]}},{"key":"geid_138_708","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.image_utils`/numpy_to_pil().","attributes":{"type":"defines","at":[23,0,38,21]}},{"key":"geid_138_709","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.image_utils`/pt_to_numpy().","attributes":{"type":"defines","at":[15,0,20,17]}},{"key":"geid_138_710","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.image_utils`/denormalize().","attributes":{"type":"defines","at":[8,0,12,41]}},{"key":"geid_138_711","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/__init__:","target":"scip-python python python-stdlib 3.11 typing/__init__:","attributes":{"type":"uses","at":[0,5,0,11]}},{"key":"geid_138_712","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/__init__:","target":"scip-python python python-stdlib 3.11 typing/List.","attributes":{"type":"uses","at":[0,19,0,23]}},{"key":"geid_138_713","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[0,25,0,33]}},{"key":"geid_138_714","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Tuple.","attributes":{"type":"uses","at":[0,35,0,40]}},{"key":"geid_138_715","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Union.","attributes":{"type":"uses","at":[0,42,0,47]}},{"key":"geid_138_716","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/__init__:","target":"scip-python python numpy 1.25.2 numpy/__init__:","attributes":{"type":"uses","at":[2,7,2,12]}},{"key":"geid_138_717","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/__init__:","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[3,7,3,16]}},{"key":"geid_138_718","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/denormalize().","target":"scip-python python python-stdlib 3.11 typing/Union.","attributes":{"type":"uses","at":[8,24,8,29]}},{"key":"geid_138_719","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/denormalize().","target":"scip-python python numpy 1.25.2 numpy/ndarray#","attributes":{"type":"uses","at":[8,47,8,54]}},{"key":"geid_138_720","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/pt_to_numpy().","target":"scip-python python numpy 1.25.2 numpy/ndarray#","attributes":{"type":"uses","at":[15,44,15,51]}},{"key":"geid_138_721","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/numpy_to_pil().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[23,40,23,49]}},{"key":"geid_138_722","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/numpy_to_pil().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#","attributes":{"type":"uses","at":[23,50,23,55]}},{"key":"geid_138_723","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/numpy_to_pil().","target":"scip-python python numpy 1.25.2 numpy/ndarray#","attributes":{"type":"uses","at":[23,28,23,35]}},{"key":"geid_138_724","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/numpy_to_pil().","target":"scip-python python numpy 1.25.2 numpy/ndarray#ndim().","attributes":{"type":"uses","at":[27,14,27,18]}},{"key":"geid_138_725","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/numpy_to_pil().","target":"scip-python python numpy 1.25.2 numpy/_ArrayOrScalarCommon#round().","attributes":{"type":"uses","at":[29,28,29,33]}},{"key":"geid_138_726","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/numpy_to_pil().","target":"scip-python python numpy 1.25.2 numpy/ndarray#astype().","attributes":{"type":"uses","at":[29,36,29,42]}},{"key":"geid_138_727","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/numpy_to_pil().","target":"scip-python python numpy 1.25.2 numpy/ndarray#shape().","attributes":{"type":"uses","at":[30,14,30,19]}},{"key":"geid_138_728","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/numpy_to_pil().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[33,12,33,15]}},{"key":"geid_138_729","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/numpy_to_pil().","target":"scip-python python Pillow 10.0.0 `pil.image`/fromarray().","attributes":{"type":"uses","at":[33,22,33,31]}},{"key":"geid_138_730","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/numpy_to_pil().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[36,22,36,25]}},{"key":"geid_138_731","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/numpy_to_pil().","target":"scip-python python Pillow 10.0.0 `pil.image`/fromarray().","attributes":{"type":"uses","at":[36,32,36,41]}},{"key":"geid_138_732","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/postprocess_image().","target":"scip-python python python-stdlib 3.11 typing/Union.","attributes":{"type":"uses","at":[45,5,45,10]}},{"key":"geid_138_733","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/postprocess_image().","target":"scip-python python numpy 1.25.2 numpy/ndarray#","attributes":{"type":"uses","at":[45,28,45,35]}},{"key":"geid_138_734","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/postprocess_image().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[45,37,45,40]}},{"key":"geid_138_735","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/postprocess_image().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#","attributes":{"type":"uses","at":[45,47,45,52]}},{"key":"geid_138_736","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/postprocess_image().","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[44,20,44,28]}},{"key":"geid_138_737","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/postprocess_image().","target":"scip-python python python-stdlib 3.11 typing/List.","attributes":{"type":"uses","at":[44,29,44,33]}},{"key":"geid_138_738","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/postprocess_image().","target":"scip-python python temp indexer `src.streamdiffusion.image_utils`/denormalize().","attributes":{"type":"uses","at":[60,12,60,23]}},{"key":"geid_138_739","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/postprocess_image().","target":"scip-python python temp indexer `src.streamdiffusion.image_utils`/pt_to_numpy().","attributes":{"type":"uses","at":[68,12,68,23]}},{"key":"geid_138_740","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/postprocess_image().","target":"scip-python python temp indexer `src.streamdiffusion.image_utils`/numpy_to_pil().","attributes":{"type":"uses","at":[74,15,74,27]}},{"key":"geid_138_741","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/process_image().","target":"scip-python python python-stdlib 3.11 typing/Tuple.","attributes":{"type":"uses","at":[79,5,79,10]}},{"key":"geid_138_742","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/process_image().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[79,25,79,28]}},{"key":"geid_138_743","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/process_image().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#","attributes":{"type":"uses","at":[79,35,79,40]}},{"key":"geid_138_744","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/process_image().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[78,15,78,18]}},{"key":"geid_138_745","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/process_image().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#","attributes":{"type":"uses","at":[78,25,78,30]}},{"key":"geid_138_746","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/process_image().","target":"scip-python python python-stdlib 3.11 typing/Tuple.","attributes":{"type":"uses","at":[78,39,78,44]}},{"key":"geid_138_747","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/pil2tensor().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[86,26,86,29]}},{"key":"geid_138_748","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/pil2tensor().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#","attributes":{"type":"uses","at":[86,36,86,41]}},{"key":"geid_138_749","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/pil2tensor().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#height().","attributes":{"type":"uses","at":[87,23,87,29]}},{"key":"geid_138_750","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/pil2tensor().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#width().","attributes":{"type":"uses","at":[88,22,88,27]}},{"key":"geid_138_751","source":"scip-python python temp indexer `src.streamdiffusion.image_utils`/pil2tensor().","target":"scip-python python temp indexer `src.streamdiffusion.image_utils`/process_image().","attributes":{"type":"uses","at":[90,13,90,26]}},{"key":"geid_138_752","source":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/run_pip().","attributes":{"type":"defines","at":[50,0,51,47]}},{"key":"geid_138_753","source":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/run_python().","attributes":{"type":"defines","at":[30,0,47,30]}},{"key":"geid_138_754","source":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/is_installed().","attributes":{"type":"defines","at":[21,0,27,27]}},{"key":"geid_138_755","source":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/version().","attributes":{"type":"defines","at":[14,0,18,19]}},{"key":"geid_138_756","source":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/__init__:","target":"scip-python python python-stdlib 3.11 importlib/__init__:","attributes":{"type":"uses","at":[0,7,0,16]}},{"key":"geid_138_757","source":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/__init__:","target":"scip-python python python-stdlib 3.11 `importlib.util`/__init__:","attributes":{"type":"uses","at":[1,7,1,21]}},{"key":"geid_138_758","source":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[2,7,2,9]}},{"key":"geid_138_759","source":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/__init__:","target":"scip-python python python-stdlib 3.11 subprocess/__init__:","attributes":{"type":"uses","at":[3,7,3,17]}},{"key":"geid_138_760","source":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/__init__:","target":"scip-python python python-stdlib 3.11 sys/__init__:","attributes":{"type":"uses","at":[4,7,4,10]}},{"key":"geid_138_761","source":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/__init__:","target":"scip-python python python-stdlib 3.11 typing/__init__:","attributes":{"type":"uses","at":[5,5,5,11]}},{"key":"geid_138_762","source":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Dict.","attributes":{"type":"uses","at":[5,19,5,23]}},{"key":"geid_138_763","source":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[5,25,5,33]}},{"key":"geid_138_764","source":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/__init__:","target":"scip-python python packaging 23.1 `packaging.version`/__init__:","attributes":{"type":"uses","at":[7,5,7,22]}},{"key":"geid_138_765","source":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/__init__:","target":"scip-python python packaging 23.1 `packaging.version`/Version#","attributes":{"type":"uses","at":[7,30,7,37]}},{"key":"geid_138_766","source":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/__init__:","target":"scip-python python python-stdlib 3.11 sys/__init__:","attributes":{"type":"uses","at":[10,9,10,12]}},{"key":"geid_138_767","source":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/__init__:","target":"scip-python python python-stdlib 3.11 sys/executable.executable.","attributes":{"type":"uses","at":[10,13,10,23]}},{"key":"geid_138_768","source":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[11,12,11,14]}},{"key":"geid_138_769","source":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/__init__:","target":"scip-python python python-stdlib 3.11 os/environ.environ.","attributes":{"type":"uses","at":[11,15,11,22]}},{"key":"geid_138_770","source":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Mapping#get().","attributes":{"type":"uses","at":[11,23,11,26]}},{"key":"geid_138_771","source":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/version().","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[14,29,14,37]}},{"key":"geid_138_772","source":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/version().","target":"scip-python python packaging 23.1 `packaging.version`/Version#","attributes":{"type":"uses","at":[14,38,14,45]}},{"key":"geid_138_773","source":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/version().","target":"scip-python python packaging 23.1 `packaging.version`/Version#","attributes":{"type":"uses","at":[16,15,16,22]}},{"key":"geid_138_774","source":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/version().","target":"scip-python python python-stdlib 3.11 importlib/__init__:","attributes":{"type":"uses","at":[16,23,16,32]}},{"key":"geid_138_775","source":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/version().","target":"scip-python python python-stdlib 3.11 importlib/import_module().","attributes":{"type":"uses","at":[16,33,16,46]}},{"key":"geid_138_776","source":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/is_installed().","target":"scip-python python python-stdlib 3.11 importlib/__init__:","attributes":{"type":"uses","at":[23,15,23,24]}},{"key":"geid_138_777","source":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/run_python().","target":"scip-python python python-stdlib 3.11 typing/Dict.","attributes":{"type":"uses","at":[30,34,30,38]}},{"key":"geid_138_778","source":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/run_python().","target":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/python.","attributes":{"type":"uses","at":[32,19,32,25]}},{"key":"geid_138_779","source":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/run_python().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[34,15,34,17]}},{"key":"geid_138_780","source":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/run_python().","target":"scip-python python python-stdlib 3.11 os/environ.environ.","attributes":{"type":"uses","at":[34,18,34,25]}},{"key":"geid_138_781","source":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/run_python().","target":"scip-python python python-stdlib 3.11 subprocess/__init__:","attributes":{"type":"uses","at":[41,13,41,23]}},{"key":"geid_138_782","source":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/run_python().","target":"scip-python python python-stdlib 3.11 subprocess/run().","attributes":{"type":"uses","at":[41,24,41,27]}},{"key":"geid_138_783","source":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/run_python().","target":"scip-python python python-stdlib 3.11 sys/__init__:","attributes":{"type":"uses","at":[44,56,44,59]}},{"key":"geid_138_784","source":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/run_python().","target":"scip-python python python-stdlib 3.11 sys/stderr.stderr.","attributes":{"type":"uses","at":[44,60,44,66]}},{"key":"geid_138_785","source":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/run_pip().","target":"scip-python python python-stdlib 3.11 typing/Dict.","attributes":{"type":"uses","at":[50,31,50,35]}},{"key":"geid_138_786","source":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/run_pip().","target":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/run_python().","attributes":{"type":"uses","at":[51,11,51,21]}},{"key":"geid_138_787","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#txt2img_sd_turbo().","attributes":{"type":"defines","at":[480,4,495,46]}},{"key":"geid_138_788","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#txt2img().","attributes":{"type":"defines","at":[470,4,478,23]}},{"key":"geid_138_789","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__call__().","attributes":{"type":"defines","at":[438,4,468,23]}},{"key":"geid_138_790","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#predict_x0_batch().","attributes":{"type":"defines","at":[388,4,436,27]}},{"key":"geid_138_791","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#decode_image().","attributes":{"type":"defines","at":[382,4,386,28]}},{"key":"geid_138_792","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#encode_image().","attributes":{"type":"defines","at":[372,4,380,25]}},{"key":"geid_138_793","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","attributes":{"type":"defines","at":[297,4,370,41]}},{"key":"geid_138_794","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#scheduler_step_batch().","attributes":{"type":"defines","at":[275,4,295,29]}},{"key":"geid_138_795","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#add_noise().","attributes":{"type":"defines","at":[263,4,273,28]}},{"key":"geid_138_796","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#update_prompt().","attributes":{"type":"defines","at":[253,4,261,76]}},{"key":"geid_138_797","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","attributes":{"type":"defines","at":[122,4,251,9]}},{"key":"geid_138_798","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#disable_similar_image_filter().","attributes":{"type":"defines","at":[119,4,120,41]}},{"key":"geid_138_799","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#enable_similar_image_filter().","attributes":{"type":"defines","at":[114,4,117,62]}},{"key":"geid_138_800","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#fuse_lora().","attributes":{"type":"defines","at":[100,4,112,9]}},{"key":"geid_138_801","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#load_lora().","attributes":{"type":"defines","at":[90,4,98,9]}},{"key":"geid_138_802","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#load_lcm_lora().","attributes":{"type":"defines","at":[78,4,88,9]}},{"key":"geid_138_803","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__init__().","attributes":{"type":"defines","at":[16,4,76,35]}},{"key":"geid_138_804","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#","attributes":{"type":"defines","at":[15,0,495,46]}},{"key":"geid_138_805","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/__init__:","target":"scip-python python python-stdlib 3.11 time/__init__:","attributes":{"type":"uses","at":[0,7,0,11]}},{"key":"geid_138_806","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/__init__:","target":"scip-python python python-stdlib 3.11 typing/__init__:","attributes":{"type":"uses","at":[1,5,1,11]}},{"key":"geid_138_807","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/__init__:","target":"scip-python python python-stdlib 3.11 typing/List.","attributes":{"type":"uses","at":[1,19,1,23]}},{"key":"geid_138_808","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[1,25,1,33]}},{"key":"geid_138_809","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Union.","attributes":{"type":"uses","at":[1,35,1,40]}},{"key":"geid_138_810","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Any.","attributes":{"type":"uses","at":[1,42,1,45]}},{"key":"geid_138_811","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Dict.","attributes":{"type":"uses","at":[1,47,1,51]}},{"key":"geid_138_812","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Tuple.","attributes":{"type":"uses","at":[1,53,1,58]}},{"key":"geid_138_813","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[1,60,1,67]}},{"key":"geid_138_814","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/__init__:","target":"scip-python python numpy 1.25.2 numpy/__init__:","attributes":{"type":"uses","at":[3,7,3,12]}},{"key":"geid_138_815","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/__init__:","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[4,7,4,16]}},{"key":"geid_138_816","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/__init__:","target":"scip-python python temp indexer diffusers/__init__:","attributes":{"type":"uses","at":[6,5,6,14]}},{"key":"geid_138_817","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/__init__:","target":"scip-python python temp indexer `diffusers.image_processor`/__init__:","attributes":{"type":"uses","at":[7,5,7,30]}},{"key":"geid_138_818","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/__init__:","target":"scip-python python temp indexer `diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img`/__init__:","attributes":{"type":"uses","at":[8,5,8,75]}},{"key":"geid_138_819","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.image_filter`/__init__:","attributes":{"type":"uses","at":[12,5,12,33]}},{"key":"geid_138_820","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#","attributes":{"type":"uses","at":[12,41,12,59]}},{"key":"geid_138_821","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__init__().","target":"scip-python python python-stdlib 3.11 typing/List.","attributes":{"type":"uses","at":[19,22,19,26]}},{"key":"geid_138_822","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__init__().","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[26,18,26,25]}},{"key":"geid_138_823","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#denoising_steps_num.","attributes":{"type":"uses","at":[44,35,44,54]}},{"key":"geid_138_824","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#cfg_type.","attributes":{"type":"uses","at":[45,20,45,28]}},{"key":"geid_138_825","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#denoising_steps_num.","attributes":{"type":"uses","at":[47,25,47,44]}},{"key":"geid_138_826","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#frame_bff_size.","attributes":{"type":"uses","at":[48,25,48,39]}},{"key":"geid_138_827","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#cfg_type.","attributes":{"type":"uses","at":[49,22,49,30]}},{"key":"geid_138_828","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#trt_unet_batch_size.","attributes":{"type":"uses","at":[50,21,50,40]}},{"key":"geid_138_829","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#denoising_steps_num.","attributes":{"type":"uses","at":[51,29,51,48]}},{"key":"geid_138_830","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#frame_bff_size.","attributes":{"type":"uses","at":[51,56,51,70]}},{"key":"geid_138_831","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#trt_unet_batch_size.","attributes":{"type":"uses","at":[54,21,54,40]}},{"key":"geid_138_832","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#denoising_steps_num.","attributes":{"type":"uses","at":[54,48,54,67]}},{"key":"geid_138_833","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#trt_unet_batch_size.","attributes":{"type":"uses","at":[56,17,56,36]}},{"key":"geid_138_834","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#frame_bff_size.","attributes":{"type":"uses","at":[56,44,56,58]}},{"key":"geid_138_835","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#batch_size.","attributes":{"type":"uses","at":[57,17,57,27]}},{"key":"geid_138_836","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#","attributes":{"type":"uses","at":[65,30,65,48]}},{"key":"geid_138_837","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#pipe.","attributes":{"type":"uses","at":[71,55,71,59]}},{"key":"geid_138_838","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#load_lcm_lora().","target":"scip-python python python-stdlib 3.11 typing/Union.","attributes":{"type":"uses","at":[80,47,80,52]}},{"key":"geid_138_839","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#load_lcm_lora().","target":"scip-python python python-stdlib 3.11 typing/Dict.","attributes":{"type":"uses","at":[81,17,81,21]}},{"key":"geid_138_840","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#load_lcm_lora().","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[83,22,83,30]}},{"key":"geid_138_841","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#load_lcm_lora().","target":"scip-python python python-stdlib 3.11 typing/Any.","attributes":{"type":"uses","at":[83,31,83,34]}},{"key":"geid_138_842","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#load_lcm_lora().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#pipe.","attributes":{"type":"uses","at":[86,13,86,17]}},{"key":"geid_138_843","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#load_lora().","target":"scip-python python python-stdlib 3.11 typing/Union.","attributes":{"type":"uses","at":[92,52,92,57]}},{"key":"geid_138_844","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#load_lora().","target":"scip-python python python-stdlib 3.11 typing/Dict.","attributes":{"type":"uses","at":[92,63,92,67]}},{"key":"geid_138_845","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#load_lora().","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[93,22,93,30]}},{"key":"geid_138_846","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#load_lora().","target":"scip-python python python-stdlib 3.11 typing/Any.","attributes":{"type":"uses","at":[93,31,93,34]}},{"key":"geid_138_847","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#load_lora().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#pipe.","attributes":{"type":"uses","at":[96,13,96,17]}},{"key":"geid_138_848","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#fuse_lora().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#pipe.","attributes":{"type":"uses","at":[107,13,107,17]}},{"key":"geid_138_849","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#enable_similar_image_filter().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#similar_image_filter.","attributes":{"type":"uses","at":[115,13,115,33]}},{"key":"geid_138_850","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#enable_similar_image_filter().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#similar_filter.","attributes":{"type":"uses","at":[116,13,116,27]}},{"key":"geid_138_851","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#enable_similar_image_filter().","target":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#set_threshold().","attributes":{"type":"uses","at":[116,28,116,41]}},{"key":"geid_138_852","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#enable_similar_image_filter().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#similar_filter.","attributes":{"type":"uses","at":[117,13,117,27]}},{"key":"geid_138_853","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#enable_similar_image_filter().","target":"scip-python python temp indexer `src.streamdiffusion.image_filter`/SimilarImageFilter#set_max_skip_frame().","attributes":{"type":"uses","at":[117,28,117,46]}},{"key":"geid_138_854","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#disable_similar_image_filter().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#similar_image_filter.","attributes":{"type":"uses","at":[120,13,120,33]}},{"key":"geid_138_855","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[130,19,130,27]}},{"key":"geid_138_856","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#generator.","attributes":{"type":"uses","at":[133,13,133,22]}},{"key":"geid_138_857","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#generator.","attributes":{"type":"uses","at":[134,13,134,22]}},{"key":"geid_138_858","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#denoising_steps_num.","attributes":{"type":"uses","at":[136,16,136,35]}},{"key":"geid_138_859","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#denoising_steps_num.","attributes":{"type":"uses","at":[139,26,139,45]}},{"key":"geid_138_860","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#frame_bff_size.","attributes":{"type":"uses","at":[139,58,139,72]}},{"key":"geid_138_861","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#latent_height.","attributes":{"type":"uses","at":[141,25,141,38]}},{"key":"geid_138_862","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#latent_width.","attributes":{"type":"uses","at":[142,25,142,37]}},{"key":"geid_138_863","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#dtype.","attributes":{"type":"uses","at":[144,27,144,32]}},{"key":"geid_138_864","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#device.","attributes":{"type":"uses","at":[145,28,145,34]}},{"key":"geid_138_865","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#x_t_latent_buffer.","attributes":{"type":"uses","at":[148,17,148,34]}},{"key":"geid_138_866","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#cfg_type.","attributes":{"type":"uses","at":[150,16,150,24]}},{"key":"geid_138_867","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#guidance_scale.","attributes":{"type":"uses","at":[153,17,153,31]}},{"key":"geid_138_868","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#guidance_scale.","attributes":{"type":"uses","at":[157,16,157,30]}},{"key":"geid_138_869","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#pipe.","attributes":{"type":"uses","at":[160,30,160,34]}},{"key":"geid_138_870","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#device.","attributes":{"type":"uses","at":[162,24,162,30]}},{"key":"geid_138_871","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#batch_size.","attributes":{"type":"uses","at":[167,59,167,69]}},{"key":"geid_138_872","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#use_denoising_batch.","attributes":{"type":"uses","at":[169,16,169,35]}},{"key":"geid_138_873","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#cfg_type.","attributes":{"type":"uses","at":[169,45,169,53]}},{"key":"geid_138_874","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#batch_size.","attributes":{"type":"uses","at":[170,65,170,75]}},{"key":"geid_138_875","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#cfg_type.","attributes":{"type":"uses","at":[171,18,171,26]}},{"key":"geid_138_876","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#frame_bff_size.","attributes":{"type":"uses","at":[172,65,172,79]}},{"key":"geid_138_877","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#guidance_scale.","attributes":{"type":"uses","at":[174,16,174,30]}},{"key":"geid_138_878","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#cfg_type.","attributes":{"type":"uses","at":[175,17,175,25]}},{"key":"geid_138_879","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#cfg_type.","attributes":{"type":"uses","at":[175,50,175,58]}},{"key":"geid_138_880","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prompt_embeds.","attributes":{"type":"uses","at":[177,17,177,30]}},{"key":"geid_138_881","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prompt_embeds.","attributes":{"type":"uses","at":[178,44,178,57]}},{"key":"geid_138_882","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#scheduler.","attributes":{"type":"uses","at":[181,13,181,22]}},{"key":"geid_138_883","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#device.","attributes":{"type":"uses","at":[181,63,181,69]}},{"key":"geid_138_884","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#scheduler.","attributes":{"type":"uses","at":[182,30,182,39]}},{"key":"geid_138_885","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#device.","attributes":{"type":"uses","at":[182,58,182,64]}},{"key":"geid_138_886","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#t_list.","attributes":{"type":"uses","at":[186,22,186,28]}},{"key":"geid_138_887","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#sub_timesteps.","attributes":{"type":"uses","at":[187,17,187,30]}},{"key":"geid_138_888","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#timesteps.","attributes":{"type":"uses","at":[187,43,187,52]}},{"key":"geid_138_889","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#sub_timesteps.","attributes":{"type":"uses","at":[190,17,190,30]}},{"key":"geid_138_890","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#device.","attributes":{"type":"uses","at":[190,62,190,68]}},{"key":"geid_138_891","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#frame_bff_size.","attributes":{"type":"uses","at":[194,25,194,39]}},{"key":"geid_138_892","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#use_denoising_batch.","attributes":{"type":"uses","at":[194,48,194,67]}},{"key":"geid_138_893","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#batch_size.","attributes":{"type":"uses","at":[199,18,199,28]}},{"key":"geid_138_894","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#latent_height.","attributes":{"type":"uses","at":[199,38,199,51]}},{"key":"geid_138_895","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#latent_width.","attributes":{"type":"uses","at":[199,58,199,70]}},{"key":"geid_138_896","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#device.","attributes":{"type":"uses","at":[201,25,201,31]}},{"key":"geid_138_897","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#dtype.","attributes":{"type":"uses","at":[201,44,201,49]}},{"key":"geid_138_898","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#init_noise.","attributes":{"type":"uses","at":[203,49,203,59]}},{"key":"geid_138_899","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#sub_timesteps.","attributes":{"type":"uses","at":[207,29,207,42]}},{"key":"geid_138_900","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#scheduler.","attributes":{"type":"uses","at":[208,33,208,42]}},{"key":"geid_138_901","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#t_list.","attributes":{"type":"uses","at":[216,27,216,33]}},{"key":"geid_138_902","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#dtype.","attributes":{"type":"uses","at":[217,27,217,32]}},{"key":"geid_138_903","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#device.","attributes":{"type":"uses","at":[217,46,217,52]}},{"key":"geid_138_904","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#t_list.","attributes":{"type":"uses","at":[221,27,221,33]}},{"key":"geid_138_905","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#dtype.","attributes":{"type":"uses","at":[222,27,222,32]}},{"key":"geid_138_906","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#device.","attributes":{"type":"uses","at":[222,46,222,52]}},{"key":"geid_138_907","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#sub_timesteps.","attributes":{"type":"uses","at":[227,29,227,42]}},{"key":"geid_138_908","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#scheduler.","attributes":{"type":"uses","at":[228,37,228,46]}},{"key":"geid_138_909","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#scheduler.","attributes":{"type":"uses","at":[229,41,229,50]}},{"key":"geid_138_910","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#t_list.","attributes":{"type":"uses","at":[234,27,234,33]}},{"key":"geid_138_911","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#dtype.","attributes":{"type":"uses","at":[235,27,235,32]}},{"key":"geid_138_912","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#device.","attributes":{"type":"uses","at":[235,46,235,52]}},{"key":"geid_138_913","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#t_list.","attributes":{"type":"uses","at":[239,27,239,33]}},{"key":"geid_138_914","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#dtype.","attributes":{"type":"uses","at":[240,27,240,32]}},{"key":"geid_138_915","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#device.","attributes":{"type":"uses","at":[240,46,240,52]}},{"key":"geid_138_916","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#frame_bff_size.","attributes":{"type":"uses","at":[244,25,244,39]}},{"key":"geid_138_917","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#use_denoising_batch.","attributes":{"type":"uses","at":[244,48,244,67]}},{"key":"geid_138_918","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#frame_bff_size.","attributes":{"type":"uses","at":[249,25,249,39]}},{"key":"geid_138_919","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prepare().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#use_denoising_batch.","attributes":{"type":"uses","at":[249,48,249,67]}},{"key":"geid_138_920","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#update_prompt().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#pipe.","attributes":{"type":"uses","at":[255,30,255,34]}},{"key":"geid_138_921","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#update_prompt().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#device.","attributes":{"type":"uses","at":[257,24,257,30]}},{"key":"geid_138_922","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#update_prompt().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prompt_embeds.","attributes":{"type":"uses","at":[261,13,261,26]}},{"key":"geid_138_923","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#update_prompt().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#batch_size.","attributes":{"type":"uses","at":[261,59,261,69]}},{"key":"geid_138_924","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#add_noise().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#alpha_prod_t_sqrt.","attributes":{"type":"uses","at":[270,17,270,34]}},{"key":"geid_138_925","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#add_noise().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#beta_prod_t_sqrt.","attributes":{"type":"uses","at":[271,19,271,35]}},{"key":"geid_138_926","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#scheduler_step_batch().","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[279,13,279,21]}},{"key":"geid_138_927","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#scheduler_step_batch().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#beta_prod_t_sqrt.","attributes":{"type":"uses","at":[284,40,284,56]}},{"key":"geid_138_928","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#scheduler_step_batch().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#alpha_prod_t_sqrt.","attributes":{"type":"uses","at":[285,21,285,38]}},{"key":"geid_138_929","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#scheduler_step_batch().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#c_out.","attributes":{"type":"uses","at":[286,34,286,39]}},{"key":"geid_138_930","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#scheduler_step_batch().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#c_skip.","attributes":{"type":"uses","at":[286,57,286,63]}},{"key":"geid_138_931","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#scheduler_step_batch().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#beta_prod_t_sqrt.","attributes":{"type":"uses","at":[289,40,289,56]}},{"key":"geid_138_932","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#scheduler_step_batch().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#alpha_prod_t_sqrt.","attributes":{"type":"uses","at":[290,21,290,38]}},{"key":"geid_138_933","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#scheduler_step_batch().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#c_out.","attributes":{"type":"uses","at":[292,21,292,26]}},{"key":"geid_138_934","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#scheduler_step_batch().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#c_skip.","attributes":{"type":"uses","at":[292,49,292,55]}},{"key":"geid_138_935","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python python-stdlib 3.11 typing/Tuple.","attributes":{"type":"uses","at":[302,9,302,14]}},{"key":"geid_138_936","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python python-stdlib 3.11 typing/Union.","attributes":{"type":"uses","at":[300,16,300,21]}},{"key":"geid_138_937","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[301,13,301,21]}},{"key":"geid_138_938","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#guidance_scale.","attributes":{"type":"uses","at":[303,16,303,30]}},{"key":"geid_138_939","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#cfg_type.","attributes":{"type":"uses","at":[303,47,303,55]}},{"key":"geid_138_940","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#guidance_scale.","attributes":{"type":"uses","at":[306,18,306,32]}},{"key":"geid_138_941","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#cfg_type.","attributes":{"type":"uses","at":[306,49,306,57]}},{"key":"geid_138_942","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet.","attributes":{"type":"uses","at":[312,26,312,30]}},{"key":"geid_138_943","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prompt_embeds.","attributes":{"type":"uses","at":[315,39,315,52]}},{"key":"geid_138_944","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#guidance_scale.","attributes":{"type":"uses","at":[319,16,319,30]}},{"key":"geid_138_945","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#cfg_type.","attributes":{"type":"uses","at":[319,47,319,55]}},{"key":"geid_138_946","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#stock_noise.","attributes":{"type":"uses","at":[321,17,321,28]}},{"key":"geid_138_947","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#stock_noise.","attributes":{"type":"uses","at":[322,39,322,50]}},{"key":"geid_138_948","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#guidance_scale.","attributes":{"type":"uses","at":[324,18,324,32]}},{"key":"geid_138_949","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#cfg_type.","attributes":{"type":"uses","at":[324,49,324,57]}},{"key":"geid_138_950","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#guidance_scale.","attributes":{"type":"uses","at":[328,16,328,30]}},{"key":"geid_138_951","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#cfg_type.","attributes":{"type":"uses","at":[329,17,329,25]}},{"key":"geid_138_952","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#cfg_type.","attributes":{"type":"uses","at":[329,44,329,52]}},{"key":"geid_138_953","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#stock_noise.","attributes":{"type":"uses","at":[331,37,331,48]}},{"key":"geid_138_954","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#delta.","attributes":{"type":"uses","at":[331,56,331,61]}},{"key":"geid_138_955","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#guidance_scale.","attributes":{"type":"uses","at":[332,16,332,30]}},{"key":"geid_138_956","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#cfg_type.","attributes":{"type":"uses","at":[332,46,332,54]}},{"key":"geid_138_957","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#guidance_scale.","attributes":{"type":"uses","at":[333,50,333,64]}},{"key":"geid_138_958","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#use_denoising_batch.","attributes":{"type":"uses","at":[340,16,340,35]}},{"key":"geid_138_959","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#scheduler_step_batch().","attributes":{"type":"uses","at":[341,34,341,54]}},{"key":"geid_138_960","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#cfg_type.","attributes":{"type":"uses","at":[342,20,342,28]}},{"key":"geid_138_961","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#cfg_type.","attributes":{"type":"uses","at":[342,47,342,55]}},{"key":"geid_138_962","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#beta_prod_t_sqrt.","attributes":{"type":"uses","at":[343,36,343,52]}},{"key":"geid_138_963","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#stock_noise.","attributes":{"type":"uses","at":[343,60,343,71]}},{"key":"geid_138_964","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#scheduler_step_batch().","attributes":{"type":"uses","at":[344,31,344,51]}},{"key":"geid_138_965","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#alpha_prod_t_sqrt.","attributes":{"type":"uses","at":[347,29,347,46]}},{"key":"geid_138_966","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#alpha_prod_t_sqrt.","attributes":{"type":"uses","at":[348,45,348,62]}},{"key":"geid_138_967","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#beta_prod_t_sqrt.","attributes":{"type":"uses","at":[355,29,355,45]}},{"key":"geid_138_968","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#beta_prod_t_sqrt.","attributes":{"type":"uses","at":[356,45,356,61]}},{"key":"geid_138_969","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#init_noise.","attributes":{"type":"uses","at":[362,26,362,36]}},{"key":"geid_138_970","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#init_noise.","attributes":{"type":"uses","at":[362,47,362,57]}},{"key":"geid_138_971","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#stock_noise.","attributes":{"type":"uses","at":[364,21,364,32]}},{"key":"geid_138_972","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#scheduler_step_batch().","attributes":{"type":"uses","at":[368,34,368,54]}},{"key":"geid_138_973","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#encode_image().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#device.","attributes":{"type":"uses","at":[374,24,374,30]}},{"key":"geid_138_974","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#encode_image().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#vae.","attributes":{"type":"uses","at":[375,23,375,26]}},{"key":"geid_138_975","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#encode_image().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#vae.","attributes":{"type":"uses","at":[377,43,377,46]}},{"key":"geid_138_976","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#encode_image().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#generator.","attributes":{"type":"uses","at":[377,75,377,84]}},{"key":"geid_138_977","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#encode_image().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#vae.","attributes":{"type":"uses","at":[378,39,378,42]}},{"key":"geid_138_978","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#encode_image().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#add_noise().","attributes":{"type":"uses","at":[379,26,379,35]}},{"key":"geid_138_979","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#encode_image().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#init_noise.","attributes":{"type":"uses","at":[379,53,379,63]}},{"key":"geid_138_980","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#decode_image().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#vae.","attributes":{"type":"uses","at":[383,29,383,32]}},{"key":"geid_138_981","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#decode_image().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#vae.","attributes":{"type":"uses","at":[384,32,384,35]}},{"key":"geid_138_982","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#predict_x0_batch().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#x_t_latent_buffer.","attributes":{"type":"uses","at":[389,33,389,50]}},{"key":"geid_138_983","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#predict_x0_batch().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#use_denoising_batch.","attributes":{"type":"uses","at":[391,16,391,35]}},{"key":"geid_138_984","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#predict_x0_batch().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#sub_timesteps_tensor.","attributes":{"type":"uses","at":[392,26,392,46]}},{"key":"geid_138_985","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#predict_x0_batch().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#denoising_steps_num.","attributes":{"type":"uses","at":[393,20,393,39]}},{"key":"geid_138_986","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#predict_x0_batch().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#stock_noise.","attributes":{"type":"uses","at":[395,21,395,32]}},{"key":"geid_138_987","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#predict_x0_batch().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#init_noise.","attributes":{"type":"uses","at":[396,26,396,36]}},{"key":"geid_138_988","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#predict_x0_batch().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#stock_noise.","attributes":{"type":"uses","at":[396,48,396,59]}},{"key":"geid_138_989","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#predict_x0_batch().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","attributes":{"type":"uses","at":[398,46,398,55]}},{"key":"geid_138_990","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#predict_x0_batch().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#denoising_steps_num.","attributes":{"type":"uses","at":[400,20,400,39]}},{"key":"geid_138_991","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#predict_x0_batch().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#do_add_noise.","attributes":{"type":"uses","at":[402,24,402,36]}},{"key":"geid_138_992","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#predict_x0_batch().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#x_t_latent_buffer.","attributes":{"type":"uses","at":[403,25,403,42]}},{"key":"geid_138_993","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#predict_x0_batch().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#alpha_prod_t_sqrt.","attributes":{"type":"uses","at":[404,29,404,46]}},{"key":"geid_138_994","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#predict_x0_batch().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#beta_prod_t_sqrt.","attributes":{"type":"uses","at":[405,31,405,47]}},{"key":"geid_138_995","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#predict_x0_batch().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#init_noise.","attributes":{"type":"uses","at":[405,59,405,69]}},{"key":"geid_138_996","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#predict_x0_batch().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#x_t_latent_buffer.","attributes":{"type":"uses","at":[408,25,408,42]}},{"key":"geid_138_997","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#predict_x0_batch().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#alpha_prod_t_sqrt.","attributes":{"type":"uses","at":[409,29,409,46]}},{"key":"geid_138_998","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#predict_x0_batch().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#x_t_latent_buffer.","attributes":{"type":"uses","at":[413,21,413,38]}},{"key":"geid_138_999","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#predict_x0_batch().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#init_noise.","attributes":{"type":"uses","at":[415,17,415,27]}},{"key":"geid_138_1000","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#predict_x0_batch().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#sub_timesteps_tensor.","attributes":{"type":"uses","at":[416,41,416,61]}},{"key":"geid_138_1001","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#predict_x0_batch().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#frame_bff_size.","attributes":{"type":"uses","at":[420,25,420,39]}},{"key":"geid_138_1002","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#predict_x0_batch().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet_step().","attributes":{"type":"uses","at":[422,44,422,53]}},{"key":"geid_138_1003","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#predict_x0_batch().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#sub_timesteps_tensor.","attributes":{"type":"uses","at":[423,34,423,54]}},{"key":"geid_138_1004","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#predict_x0_batch().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#do_add_noise.","attributes":{"type":"uses","at":[424,28,424,40]}},{"key":"geid_138_1005","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#predict_x0_batch().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#alpha_prod_t_sqrt.","attributes":{"type":"uses","at":[425,42,425,59]}},{"key":"geid_138_1006","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#predict_x0_batch().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#beta_prod_t_sqrt.","attributes":{"type":"uses","at":[427,44,427,60]}},{"key":"geid_138_1007","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#predict_x0_batch().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#device.","attributes":{"type":"uses","at":[430,50,430,56]}},{"key":"geid_138_1008","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#predict_x0_batch().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#dtype.","attributes":{"type":"uses","at":[430,69,430,74]}},{"key":"geid_138_1009","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#predict_x0_batch().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#alpha_prod_t_sqrt.","attributes":{"type":"uses","at":[433,42,433,59]}},{"key":"geid_138_1010","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__call__().","target":"scip-python python python-stdlib 3.11 typing/Union.","attributes":{"type":"uses","at":[440,17,440,22]}},{"key":"geid_138_1011","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__call__().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[440,37,440,46]}},{"key":"geid_138_1012","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__call__().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#","attributes":{"type":"uses","at":[440,47,440,52]}},{"key":"geid_138_1013","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__call__().","target":"scip-python python numpy 1.25.2 numpy/ndarray#","attributes":{"type":"uses","at":[440,57,440,64]}},{"key":"geid_138_1014","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#image_processor.","attributes":{"type":"uses","at":[446,21,446,36]}},{"key":"geid_138_1015","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#height.","attributes":{"type":"uses","at":[446,56,446,62]}},{"key":"geid_138_1016","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#width.","attributes":{"type":"uses","at":[446,69,446,74]}},{"key":"geid_138_1017","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#device.","attributes":{"type":"uses","at":[447,28,447,34]}},{"key":"geid_138_1018","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#dtype.","attributes":{"type":"uses","at":[447,47,447,52]}},{"key":"geid_138_1019","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#similar_image_filter.","attributes":{"type":"uses","at":[449,20,449,40]}},{"key":"geid_138_1020","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#similar_filter.","attributes":{"type":"uses","at":[450,25,450,39]}},{"key":"geid_138_1021","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__call__().","target":"scip-python python python-stdlib 3.11 time/__init__:","attributes":{"type":"uses","at":[452,20,452,24]}},{"key":"geid_138_1022","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__call__().","target":"scip-python python python-stdlib 3.11 time/sleep().","attributes":{"type":"uses","at":[452,25,452,30]}},{"key":"geid_138_1023","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#inference_time_ema.","attributes":{"type":"uses","at":[452,36,452,54]}},{"key":"geid_138_1024","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prev_image_result.","attributes":{"type":"uses","at":[453,32,453,49]}},{"key":"geid_138_1025","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#encode_image().","attributes":{"type":"uses","at":[454,30,454,42]}},{"key":"geid_138_1026","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#latent_height.","attributes":{"type":"uses","at":[457,49,457,62]}},{"key":"geid_138_1027","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#latent_width.","attributes":{"type":"uses","at":[457,69,457,81]}},{"key":"geid_138_1028","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#device.","attributes":{"type":"uses","at":[458,28,458,34]}},{"key":"geid_138_1029","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#dtype.","attributes":{"type":"uses","at":[458,47,458,52]}},{"key":"geid_138_1030","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#predict_x0_batch().","attributes":{"type":"uses","at":[460,28,460,44]}},{"key":"geid_138_1031","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#decode_image().","attributes":{"type":"uses","at":[461,24,461,36]}},{"key":"geid_138_1032","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prev_image_result.","attributes":{"type":"uses","at":[463,13,463,30]}},{"key":"geid_138_1033","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#inference_time_ema.","attributes":{"type":"uses","at":[467,13,467,31]}},{"key":"geid_138_1034","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#inference_time_ema.","attributes":{"type":"uses","at":[467,45,467,63]}},{"key":"geid_138_1035","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#txt2img().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#predict_x0_batch().","attributes":{"type":"uses","at":[472,28,472,44]}},{"key":"geid_138_1036","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#txt2img().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#latent_height.","attributes":{"type":"uses","at":[473,45,473,58]}},{"key":"geid_138_1037","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#txt2img().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#latent_width.","attributes":{"type":"uses","at":[473,65,473,77]}},{"key":"geid_138_1038","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#txt2img().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#device.","attributes":{"type":"uses","at":[474,28,474,34]}},{"key":"geid_138_1039","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#txt2img().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#dtype.","attributes":{"type":"uses","at":[474,47,474,52]}},{"key":"geid_138_1040","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#txt2img().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#decode_image().","attributes":{"type":"uses","at":[477,24,477,36]}},{"key":"geid_138_1041","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#txt2img_sd_turbo().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#latent_height.","attributes":{"type":"uses","at":[482,33,482,46]}},{"key":"geid_138_1042","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#txt2img_sd_turbo().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#latent_width.","attributes":{"type":"uses","at":[482,53,482,65]}},{"key":"geid_138_1043","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#txt2img_sd_turbo().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#device.","attributes":{"type":"uses","at":[483,24,483,30]}},{"key":"geid_138_1044","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#txt2img_sd_turbo().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#dtype.","attributes":{"type":"uses","at":[484,23,484,28]}},{"key":"geid_138_1045","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#txt2img_sd_turbo().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet.","attributes":{"type":"uses","at":[486,26,486,30]}},{"key":"geid_138_1046","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#txt2img_sd_turbo().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#sub_timesteps_tensor.","attributes":{"type":"uses","at":[488,17,488,37]}},{"key":"geid_138_1047","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#txt2img_sd_turbo().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#prompt_embeds.","attributes":{"type":"uses","at":[489,39,489,52]}},{"key":"geid_138_1048","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#txt2img_sd_turbo().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#beta_prod_t_sqrt.","attributes":{"type":"uses","at":[493,30,493,46]}},{"key":"geid_138_1049","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#txt2img_sd_turbo().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#alpha_prod_t_sqrt.","attributes":{"type":"uses","at":[494,17,494,34]}},{"key":"geid_138_1050","source":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#txt2img_sd_turbo().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#decode_image().","attributes":{"type":"uses","at":[495,20,495,32]}},{"key":"geid_138_1051","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.sfast`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.sfast`/accelerate_with_stable_fast().","attributes":{"type":"defines","at":[7,0,32,17]}},{"key":"geid_138_1052","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.sfast`/__init__:","target":"scip-python python python-stdlib 3.11 typing/__init__:","attributes":{"type":"uses","at":[0,5,0,11]}},{"key":"geid_138_1053","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.sfast`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[0,19,0,27]}},{"key":"geid_138_1054","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.sfast`/__init__:","target":"scip-python python temp indexer `sfast.compilers.stable_diffusion_pipeline_compiler`/__init__:","attributes":{"type":"uses","at":[2,5,2,55]}},{"key":"geid_138_1055","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.sfast`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/__init__:","attributes":{"type":"uses","at":[4,5,4,16]}},{"key":"geid_138_1056","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.sfast`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#","attributes":{"type":"uses","at":[4,24,4,39]}},{"key":"geid_138_1057","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.sfast`/accelerate_with_stable_fast().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#","attributes":{"type":"uses","at":[8,12,8,27]}},{"key":"geid_138_1058","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.sfast`/accelerate_with_stable_fast().","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[9,12,9,20]}},{"key":"geid_138_1059","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.sfast`/accelerate_with_stable_fast().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#pipe.","attributes":{"type":"uses","at":[28,11,28,15]}},{"key":"geid_138_1060","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.sfast`/accelerate_with_stable_fast().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#pipe.","attributes":{"type":"uses","at":[28,33,28,37]}},{"key":"geid_138_1061","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.sfast`/accelerate_with_stable_fast().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet.","attributes":{"type":"uses","at":[29,11,29,15]}},{"key":"geid_138_1062","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.sfast`/accelerate_with_stable_fast().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#pipe.","attributes":{"type":"uses","at":[29,25,29,29]}},{"key":"geid_138_1063","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.sfast`/accelerate_with_stable_fast().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#vae.","attributes":{"type":"uses","at":[30,11,30,14]}},{"key":"geid_138_1064","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.sfast`/accelerate_with_stable_fast().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#pipe.","attributes":{"type":"uses","at":[30,24,30,28]}},{"key":"geid_138_1065","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.sfast`/accelerate_with_stable_fast().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#text_encoder.","attributes":{"type":"uses","at":[31,11,31,23]}},{"key":"geid_138_1066","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.sfast`/accelerate_with_stable_fast().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#pipe.","attributes":{"type":"uses","at":[31,33,31,37]}},{"key":"geid_138_1067","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","attributes":{"type":"defines","at":[84,0,186,17]}},{"key":"geid_138_1068","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/compile_unet().","attributes":{"type":"defines","at":[64,0,81,5]}},{"key":"geid_138_1069","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/compile_vae_decoder().","attributes":{"type":"defines","at":[44,0,61,5]}},{"key":"geid_138_1070","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/compile_vae_encoder().","attributes":{"type":"defines","at":[25,0,41,5]}},{"key":"geid_138_1071","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/TorchVAEEncoder#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/TorchVAEEncoder#forward().","attributes":{"type":"defines","at":[21,4,22,51]}},{"key":"geid_138_1072","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/TorchVAEEncoder#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/TorchVAEEncoder#__init__().","attributes":{"type":"defines","at":[17,4,19,22]}},{"key":"geid_138_1073","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/TorchVAEEncoder#","attributes":{"type":"defines","at":[16,0,22,51]}},{"key":"geid_138_1074","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/__init__:","target":"scip-python python python-stdlib 3.11 gc/__init__:","attributes":{"type":"uses","at":[0,7,0,9]}},{"key":"geid_138_1075","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[1,7,1,9]}},{"key":"geid_138_1076","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/__init__:","target":"scip-python python temp indexer diffusers/__init__:","attributes":{"type":"uses","at":[4,5,4,14]}},{"key":"geid_138_1077","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/__init__:","target":"scip-python python temp indexer `diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img`/__init__:","attributes":{"type":"uses","at":[5,5,5,75]}},{"key":"geid_138_1078","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/__init__:","target":"scip-python python temp indexer polygraphy/__init__:","attributes":{"type":"uses","at":[8,5,8,15]}},{"key":"geid_138_1079","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/__init__:","attributes":{"type":"uses","at":[10,5,10,16]}},{"key":"geid_138_1080","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#","attributes":{"type":"uses","at":[10,24,10,39]}},{"key":"geid_138_1081","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/__init__:","attributes":{"type":"uses","at":[11,5,11,13]}},{"key":"geid_138_1082","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#","attributes":{"type":"uses","at":[11,21,11,34]}},{"key":"geid_138_1083","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/create_onnx_path().","attributes":{"type":"uses","at":[11,36,11,52]}},{"key":"geid_138_1084","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/__init__:","attributes":{"type":"uses","at":[12,5,12,12]}},{"key":"geid_138_1085","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#","attributes":{"type":"uses","at":[12,20,12,39]}},{"key":"geid_138_1086","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#","attributes":{"type":"uses","at":[12,41,12,67]}},{"key":"geid_138_1087","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/__init__:","attributes":{"type":"uses","at":[13,5,13,12]}},{"key":"geid_138_1088","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#","attributes":{"type":"uses","at":[13,20,13,23]}},{"key":"geid_138_1089","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#","attributes":{"type":"uses","at":[13,25,13,34]}},{"key":"geid_138_1090","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#","attributes":{"type":"uses","at":[13,36,13,40]}},{"key":"geid_138_1091","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#","attributes":{"type":"uses","at":[13,42,13,52]}},{"key":"geid_138_1092","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/TorchVAEEncoder#forward().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/TorchVAEEncoder#vae.","attributes":{"type":"uses","at":[22,37,22,40]}},{"key":"geid_138_1093","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/compile_vae_encoder().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/TorchVAEEncoder#","attributes":{"type":"uses","at":[26,9,26,24]}},{"key":"geid_138_1094","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/compile_vae_encoder().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#","attributes":{"type":"uses","at":[27,16,27,25]}},{"key":"geid_138_1095","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/compile_vae_encoder().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#","attributes":{"type":"uses","at":[34,14,34,27]}},{"key":"geid_138_1096","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/compile_vae_encoder().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#build().","attributes":{"type":"uses","at":[35,12,35,17]}},{"key":"geid_138_1097","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/compile_vae_decoder().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#","attributes":{"type":"uses","at":[46,16,46,25]}},{"key":"geid_138_1098","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/compile_vae_decoder().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#","attributes":{"type":"uses","at":[54,14,54,27]}},{"key":"geid_138_1099","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/compile_vae_decoder().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#build().","attributes":{"type":"uses","at":[55,12,55,17]}},{"key":"geid_138_1100","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/compile_unet().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#","attributes":{"type":"uses","at":[66,16,66,25]}},{"key":"geid_138_1101","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/compile_unet().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#","attributes":{"type":"uses","at":[74,14,74,27]}},{"key":"geid_138_1102","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/compile_unet().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#build().","attributes":{"type":"uses","at":[75,12,75,17]}},{"key":"geid_138_1103","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#","attributes":{"type":"uses","at":[85,12,85,27]}},{"key":"geid_138_1104","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#text_encoder.","attributes":{"type":"uses","at":[94,26,94,38]}},{"key":"geid_138_1105","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet.","attributes":{"type":"uses","at":[95,18,95,22]}},{"key":"geid_138_1106","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#vae.","attributes":{"type":"uses","at":[96,17,96,20]}},{"key":"geid_138_1107","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet.","attributes":{"type":"uses","at":[98,15,98,19]}},{"key":"geid_138_1108","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#vae.","attributes":{"type":"uses","at":[98,28,98,31]}},{"key":"geid_138_1109","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#pipe.","attributes":{"type":"uses","at":[98,40,98,44]}},{"key":"geid_138_1110","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#pipe.","attributes":{"type":"uses","at":[98,58,98,62]}},{"key":"geid_138_1111","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python python-stdlib 3.11 gc/__init__:","attributes":{"type":"uses","at":[106,4,106,6]}},{"key":"geid_138_1112","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python python-stdlib 3.11 gc/collect().","attributes":{"type":"uses","at":[106,7,106,14]}},{"key":"geid_138_1113","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[109,15,109,17]}},{"key":"geid_138_1114","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[109,18,109,22]}},{"key":"geid_138_1115","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[109,23,109,27]}},{"key":"geid_138_1116","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[110,4,110,6]}},{"key":"geid_138_1117","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python python-stdlib 3.11 os/makedirs().","attributes":{"type":"uses","at":[110,7,110,15]}},{"key":"geid_138_1118","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#","attributes":{"type":"uses","at":[116,17,116,21]}},{"key":"geid_138_1119","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#device.","attributes":{"type":"uses","at":[118,22,118,28]}},{"key":"geid_138_1120","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#","attributes":{"type":"uses","at":[124,24,124,27]}},{"key":"geid_138_1121","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#device.","attributes":{"type":"uses","at":[125,22,125,28]}},{"key":"geid_138_1122","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#","attributes":{"type":"uses","at":[129,24,129,34]}},{"key":"geid_138_1123","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#device.","attributes":{"type":"uses","at":[130,22,130,28]}},{"key":"geid_138_1124","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[135,11,135,13]}},{"key":"geid_138_1125","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[135,14,135,18]}},{"key":"geid_138_1126","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[135,19,135,25]}},{"key":"geid_138_1127","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/compile_unet().","attributes":{"type":"uses","at":[136,8,136,20]}},{"key":"geid_138_1128","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/create_onnx_path().","attributes":{"type":"uses","at":[139,12,139,28]}},{"key":"geid_138_1129","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/create_onnx_path().","attributes":{"type":"uses","at":[140,12,140,28]}},{"key":"geid_138_1130","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[147,11,147,13]}},{"key":"geid_138_1131","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[147,14,147,18]}},{"key":"geid_138_1132","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[147,19,147,25]}},{"key":"geid_138_1133","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/compile_vae_decoder().","attributes":{"type":"uses","at":[148,8,148,27]}},{"key":"geid_138_1134","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/create_onnx_path().","attributes":{"type":"uses","at":[151,12,151,28]}},{"key":"geid_138_1135","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/create_onnx_path().","attributes":{"type":"uses","at":[152,12,152,28]}},{"key":"geid_138_1136","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[157,11,157,13]}},{"key":"geid_138_1137","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[157,14,157,18]}},{"key":"geid_138_1138","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[157,19,157,25]}},{"key":"geid_138_1139","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/TorchVAEEncoder#","attributes":{"type":"uses","at":[158,22,158,37]}},{"key":"geid_138_1140","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/compile_vae_encoder().","attributes":{"type":"uses","at":[159,8,159,27]}},{"key":"geid_138_1141","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/create_onnx_path().","attributes":{"type":"uses","at":[162,12,162,28]}},{"key":"geid_138_1142","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/create_onnx_path().","attributes":{"type":"uses","at":[163,12,163,28]}},{"key":"geid_138_1143","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#unet.","attributes":{"type":"uses","at":[172,11,172,15]}},{"key":"geid_138_1144","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#","attributes":{"type":"uses","at":[172,18,172,44]}},{"key":"geid_138_1145","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#vae.","attributes":{"type":"uses","at":[173,11,173,14]}},{"key":"geid_138_1146","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#","attributes":{"type":"uses","at":[173,17,173,36]}},{"key":"geid_138_1147","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#pipe.","attributes":{"type":"uses","at":[177,15,177,19]}},{"key":"geid_138_1148","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#vae.","attributes":{"type":"uses","at":[180,19,180,22]}},{"key":"geid_138_1149","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python temp indexer `src.streamdiffusion.pipeline`/StreamDiffusion#vae.","attributes":{"type":"uses","at":[181,19,181,22]}},{"key":"geid_138_1150","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python python-stdlib 3.11 gc/__init__:","attributes":{"type":"uses","at":[183,4,183,6]}},{"key":"geid_138_1151","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/accelerate_with_tensorrt().","target":"scip-python python python-stdlib 3.11 gc/collect().","attributes":{"type":"uses","at":[183,7,183,14]}},{"key":"geid_138_1152","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#build().","attributes":{"type":"defines","at":[30,4,93,32]}},{"key":"geid_138_1153","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#__init__().","attributes":{"type":"defines","at":[19,4,28,30]}},{"key":"geid_138_1154","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#","attributes":{"type":"defines","at":[18,0,93,32]}},{"key":"geid_138_1155","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/create_onnx_path().","attributes":{"type":"defines","at":[14,0,15,75]}},{"key":"geid_138_1156","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/__init__:","target":"scip-python python python-stdlib 3.11 gc/__init__:","attributes":{"type":"uses","at":[0,7,0,9]}},{"key":"geid_138_1157","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[1,7,1,9]}},{"key":"geid_138_1158","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/__init__:","target":"scip-python python python-stdlib 3.11 typing/__init__:","attributes":{"type":"uses","at":[2,5,2,11]}},{"key":"geid_138_1159","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/__init__:","attributes":{"type":"uses","at":[6,5,6,12]}},{"key":"geid_138_1160","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#","attributes":{"type":"uses","at":[6,20,6,29]}},{"key":"geid_138_1161","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","attributes":{"type":"uses","at":[7,5,7,15]}},{"key":"geid_138_1162","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/build_engine().","attributes":{"type":"uses","at":[8,4,8,16]}},{"key":"geid_138_1163","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/export_onnx().","attributes":{"type":"uses","at":[9,4,9,15]}},{"key":"geid_138_1164","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/optimize_onnx().","attributes":{"type":"uses","at":[10,4,10,17]}},{"key":"geid_138_1165","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/create_onnx_path().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[15,11,15,13]}},{"key":"geid_138_1166","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/create_onnx_path().","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[15,14,15,18]}},{"key":"geid_138_1167","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/create_onnx_path().","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[15,19,15,23]}},{"key":"geid_138_1168","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#","attributes":{"type":"uses","at":[21,15,21,24]}},{"key":"geid_138_1169","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#__init__().","target":"scip-python python python-stdlib 3.11 typing/__init__:","attributes":{"type":"uses","at":[22,17,22,20]}},{"key":"geid_138_1170","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#build().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[49,37,49,39]}},{"key":"geid_138_1171","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#build().","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[49,40,49,44]}},{"key":"geid_138_1172","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#build().","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[49,45,49,51]}},{"key":"geid_138_1173","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#build().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/export_onnx().","attributes":{"type":"uses","at":[53,12,53,23]}},{"key":"geid_138_1174","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#build().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#network.","attributes":{"type":"uses","at":[54,21,54,28]}},{"key":"geid_138_1175","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#build().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#model.","attributes":{"type":"uses","at":[56,32,56,37]}},{"key":"geid_138_1176","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#build().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#network.","attributes":{"type":"uses","at":[62,21,62,28]}},{"key":"geid_138_1177","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#build().","target":"scip-python python python-stdlib 3.11 gc/__init__:","attributes":{"type":"uses","at":[63,12,63,14]}},{"key":"geid_138_1178","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#build().","target":"scip-python python python-stdlib 3.11 gc/collect().","attributes":{"type":"uses","at":[63,15,63,22]}},{"key":"geid_138_1179","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#build().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[65,39,65,41]}},{"key":"geid_138_1180","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#build().","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[65,42,65,46]}},{"key":"geid_138_1181","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#build().","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[65,47,65,53]}},{"key":"geid_138_1182","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#build().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/optimize_onnx().","attributes":{"type":"uses","at":[69,12,69,25]}},{"key":"geid_138_1183","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#build().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#model.","attributes":{"type":"uses","at":[72,32,72,37]}},{"key":"geid_138_1184","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#build().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#model.","attributes":{"type":"uses","at":[74,13,74,18]}},{"key":"geid_138_1185","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#build().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#min_latent_shape.","attributes":{"type":"uses","at":[74,19,74,35]}},{"key":"geid_138_1186","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#build().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#model.","attributes":{"type":"uses","at":[75,13,75,18]}},{"key":"geid_138_1187","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#build().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#max_latent_shape.","attributes":{"type":"uses","at":[75,19,75,35]}},{"key":"geid_138_1188","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#build().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[76,38,76,40]}},{"key":"geid_138_1189","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#build().","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[76,41,76,45]}},{"key":"geid_138_1190","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#build().","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[76,46,76,52]}},{"key":"geid_138_1191","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#build().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/build_engine().","attributes":{"type":"uses","at":[79,12,79,24]}},{"key":"geid_138_1192","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#build().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#model.","attributes":{"type":"uses","at":[82,32,82,37]}},{"key":"geid_138_1193","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#build().","target":"scip-python python python-stdlib 3.11 gc/__init__:","attributes":{"type":"uses","at":[92,8,92,10]}},{"key":"geid_138_1194","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.builder`/EngineBuilder#build().","target":"scip-python python python-stdlib 3.11 gc/collect().","attributes":{"type":"uses","at":[92,11,92,18]}},{"key":"geid_138_1195","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#forward().","attributes":{"type":"defines","at":[121,4,122,12]}},{"key":"geid_138_1196","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#to().","attributes":{"type":"defines","at":[118,4,119,12]}},{"key":"geid_138_1197","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#decode().","attributes":{"type":"defines","at":[98,4,116,43]}},{"key":"geid_138_1198","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#encode().","attributes":{"type":"defines","at":[78,4,96,53]}},{"key":"geid_138_1199","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#__init__().","attributes":{"type":"defines","at":[59,4,76,31]}},{"key":"geid_138_1200","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#","attributes":{"type":"defines","at":[58,0,122,12]}},{"key":"geid_138_1201","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#forward().","attributes":{"type":"defines","at":[54,4,55,12]}},{"key":"geid_138_1202","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#to().","attributes":{"type":"defines","at":[51,4,52,12]}},{"key":"geid_138_1203","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#__call__().","attributes":{"type":"defines","at":[20,4,49,55]}},{"key":"geid_138_1204","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#__init__().","attributes":{"type":"defines","at":[12,4,18,30]}},{"key":"geid_138_1205","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#","attributes":{"type":"defines","at":[11,0,55,12]}},{"key":"geid_138_1206","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/__init__:","target":"scip-python python python-stdlib 3.11 typing/__init__:","attributes":{"type":"uses","at":[0,5,0,11]}},{"key":"geid_138_1207","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/__init__:","target":"scip-python python temp indexer `diffusers.models.autoencoder_tiny`/__init__:","attributes":{"type":"uses","at":[3,5,3,38]}},{"key":"geid_138_1208","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/__init__:","target":"scip-python python temp indexer `diffusers.models.unet_2d_condition`/__init__:","attributes":{"type":"uses","at":[4,5,4,39]}},{"key":"geid_138_1209","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/__init__:","target":"scip-python python temp indexer `diffusers.models.vae`/__init__:","attributes":{"type":"uses","at":[5,5,5,25]}},{"key":"geid_138_1210","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/__init__:","target":"scip-python python temp indexer polygraphy/__init__:","attributes":{"type":"uses","at":[6,5,6,15]}},{"key":"geid_138_1211","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","attributes":{"type":"uses","at":[8,5,8,15]}},{"key":"geid_138_1212","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#","attributes":{"type":"uses","at":[8,23,8,29]}},{"key":"geid_138_1213","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#","attributes":{"type":"uses","at":[13,22,13,28]}},{"key":"geid_138_1214","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#engine.","attributes":{"type":"uses","at":[17,13,17,19]}},{"key":"geid_138_1215","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#load().","attributes":{"type":"uses","at":[17,20,17,24]}},{"key":"geid_138_1216","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#engine.","attributes":{"type":"uses","at":[18,13,18,19]}},{"key":"geid_138_1217","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#activate().","attributes":{"type":"uses","at":[18,20,18,28]}},{"key":"geid_138_1218","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#__call__().","target":"scip-python python python-stdlib 3.11 typing/__init__:","attributes":{"type":"uses","at":[26,9,26,12]}},{"key":"geid_138_1219","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#engine.","attributes":{"type":"uses","at":[30,13,30,19]}},{"key":"geid_138_1220","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#allocate_buffers().","attributes":{"type":"uses","at":[30,20,30,36]}},{"key":"geid_138_1221","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#engine.","attributes":{"type":"uses","at":[40,26,40,32]}},{"key":"geid_138_1222","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#infer().","attributes":{"type":"uses","at":[40,33,40,38]}},{"key":"geid_138_1223","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#stream.","attributes":{"type":"uses","at":[46,17,46,23]}},{"key":"geid_138_1224","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#__call__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/UNet2DConditionModelEngine#use_cuda_graph.","attributes":{"type":"uses","at":[47,32,47,46]}},{"key":"geid_138_1225","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#","attributes":{"type":"uses","at":[67,23,67,29]}},{"key":"geid_138_1226","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#","attributes":{"type":"uses","at":[68,23,68,29]}},{"key":"geid_138_1227","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#encoder.","attributes":{"type":"uses","at":[73,13,73,20]}},{"key":"geid_138_1228","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#load().","attributes":{"type":"uses","at":[73,21,73,25]}},{"key":"geid_138_1229","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#decoder.","attributes":{"type":"uses","at":[74,13,74,20]}},{"key":"geid_138_1230","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#load().","attributes":{"type":"uses","at":[74,21,74,25]}},{"key":"geid_138_1231","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#encoder.","attributes":{"type":"uses","at":[75,13,75,20]}},{"key":"geid_138_1232","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#activate().","attributes":{"type":"uses","at":[75,21,75,29]}},{"key":"geid_138_1233","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#decoder.","attributes":{"type":"uses","at":[76,13,76,20]}},{"key":"geid_138_1234","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#activate().","attributes":{"type":"uses","at":[76,21,76,29]}},{"key":"geid_138_1235","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#encode().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#encoder.","attributes":{"type":"uses","at":[79,13,79,20]}},{"key":"geid_138_1236","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#encode().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#allocate_buffers().","attributes":{"type":"uses","at":[79,21,79,37]}},{"key":"geid_138_1237","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#encode().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#vae_scale_factor.","attributes":{"type":"uses","at":[85,44,85,60]}},{"key":"geid_138_1238","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#encode().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#vae_scale_factor.","attributes":{"type":"uses","at":[86,44,86,60]}},{"key":"geid_138_1239","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#encode().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#encoder.","attributes":{"type":"uses","at":[91,23,91,30]}},{"key":"geid_138_1240","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#encode().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#infer().","attributes":{"type":"uses","at":[91,31,91,36]}},{"key":"geid_138_1241","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#encode().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#stream.","attributes":{"type":"uses","at":[93,17,93,23]}},{"key":"geid_138_1242","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#encode().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#use_cuda_graph.","attributes":{"type":"uses","at":[94,32,94,46]}},{"key":"geid_138_1243","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#decode().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#decoder.","attributes":{"type":"uses","at":[99,13,99,20]}},{"key":"geid_138_1244","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#decode().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#allocate_buffers().","attributes":{"type":"uses","at":[99,21,99,37]}},{"key":"geid_138_1245","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#decode().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#vae_scale_factor.","attributes":{"type":"uses","at":[105,43,105,59]}},{"key":"geid_138_1246","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#decode().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#vae_scale_factor.","attributes":{"type":"uses","at":[106,43,106,59]}},{"key":"geid_138_1247","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#decode().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#decoder.","attributes":{"type":"uses","at":[111,22,111,29]}},{"key":"geid_138_1248","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#decode().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#infer().","attributes":{"type":"uses","at":[111,30,111,35]}},{"key":"geid_138_1249","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#decode().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#stream.","attributes":{"type":"uses","at":[113,17,113,23]}},{"key":"geid_138_1250","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#decode().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.engine`/AutoencoderKLEngine#use_cuda_graph.","attributes":{"type":"uses","at":[114,32,114,46]}},{"key":"geid_138_1251","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#get_sample_input().","attributes":{"type":"defines","at":[424,4,433,9]}},{"key":"geid_138_1252","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#get_shape_dict().","attributes":{"type":"defines","at":[417,4,422,9]}},{"key":"geid_138_1253","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#get_input_profile().","attributes":{"type":"defines","at":[391,4,415,9]}},{"key":"geid_138_1254","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#get_dynamic_axes().","attributes":{"type":"defines","at":[385,4,389,9]}},{"key":"geid_138_1255","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#get_output_names().","attributes":{"type":"defines","at":[382,4,383,25]}},{"key":"geid_138_1256","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#get_input_names().","attributes":{"type":"defines","at":[379,4,380,25]}},{"key":"geid_138_1257","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#__init__().","attributes":{"type":"defines","at":[370,4,377,33]}},{"key":"geid_138_1258","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#","attributes":{"type":"defines","at":[369,0,433,9]}},{"key":"geid_138_1259","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#get_sample_input().","attributes":{"type":"defines","at":[357,4,366,9]}},{"key":"geid_138_1260","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#get_shape_dict().","attributes":{"type":"defines","at":[350,4,355,9]}},{"key":"geid_138_1261","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#get_input_profile().","attributes":{"type":"defines","at":[328,4,348,9]}},{"key":"geid_138_1262","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#get_dynamic_axes().","attributes":{"type":"defines","at":[322,4,326,9]}},{"key":"geid_138_1263","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#get_output_names().","attributes":{"type":"defines","at":[319,4,320,25]}},{"key":"geid_138_1264","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#get_input_names().","attributes":{"type":"defines","at":[316,4,317,25]}},{"key":"geid_138_1265","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#__init__().","attributes":{"type":"defines","at":[307,4,314,33]}},{"key":"geid_138_1266","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#","attributes":{"type":"defines","at":[306,0,366,9]}},{"key":"geid_138_1267","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_sample_input().","attributes":{"type":"defines","at":[294,4,303,9]}},{"key":"geid_138_1268","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_shape_dict().","attributes":{"type":"defines","at":[285,4,292,9]}},{"key":"geid_138_1269","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_input_profile().","attributes":{"type":"defines","at":[257,4,283,9]}},{"key":"geid_138_1270","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_dynamic_axes().","attributes":{"type":"defines","at":[249,4,255,9]}},{"key":"geid_138_1271","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_output_names().","attributes":{"type":"defines","at":[246,4,247,25]}},{"key":"geid_138_1272","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_input_names().","attributes":{"type":"defines","at":[243,4,244,62]}},{"key":"geid_138_1273","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#__init__().","attributes":{"type":"defines","at":[222,4,241,26]}},{"key":"geid_138_1274","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#","attributes":{"type":"defines","at":[221,0,303,9]}},{"key":"geid_138_1275","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#optimize().","attributes":{"type":"defines","at":[204,4,218,29]}},{"key":"geid_138_1276","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#get_sample_input().","attributes":{"type":"defines","at":[200,4,202,95]}},{"key":"geid_138_1277","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#get_shape_dict().","attributes":{"type":"defines","at":[193,4,198,9]}},{"key":"geid_138_1278","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#get_input_profile().","attributes":{"type":"defines","at":[180,4,191,9]}},{"key":"geid_138_1279","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#get_dynamic_axes().","attributes":{"type":"defines","at":[177,4,178,67]}},{"key":"geid_138_1280","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#get_output_names().","attributes":{"type":"defines","at":[174,4,175,51]}},{"key":"geid_138_1281","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#get_input_names().","attributes":{"type":"defines","at":[171,4,172,28]}},{"key":"geid_138_1282","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#__init__().","attributes":{"type":"defines","at":[162,4,169,26]}},{"key":"geid_138_1283","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#","attributes":{"type":"defines","at":[161,0,218,29]}},{"key":"geid_138_1284","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_minmax_dims().","attributes":{"type":"defines","at":[134,4,158,9]}},{"key":"geid_138_1285","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#check_dims().","attributes":{"type":"defines","at":[125,4,132,44]}},{"key":"geid_138_1286","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#optimize().","attributes":{"type":"defines","at":[112,4,123,29]}},{"key":"geid_138_1287","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_shape_dict().","attributes":{"type":"defines","at":[109,4,110,19]}},{"key":"geid_138_1288","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_input_profile().","attributes":{"type":"defines","at":[106,4,107,19]}},{"key":"geid_138_1289","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_sample_input().","attributes":{"type":"defines","at":[103,4,104,12]}},{"key":"geid_138_1290","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_dynamic_axes().","attributes":{"type":"defines","at":[100,4,101,19]}},{"key":"geid_138_1291","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_output_names().","attributes":{"type":"defines","at":[97,4,98,12]}},{"key":"geid_138_1292","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_input_names().","attributes":{"type":"defines","at":[94,4,95,12]}},{"key":"geid_138_1293","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_model().","attributes":{"type":"defines","at":[91,4,92,12]}},{"key":"geid_138_1294","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#__init__().","attributes":{"type":"defines","at":[66,4,89,38]}},{"key":"geid_138_1295","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#","attributes":{"type":"defines","at":[65,0,158,9]}},{"key":"geid_138_1296","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#infer_shapes().","attributes":{"type":"defines","at":[53,4,62,29]}},{"key":"geid_138_1297","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#fold_constants().","attributes":{"type":"defines","at":[47,4,51,29]}},{"key":"geid_138_1298","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#select_outputs().","attributes":{"type":"defines","at":[41,4,45,49]}},{"key":"geid_138_1299","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#cleanup().","attributes":{"type":"defines","at":[36,4,39,45]}},{"key":"geid_138_1300","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#info().","attributes":{"type":"defines","at":[30,4,34,13]}},{"key":"geid_138_1301","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#__init__().","attributes":{"type":"defines","at":[26,4,28,30]}},{"key":"geid_138_1302","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#","attributes":{"type":"defines","at":[25,0,62,29]}},{"key":"geid_138_1303","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/__init__:","target":"scip-python python temp indexer onnx/__init__:","attributes":{"type":"uses","at":[21,5,21,9]}},{"key":"geid_138_1304","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/__init__:","target":"scip-python python temp indexer `polygraphy.backend.onnx.loader`/__init__:","attributes":{"type":"uses","at":[22,5,22,35]}},{"key":"geid_138_1305","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#info().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#verbose.","attributes":{"type":"uses","at":[31,16,31,23]}},{"key":"geid_138_1306","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#info().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#graph.","attributes":{"type":"uses","at":[33,40,33,45]}},{"key":"geid_138_1307","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#info().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#graph.","attributes":{"type":"uses","at":[33,71,33,76]}},{"key":"geid_138_1308","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#info().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#graph.","attributes":{"type":"uses","at":[33,115,33,120]}},{"key":"geid_138_1309","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#info().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#graph.","attributes":{"type":"uses","at":[33,148,33,153]}},{"key":"geid_138_1310","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#cleanup().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#graph.","attributes":{"type":"uses","at":[37,13,37,18]}},{"key":"geid_138_1311","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#cleanup().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#graph.","attributes":{"type":"uses","at":[39,39,39,44]}},{"key":"geid_138_1312","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#select_outputs().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#graph.","attributes":{"type":"uses","at":[42,13,42,18]}},{"key":"geid_138_1313","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#select_outputs().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#graph.","attributes":{"type":"uses","at":[42,35,42,40]}},{"key":"geid_138_1314","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#select_outputs().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#graph.","attributes":{"type":"uses","at":[45,21,45,26]}},{"key":"geid_138_1315","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#fold_constants().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#graph.","attributes":{"type":"uses","at":[48,56,48,61]}},{"key":"geid_138_1316","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#fold_constants().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#graph.","attributes":{"type":"uses","at":[49,13,49,18]}},{"key":"geid_138_1317","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#infer_shapes().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#graph.","attributes":{"type":"uses","at":[54,41,54,46]}},{"key":"geid_138_1318","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#infer_shapes().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#graph.","attributes":{"type":"uses","at":[60,13,60,18]}},{"key":"geid_138_1319","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#min_image_shape.","attributes":{"type":"uses","at":[85,37,85,52]}},{"key":"geid_138_1320","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#max_image_shape.","attributes":{"type":"uses","at":[86,37,86,52]}},{"key":"geid_138_1321","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#optimize().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#","attributes":{"type":"uses","at":[113,14,113,23]}},{"key":"geid_138_1322","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#optimize().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#verbose.","attributes":{"type":"uses","at":[113,49,113,56]}},{"key":"geid_138_1323","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#optimize().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#info().","attributes":{"type":"uses","at":[114,12,114,16]}},{"key":"geid_138_1324","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#optimize().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#name.","attributes":{"type":"uses","at":[114,22,114,26]}},{"key":"geid_138_1325","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#optimize().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#cleanup().","attributes":{"type":"uses","at":[115,12,115,19]}},{"key":"geid_138_1326","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#optimize().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#info().","attributes":{"type":"uses","at":[116,12,116,16]}},{"key":"geid_138_1327","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#optimize().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#name.","attributes":{"type":"uses","at":[116,22,116,26]}},{"key":"geid_138_1328","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#optimize().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#fold_constants().","attributes":{"type":"uses","at":[117,12,117,26]}},{"key":"geid_138_1329","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#optimize().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#info().","attributes":{"type":"uses","at":[118,12,118,16]}},{"key":"geid_138_1330","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#optimize().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#name.","attributes":{"type":"uses","at":[118,22,118,26]}},{"key":"geid_138_1331","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#optimize().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#infer_shapes().","attributes":{"type":"uses","at":[119,12,119,24]}},{"key":"geid_138_1332","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#optimize().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#info().","attributes":{"type":"uses","at":[120,12,120,16]}},{"key":"geid_138_1333","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#optimize().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#name.","attributes":{"type":"uses","at":[120,22,120,26]}},{"key":"geid_138_1334","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#optimize().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#cleanup().","attributes":{"type":"uses","at":[121,29,121,36]}},{"key":"geid_138_1335","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#optimize().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#info().","attributes":{"type":"uses","at":[122,12,122,16]}},{"key":"geid_138_1336","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#optimize().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#name.","attributes":{"type":"uses","at":[122,22,122,26]}},{"key":"geid_138_1337","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#check_dims().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#min_batch.","attributes":{"type":"uses","at":[126,34,126,43]}},{"key":"geid_138_1338","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#check_dims().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#max_batch.","attributes":{"type":"uses","at":[126,67,126,76]}},{"key":"geid_138_1339","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#check_dims().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#min_latent_shape.","attributes":{"type":"uses","at":[130,37,130,53]}},{"key":"geid_138_1340","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#check_dims().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#max_latent_shape.","attributes":{"type":"uses","at":[130,80,130,96]}},{"key":"geid_138_1341","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#check_dims().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#min_latent_shape.","attributes":{"type":"uses","at":[131,36,131,52]}},{"key":"geid_138_1342","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#check_dims().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#max_latent_shape.","attributes":{"type":"uses","at":[131,78,131,94]}},{"key":"geid_138_1343","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_minmax_dims().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#min_batch.","attributes":{"type":"uses","at":[135,57,135,66]}},{"key":"geid_138_1344","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_minmax_dims().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#max_batch.","attributes":{"type":"uses","at":[136,57,136,66]}},{"key":"geid_138_1345","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_minmax_dims().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#min_image_shape.","attributes":{"type":"uses","at":[139,66,139,81]}},{"key":"geid_138_1346","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_minmax_dims().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#max_image_shape.","attributes":{"type":"uses","at":[140,66,140,81]}},{"key":"geid_138_1347","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_minmax_dims().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#min_image_shape.","attributes":{"type":"uses","at":[141,64,141,79]}},{"key":"geid_138_1348","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_minmax_dims().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#max_image_shape.","attributes":{"type":"uses","at":[142,64,142,79]}},{"key":"geid_138_1349","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_minmax_dims().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#min_latent_shape.","attributes":{"type":"uses","at":[143,68,143,84]}},{"key":"geid_138_1350","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_minmax_dims().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#max_latent_shape.","attributes":{"type":"uses","at":[144,68,144,84]}},{"key":"geid_138_1351","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_minmax_dims().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#min_latent_shape.","attributes":{"type":"uses","at":[145,66,145,82]}},{"key":"geid_138_1352","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_minmax_dims().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#max_latent_shape.","attributes":{"type":"uses","at":[146,66,146,82]}},{"key":"geid_138_1353","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#","attributes":{"type":"uses","at":[161,11,161,20]}},{"key":"geid_138_1354","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#","attributes":{"type":"uses","at":[163,14,163,18]}},{"key":"geid_138_1355","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#__init__().","attributes":{"type":"uses","at":[163,26,163,34]}},{"key":"geid_138_1356","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#get_input_profile().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#check_dims().","attributes":{"type":"uses","at":[181,13,181,23]}},{"key":"geid_138_1357","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#get_input_profile().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_minmax_dims().","attributes":{"type":"uses","at":[182,60,182,75]}},{"key":"geid_138_1358","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#get_input_profile().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#text_maxlen.","attributes":{"type":"uses","at":[187,33,187,44]}},{"key":"geid_138_1359","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#get_input_profile().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#text_maxlen.","attributes":{"type":"uses","at":[188,34,188,45]}},{"key":"geid_138_1360","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#get_input_profile().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#text_maxlen.","attributes":{"type":"uses","at":[189,33,189,44]}},{"key":"geid_138_1361","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#get_shape_dict().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#check_dims().","attributes":{"type":"uses","at":[194,13,194,23]}},{"key":"geid_138_1362","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#get_shape_dict().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#text_maxlen.","attributes":{"type":"uses","at":[196,43,196,54]}},{"key":"geid_138_1363","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#get_shape_dict().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#text_maxlen.","attributes":{"type":"uses","at":[197,49,197,60]}},{"key":"geid_138_1364","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#get_shape_dict().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#embedding_dim.","attributes":{"type":"uses","at":[197,67,197,80]}},{"key":"geid_138_1365","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#get_sample_input().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#check_dims().","attributes":{"type":"uses","at":[201,13,201,23]}},{"key":"geid_138_1366","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#get_sample_input().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#text_maxlen.","attributes":{"type":"uses","at":[202,44,202,55]}},{"key":"geid_138_1367","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#get_sample_input().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#device.","attributes":{"type":"uses","at":[202,88,202,94]}},{"key":"geid_138_1368","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#optimize().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#","attributes":{"type":"uses","at":[205,14,205,23]}},{"key":"geid_138_1369","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#optimize().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#info().","attributes":{"type":"uses","at":[206,12,206,16]}},{"key":"geid_138_1370","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#optimize().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#name.","attributes":{"type":"uses","at":[206,22,206,26]}},{"key":"geid_138_1371","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#optimize().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#select_outputs().","attributes":{"type":"uses","at":[207,12,207,26]}},{"key":"geid_138_1372","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#optimize().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#cleanup().","attributes":{"type":"uses","at":[208,12,208,19]}},{"key":"geid_138_1373","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#optimize().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#info().","attributes":{"type":"uses","at":[209,12,209,16]}},{"key":"geid_138_1374","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#optimize().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#name.","attributes":{"type":"uses","at":[209,22,209,26]}},{"key":"geid_138_1375","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#optimize().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#fold_constants().","attributes":{"type":"uses","at":[210,12,210,26]}},{"key":"geid_138_1376","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#optimize().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#info().","attributes":{"type":"uses","at":[211,12,211,16]}},{"key":"geid_138_1377","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#optimize().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#name.","attributes":{"type":"uses","at":[211,22,211,26]}},{"key":"geid_138_1378","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#optimize().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#infer_shapes().","attributes":{"type":"uses","at":[212,12,212,24]}},{"key":"geid_138_1379","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#optimize().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#info().","attributes":{"type":"uses","at":[213,12,213,16]}},{"key":"geid_138_1380","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#optimize().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#name.","attributes":{"type":"uses","at":[213,22,213,26]}},{"key":"geid_138_1381","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#optimize().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#select_outputs().","attributes":{"type":"uses","at":[214,12,214,26]}},{"key":"geid_138_1382","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#optimize().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#info().","attributes":{"type":"uses","at":[215,12,215,16]}},{"key":"geid_138_1383","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#optimize().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#name.","attributes":{"type":"uses","at":[215,22,215,26]}},{"key":"geid_138_1384","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#optimize().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#cleanup().","attributes":{"type":"uses","at":[216,29,216,36]}},{"key":"geid_138_1385","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#optimize().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/Optimizer#info().","attributes":{"type":"uses","at":[217,12,217,16]}},{"key":"geid_138_1386","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#optimize().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#name.","attributes":{"type":"uses","at":[217,22,217,26]}},{"key":"geid_138_1387","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#","attributes":{"type":"uses","at":[221,11,221,20]}},{"key":"geid_138_1388","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#","attributes":{"type":"uses","at":[232,14,232,18]}},{"key":"geid_138_1389","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#__init__().","attributes":{"type":"uses","at":[232,26,232,34]}},{"key":"geid_138_1390","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_input_profile().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#check_dims().","attributes":{"type":"uses","at":[258,43,258,53]}},{"key":"geid_138_1391","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_input_profile().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_minmax_dims().","attributes":{"type":"uses","at":[270,17,270,32]}},{"key":"geid_138_1392","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_input_profile().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#unet_dim.","attributes":{"type":"uses","at":[273,33,273,41]}},{"key":"geid_138_1393","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_input_profile().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#unet_dim.","attributes":{"type":"uses","at":[274,34,274,42]}},{"key":"geid_138_1394","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_input_profile().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#unet_dim.","attributes":{"type":"uses","at":[275,33,275,41]}},{"key":"geid_138_1395","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_input_profile().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#text_maxlen.","attributes":{"type":"uses","at":[279,33,279,44]}},{"key":"geid_138_1396","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_input_profile().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#embedding_dim.","attributes":{"type":"uses","at":[279,51,279,64]}},{"key":"geid_138_1397","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_input_profile().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#text_maxlen.","attributes":{"type":"uses","at":[280,34,280,45]}},{"key":"geid_138_1398","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_input_profile().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#embedding_dim.","attributes":{"type":"uses","at":[280,52,280,65]}},{"key":"geid_138_1399","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_input_profile().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#text_maxlen.","attributes":{"type":"uses","at":[281,33,281,44]}},{"key":"geid_138_1400","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_input_profile().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#embedding_dim.","attributes":{"type":"uses","at":[281,51,281,64]}},{"key":"geid_138_1401","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_shape_dict().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#check_dims().","attributes":{"type":"uses","at":[286,43,286,53]}},{"key":"geid_138_1402","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_shape_dict().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#unet_dim.","attributes":{"type":"uses","at":[288,44,288,52]}},{"key":"geid_138_1403","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_shape_dict().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#text_maxlen.","attributes":{"type":"uses","at":[290,59,290,70]}},{"key":"geid_138_1404","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_shape_dict().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#embedding_dim.","attributes":{"type":"uses","at":[290,77,290,90]}},{"key":"geid_138_1405","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_sample_input().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#check_dims().","attributes":{"type":"uses","at":[295,43,295,53]}},{"key":"geid_138_1406","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_sample_input().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#fp16.","attributes":{"type":"uses","at":[296,38,296,42]}},{"key":"geid_138_1407","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_sample_input().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#unet_dim.","attributes":{"type":"uses","at":[299,37,299,45]}},{"key":"geid_138_1408","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_sample_input().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#device.","attributes":{"type":"uses","at":[299,109,299,115]}},{"key":"geid_138_1409","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_sample_input().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#device.","attributes":{"type":"uses","at":[301,75,301,81]}},{"key":"geid_138_1410","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_sample_input().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#text_maxlen.","attributes":{"type":"uses","at":[302,45,302,56]}},{"key":"geid_138_1411","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_sample_input().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#embedding_dim.","attributes":{"type":"uses","at":[302,63,302,76]}},{"key":"geid_138_1412","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#get_sample_input().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#device.","attributes":{"type":"uses","at":[302,103,302,109]}},{"key":"geid_138_1413","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#","attributes":{"type":"uses","at":[306,10,306,19]}},{"key":"geid_138_1414","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#","attributes":{"type":"uses","at":[308,14,308,17]}},{"key":"geid_138_1415","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#__init__().","attributes":{"type":"uses","at":[308,25,308,33]}},{"key":"geid_138_1416","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#get_input_profile().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#check_dims().","attributes":{"type":"uses","at":[329,43,329,53]}},{"key":"geid_138_1417","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#get_input_profile().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_minmax_dims().","attributes":{"type":"uses","at":[341,17,341,32]}},{"key":"geid_138_1418","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#get_shape_dict().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#check_dims().","attributes":{"type":"uses","at":[351,43,351,53]}},{"key":"geid_138_1419","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#get_sample_input().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#check_dims().","attributes":{"type":"uses","at":[358,43,358,53]}},{"key":"geid_138_1420","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#get_sample_input().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#device.","attributes":{"type":"uses","at":[365,24,365,30]}},{"key":"geid_138_1421","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#","attributes":{"type":"uses","at":[369,17,369,26]}},{"key":"geid_138_1422","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#","attributes":{"type":"uses","at":[371,14,371,24]}},{"key":"geid_138_1423","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#__init__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#__init__().","attributes":{"type":"uses","at":[371,32,371,40]}},{"key":"geid_138_1424","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#get_input_profile().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#min_batch.","attributes":{"type":"uses","at":[392,34,392,43]}},{"key":"geid_138_1425","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#get_input_profile().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#max_batch.","attributes":{"type":"uses","at":[392,67,392,76]}},{"key":"geid_138_1426","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#get_input_profile().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#min_batch.","attributes":{"type":"uses","at":[393,57,393,66]}},{"key":"geid_138_1427","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#get_input_profile().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#max_batch.","attributes":{"type":"uses","at":[394,57,394,66]}},{"key":"geid_138_1428","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#get_input_profile().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#check_dims().","attributes":{"type":"uses","at":[395,13,395,23]}},{"key":"geid_138_1429","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#get_input_profile().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_minmax_dims().","attributes":{"type":"uses","at":[407,17,407,32]}},{"key":"geid_138_1430","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#get_shape_dict().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#check_dims().","attributes":{"type":"uses","at":[418,43,418,53]}},{"key":"geid_138_1431","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#get_sample_input().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#check_dims().","attributes":{"type":"uses","at":[425,13,425,23]}},{"key":"geid_138_1432","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#get_sample_input().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#device.","attributes":{"type":"uses","at":[432,24,432,30]}},{"key":"geid_138_1433","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/optimize_onnx().","attributes":{"type":"defines","at":[431,0,440,28]}},{"key":"geid_138_1434","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/export_onnx().","attributes":{"type":"defines","at":[404,0,428,28]}},{"key":"geid_138_1435","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/build_engine().","attributes":{"type":"defines","at":[365,0,401,17]}},{"key":"geid_138_1436","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/create_models().","attributes":{"type":"defines","at":[326,0,362,17]}},{"key":"geid_138_1437","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/prepare_mask_and_masked_image().","attributes":{"type":"defines","at":[308,0,323,29]}},{"key":"geid_138_1438","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/preprocess_image().","attributes":{"type":"defines","at":[298,0,305,33]}},{"key":"geid_138_1439","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/decode_images().","attributes":{"type":"defines","at":[291,0,295,47]}},{"key":"geid_138_1440","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#infer().","attributes":{"type":"defines","at":[260,4,288,27]}},{"key":"geid_138_1441","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#allocate_buffers().","attributes":{"type":"defines","at":[247,4,258,42]}},{"key":"geid_138_1442","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#activate().","attributes":{"type":"defines","at":[240,4,245,65]}},{"key":"geid_138_1443","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#load().","attributes":{"type":"defines","at":[236,4,238,74]}},{"key":"geid_138_1444","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#build().","attributes":{"type":"defines","at":[203,4,234,50]}},{"key":"geid_138_1445","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#refit().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#map_name().","attributes":{"type":"defines","at":[137,8,140,23]}},{"key":"geid_138_1446","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#refit().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#add_to_map().","attributes":{"type":"defines","at":[107,8,112,41]}},{"key":"geid_138_1447","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#refit().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#convert_int64().","attributes":{"type":"defines","at":[101,8,105,22]}},{"key":"geid_138_1448","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#refit().","attributes":{"type":"defines","at":[100,4,201,19]}},{"key":"geid_138_1449","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#__del__().","attributes":{"type":"defines","at":[93,4,98,24]}},{"key":"geid_138_1450","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#__init__().","attributes":{"type":"defines","at":[82,4,91,39]}},{"key":"geid_138_1451","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#","attributes":{"type":"defines","at":[81,0,288,27]}},{"key":"geid_138_1452","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/CUASSERT().","attributes":{"type":"defines","at":[70,0,78,15]}},{"key":"geid_138_1453","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python python-stdlib 3.11 gc/__init__:","attributes":{"type":"uses","at":[20,7,20,9]}},{"key":"geid_138_1454","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python python-stdlib 3.11 collections/__init__:","attributes":{"type":"uses","at":[21,5,21,16]}},{"key":"geid_138_1455","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python python-stdlib 3.11 collections/OrderedDict#","attributes":{"type":"uses","at":[21,24,21,35]}},{"key":"geid_138_1456","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python python-stdlib 3.11 typing/__init__:","attributes":{"type":"uses","at":[22,5,22,11]}},{"key":"geid_138_1457","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python numpy 1.25.2 numpy/__init__:","attributes":{"type":"uses","at":[24,7,24,12]}},{"key":"geid_138_1458","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt`/__init__:","attributes":{"type":"uses","at":[27,7,27,15]}},{"key":"geid_138_1459","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python temp indexer cuda/__init__:","attributes":{"type":"uses","at":[29,5,29,9]}},{"key":"geid_138_1460","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python Pillow 10.0.0 PIL/__init__:","attributes":{"type":"uses","at":[30,5,30,8]}},{"key":"geid_138_1461","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[30,16,30,21]}},{"key":"geid_138_1462","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python temp indexer polygraphy/__init__:","attributes":{"type":"uses","at":[31,5,31,15]}},{"key":"geid_138_1463","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python temp indexer `polygraphy.backend.common`/__init__:","attributes":{"type":"uses","at":[32,5,32,30]}},{"key":"geid_138_1464","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python temp indexer `polygraphy.backend.trt`/__init__:","attributes":{"type":"uses","at":[33,5,33,27]}},{"key":"geid_138_1465","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python temp indexer `polygraphy.backend.trt`/__init__:","attributes":{"type":"uses","at":[41,5,41,27]}},{"key":"geid_138_1466","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/__init__:","attributes":{"type":"uses","at":[43,5,43,12]}},{"key":"geid_138_1467","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#","attributes":{"type":"uses","at":[43,20,43,24]}},{"key":"geid_138_1468","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#","attributes":{"type":"uses","at":[43,26,43,29]}},{"key":"geid_138_1469","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#","attributes":{"type":"uses","at":[43,31,43,40]}},{"key":"geid_138_1470","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#","attributes":{"type":"uses","at":[43,42,43,46]}},{"key":"geid_138_1471","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#","attributes":{"type":"uses","at":[43,48,43,58]}},{"key":"geid_138_1472","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python numpy 1.25.2 numpy/uint8.","attributes":{"type":"uses","at":[50,7,50,12]}},{"key":"geid_138_1473","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python numpy 1.25.2 numpy/int8.","attributes":{"type":"uses","at":[51,7,51,11]}},{"key":"geid_138_1474","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python numpy 1.25.2 numpy/int16.","attributes":{"type":"uses","at":[52,7,52,12]}},{"key":"geid_138_1475","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python numpy 1.25.2 numpy/int32.","attributes":{"type":"uses","at":[53,7,53,12]}},{"key":"geid_138_1476","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python numpy 1.25.2 numpy/int64.","attributes":{"type":"uses","at":[54,7,54,12]}},{"key":"geid_138_1477","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python numpy 1.25.2 numpy/float16.","attributes":{"type":"uses","at":[55,7,55,14]}},{"key":"geid_138_1478","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python numpy 1.25.2 numpy/float32.","attributes":{"type":"uses","at":[56,7,56,14]}},{"key":"geid_138_1479","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python numpy 1.25.2 numpy/float64.","attributes":{"type":"uses","at":[57,7,57,14]}},{"key":"geid_138_1480","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python numpy 1.25.2 numpy/complex64.","attributes":{"type":"uses","at":[58,7,58,16]}},{"key":"geid_138_1481","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python numpy 1.25.2 numpy/complex128.","attributes":{"type":"uses","at":[59,7,59,17]}},{"key":"geid_138_1482","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python numpy 1.25.2 `numpy.version`/__init__:","attributes":{"type":"uses","at":[61,6,61,13]}},{"key":"geid_138_1483","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python numpy 1.25.2 `numpy.version`/full_version.","attributes":{"type":"uses","at":[61,14,61,26]}},{"key":"geid_138_1484","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/numpy_to_torch_dtype_dict.","attributes":{"type":"uses","at":[62,4,62,29]}},{"key":"geid_138_1485","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python numpy 1.25.2 numpy/bool_#","attributes":{"type":"uses","at":[62,33,62,38]}},{"key":"geid_138_1486","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/numpy_to_torch_dtype_dict.","attributes":{"type":"uses","at":[64,4,64,29]}},{"key":"geid_138_1487","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/numpy_to_torch_dtype_dict.","attributes":{"type":"uses","at":[67,60,67,85]}},{"key":"geid_138_1488","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#__init__().","target":"scip-python python python-stdlib 3.11 collections/OrderedDict#","attributes":{"type":"uses","at":[89,23,89,34]}},{"key":"geid_138_1489","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#__init__().","target":"scip-python python python-stdlib 3.11 collections/OrderedDict#","attributes":{"type":"uses","at":[90,23,90,34]}},{"key":"geid_138_1490","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#__del__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#buffers.","attributes":{"type":"uses","at":[94,36,94,43]}},{"key":"geid_138_1491","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#__del__().","target":"scip-python python python-stdlib 3.11 collections/OrderedDict#values().","attributes":{"type":"uses","at":[94,44,94,50]}},{"key":"geid_138_1492","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#__del__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#engine.","attributes":{"type":"uses","at":[95,17,95,23]}},{"key":"geid_138_1493","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#__del__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#context.","attributes":{"type":"uses","at":[96,17,96,24]}},{"key":"geid_138_1494","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#__del__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#buffers.","attributes":{"type":"uses","at":[97,17,97,24]}},{"key":"geid_138_1495","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#__del__().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#tensors.","attributes":{"type":"uses","at":[98,17,98,24]}},{"key":"geid_138_1496","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#convert_int64().","target":"scip-python python numpy 1.25.2 numpy/int32.","attributes":{"type":"uses","at":[104,26,104,31]}},{"key":"geid_138_1497","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#add_to_map().","target":"scip-python python numpy 1.25.2 numpy/int64.","attributes":{"type":"uses","at":[110,38,110,43]}},{"key":"geid_138_1498","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#add_to_map().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#convert_int64().","attributes":{"type":"uses","at":[111,29,111,42]}},{"key":"geid_138_1499","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#refit().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#engine.","attributes":{"type":"uses","at":[144,37,144,43]}},{"key":"geid_138_1500","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#refit().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/TRT_LOGGER.","attributes":{"type":"uses","at":[144,45,144,55]}},{"key":"geid_138_1501","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#refit().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#map_name().","attributes":{"type":"uses","at":[161,23,161,31]}},{"key":"geid_138_1502","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#refit().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#add_to_map().","attributes":{"type":"uses","at":[163,16,163,26]}},{"key":"geid_138_1503","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#refit().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#map_name().","attributes":{"type":"uses","at":[168,27,168,35]}},{"key":"geid_138_1504","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#refit().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#add_to_map().","attributes":{"type":"uses","at":[169,20,169,30]}},{"key":"geid_138_1505","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#refit().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#map_name().","attributes":{"type":"uses","at":[172,27,172,35]}},{"key":"geid_138_1506","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#refit().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#add_to_map().","attributes":{"type":"uses","at":[173,20,173,30]}},{"key":"geid_138_1507","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#refit().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#map_name().","attributes":{"type":"uses","at":[178,27,178,35]}},{"key":"geid_138_1508","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#refit().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#add_to_map().","attributes":{"type":"uses","at":[180,24,180,34]}},{"key":"geid_138_1509","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#build().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#engine_path.","attributes":{"type":"uses","at":[213,64,213,75]}},{"key":"geid_138_1510","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#build().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#engine_path.","attributes":{"type":"uses","at":[234,38,234,49]}},{"key":"geid_138_1511","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#load().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#engine_path.","attributes":{"type":"uses","at":[237,47,237,58]}},{"key":"geid_138_1512","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#load().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#engine.","attributes":{"type":"uses","at":[238,13,238,19]}},{"key":"geid_138_1513","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#load().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#engine_path.","attributes":{"type":"uses","at":[238,61,238,72]}},{"key":"geid_138_1514","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#activate().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#context.","attributes":{"type":"uses","at":[242,17,242,24]}},{"key":"geid_138_1515","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#activate().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#engine.","attributes":{"type":"uses","at":[242,32,242,38]}},{"key":"geid_138_1516","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#activate().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#context.","attributes":{"type":"uses","at":[243,17,243,24]}},{"key":"geid_138_1517","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#activate().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#context.","attributes":{"type":"uses","at":[245,17,245,24]}},{"key":"geid_138_1518","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#activate().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#engine.","attributes":{"type":"uses","at":[245,32,245,38]}},{"key":"geid_138_1519","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#allocate_buffers().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#engine.","attributes":{"type":"uses","at":[248,64,248,70]}},{"key":"geid_138_1520","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#allocate_buffers().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#engine.","attributes":{"type":"uses","at":[249,27,249,33]}},{"key":"geid_138_1521","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#allocate_buffers().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#engine.","attributes":{"type":"uses","at":[253,29,253,35]}},{"key":"geid_138_1522","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#allocate_buffers().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#engine.","attributes":{"type":"uses","at":[254,36,254,42]}},{"key":"geid_138_1523","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#allocate_buffers().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#engine.","attributes":{"type":"uses","at":[255,20,255,26]}},{"key":"geid_138_1524","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#allocate_buffers().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#context.","attributes":{"type":"uses","at":[256,21,256,28]}},{"key":"geid_138_1525","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#allocate_buffers().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/numpy_to_torch_dtype_dict.","attributes":{"type":"uses","at":[257,53,257,78]}},{"key":"geid_138_1526","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#allocate_buffers().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#tensors.","attributes":{"type":"uses","at":[258,17,258,24]}},{"key":"geid_138_1527","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#infer().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#tensors.","attributes":{"type":"uses","at":[262,17,262,24]}},{"key":"geid_138_1528","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#infer().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#tensors.","attributes":{"type":"uses","at":[264,33,264,40]}},{"key":"geid_138_1529","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#infer().","target":"scip-python python python-stdlib 3.11 collections/OrderedDict#items().","attributes":{"type":"uses","at":[264,41,264,46]}},{"key":"geid_138_1530","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#infer().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#context.","attributes":{"type":"uses","at":[265,17,265,24]}},{"key":"geid_138_1531","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#infer().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#cuda_graph_instance.","attributes":{"type":"uses","at":[268,20,268,39]}},{"key":"geid_138_1532","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#infer().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/CUASSERT().","attributes":{"type":"uses","at":[269,16,269,24]}},{"key":"geid_138_1533","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#infer().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#cuda_graph_instance.","attributes":{"type":"uses","at":[269,53,269,72]}},{"key":"geid_138_1534","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#infer().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/CUASSERT().","attributes":{"type":"uses","at":[270,16,270,24]}},{"key":"geid_138_1535","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#infer().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#context.","attributes":{"type":"uses","at":[273,31,273,38]}},{"key":"geid_138_1536","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#infer().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/CUASSERT().","attributes":{"type":"uses","at":[277,16,277,24]}},{"key":"geid_138_1537","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#infer().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#context.","attributes":{"type":"uses","at":[280,21,280,28]}},{"key":"geid_138_1538","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#infer().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/CUASSERT().","attributes":{"type":"uses","at":[281,29,281,37]}},{"key":"geid_138_1539","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#infer().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#cuda_graph_instance.","attributes":{"type":"uses","at":[282,21,282,40]}},{"key":"geid_138_1540","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#infer().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/CUASSERT().","attributes":{"type":"uses","at":[282,43,282,51]}},{"key":"geid_138_1541","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#infer().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#graph.","attributes":{"type":"uses","at":[282,85,282,90]}},{"key":"geid_138_1542","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#infer().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#context.","attributes":{"type":"uses","at":[284,27,284,34]}},{"key":"geid_138_1543","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#infer().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#tensors.","attributes":{"type":"uses","at":[288,20,288,27]}},{"key":"geid_138_1544","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/decode_images().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[295,12,295,17]}},{"key":"geid_138_1545","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/decode_images().","target":"scip-python python Pillow 10.0.0 `pil.image`/fromarray().","attributes":{"type":"uses","at":[295,18,295,27]}},{"key":"geid_138_1546","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/preprocess_image().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[298,28,298,33]}},{"key":"geid_138_1547","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/preprocess_image().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#","attributes":{"type":"uses","at":[298,34,298,39]}},{"key":"geid_138_1548","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/preprocess_image().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#size().","attributes":{"type":"uses","at":[299,17,299,21]}},{"key":"geid_138_1549","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/preprocess_image().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#resize().","attributes":{"type":"uses","at":[301,18,301,24]}},{"key":"geid_138_1550","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/preprocess_image().","target":"scip-python python numpy 1.25.2 `numpy.core.multiarray`/array().","attributes":{"type":"uses","at":[302,20,302,25]}},{"key":"geid_138_1551","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/preprocess_image().","target":"scip-python python numpy 1.25.2 numpy/ndarray#astype().","attributes":{"type":"uses","at":[302,33,302,39]}},{"key":"geid_138_1552","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/preprocess_image().","target":"scip-python python numpy 1.25.2 numpy/float32.","attributes":{"type":"uses","at":[302,43,302,50]}},{"key":"geid_138_1553","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/preprocess_image().","target":"scip-python python numpy 1.25.2 numpy/ndarray#transpose().","attributes":{"type":"uses","at":[303,34,303,43]}},{"key":"geid_138_1554","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/prepare_mask_and_masked_image().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[308,41,308,46]}},{"key":"geid_138_1555","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/prepare_mask_and_masked_image().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#","attributes":{"type":"uses","at":[308,47,308,52]}},{"key":"geid_138_1556","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/prepare_mask_and_masked_image().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[308,60,308,65]}},{"key":"geid_138_1557","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/prepare_mask_and_masked_image().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#","attributes":{"type":"uses","at":[308,66,308,71]}},{"key":"geid_138_1558","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/prepare_mask_and_masked_image().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[309,25,309,30]}},{"key":"geid_138_1559","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/prepare_mask_and_masked_image().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#","attributes":{"type":"uses","at":[309,31,309,36]}},{"key":"geid_138_1560","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/prepare_mask_and_masked_image().","target":"scip-python python numpy 1.25.2 `numpy.core.multiarray`/array().","attributes":{"type":"uses","at":[310,19,310,24]}},{"key":"geid_138_1561","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/prepare_mask_and_masked_image().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#convert().","attributes":{"type":"uses","at":[310,31,310,38]}},{"key":"geid_138_1562","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/prepare_mask_and_masked_image().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[313,24,313,29]}},{"key":"geid_138_1563","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/prepare_mask_and_masked_image().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#","attributes":{"type":"uses","at":[313,30,313,35]}},{"key":"geid_138_1564","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/prepare_mask_and_masked_image().","target":"scip-python python numpy 1.25.2 `numpy.core.multiarray`/array().","attributes":{"type":"uses","at":[314,18,314,23]}},{"key":"geid_138_1565","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/prepare_mask_and_masked_image().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#convert().","attributes":{"type":"uses","at":[314,29,314,36]}},{"key":"geid_138_1566","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/prepare_mask_and_masked_image().","target":"scip-python python numpy 1.25.2 numpy/float32.","attributes":{"type":"uses","at":[315,30,315,37]}},{"key":"geid_138_1567","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/create_models().","target":"scip-python python python-stdlib 3.11 typing/__init__:","attributes":{"type":"uses","at":[328,20,328,28]}},{"key":"geid_138_1568","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/create_models().","target":"scip-python python python-stdlib 3.11 typing/__init__:","attributes":{"type":"uses","at":[329,12,329,17]}},{"key":"geid_138_1569","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/create_models().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/CLIP#","attributes":{"type":"uses","at":[335,16,335,20]}},{"key":"geid_138_1570","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/create_models().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/UNet#","attributes":{"type":"uses","at":[341,16,341,20]}},{"key":"geid_138_1571","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/create_models().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAE#","attributes":{"type":"uses","at":[349,15,349,18]}},{"key":"geid_138_1572","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/create_models().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/VAEEncoder#","attributes":{"type":"uses","at":[355,23,355,33]}},{"key":"geid_138_1573","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/build_engine().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#","attributes":{"type":"uses","at":[368,16,368,25]}},{"key":"geid_138_1574","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/build_engine().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#","attributes":{"type":"uses","at":[384,13,384,19]}},{"key":"geid_138_1575","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/build_engine().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_input_profile().","attributes":{"type":"uses","at":[385,31,385,48]}},{"key":"geid_138_1576","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/build_engine().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/Engine#build().","attributes":{"type":"uses","at":[392,11,392,16]}},{"key":"geid_138_1577","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/export_onnx().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#","attributes":{"type":"uses","at":[407,16,407,25]}},{"key":"geid_138_1578","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/export_onnx().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_sample_input().","attributes":{"type":"uses","at":[414,28,414,44]}},{"key":"geid_138_1579","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/export_onnx().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_input_names().","attributes":{"type":"uses","at":[422,35,422,50]}},{"key":"geid_138_1580","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/export_onnx().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_output_names().","attributes":{"type":"uses","at":[423,36,423,52]}},{"key":"geid_138_1581","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/export_onnx().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#get_dynamic_axes().","attributes":{"type":"uses","at":[424,36,424,52]}},{"key":"geid_138_1582","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/export_onnx().","target":"scip-python python python-stdlib 3.11 gc/__init__:","attributes":{"type":"uses","at":[427,4,427,6]}},{"key":"geid_138_1583","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/export_onnx().","target":"scip-python python python-stdlib 3.11 gc/collect().","attributes":{"type":"uses","at":[427,7,427,14]}},{"key":"geid_138_1584","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/optimize_onnx().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#","attributes":{"type":"uses","at":[434,16,434,25]}},{"key":"geid_138_1585","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/optimize_onnx().","target":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.models`/BaseModel#optimize().","attributes":{"type":"uses","at":[436,32,436,40]}},{"key":"geid_138_1586","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/optimize_onnx().","target":"scip-python python python-stdlib 3.11 gc/__init__:","attributes":{"type":"uses","at":[439,4,439,6]}},{"key":"geid_138_1587","source":"scip-python python temp indexer `src.streamdiffusion.acceleration.tensorrt.utilities`/optimize_onnx().","target":"scip-python python python-stdlib 3.11 gc/collect().","attributes":{"type":"uses","at":[439,7,439,14]}},{"key":"geid_138_1588","source":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/install().","attributes":{"type":"defines","at":[17,0,44,8]}},{"key":"geid_138_1589","source":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/get_cuda_version_from_torch().","attributes":{"type":"defines","at":[8,0,14,43]}},{"key":"geid_138_1590","source":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/__init__:","target":"scip-python python python-stdlib 3.11 typing/__init__:","attributes":{"type":"uses","at":[0,5,0,11]}},{"key":"geid_138_1591","source":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[0,19,0,26]}},{"key":"geid_138_1592","source":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[0,28,0,36]}},{"key":"geid_138_1593","source":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/__init__:","target":"scip-python python packaging 23.1 `packaging.version`/__init__:","attributes":{"type":"uses","at":[3,5,3,22]}},{"key":"geid_138_1594","source":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/__init__:","target":"scip-python python packaging 23.1 `packaging.version`/Version#","attributes":{"type":"uses","at":[3,30,3,37]}},{"key":"geid_138_1595","source":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/__init__:","attributes":{"type":"uses","at":[5,5,5,16]}},{"key":"geid_138_1596","source":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/is_installed().","attributes":{"type":"uses","at":[5,24,5,36]}},{"key":"geid_138_1597","source":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/run_pip().","attributes":{"type":"uses","at":[5,38,5,45]}},{"key":"geid_138_1598","source":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/version().","attributes":{"type":"uses","at":[5,47,5,54]}},{"key":"geid_138_1599","source":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/get_cuda_version_from_torch().","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[8,37,8,45]}},{"key":"geid_138_1600","source":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/get_cuda_version_from_torch().","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[8,46,8,53]}},{"key":"geid_138_1601","source":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/install().","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[17,16,17,24]}},{"key":"geid_138_1602","source":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/install().","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[17,25,17,32]}},{"key":"geid_138_1603","source":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/install().","target":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/get_cuda_version_from_torch().","attributes":{"type":"uses","at":[17,48,17,75]}},{"key":"geid_138_1604","source":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/install().","target":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/is_installed().","attributes":{"type":"uses","at":[23,7,23,19]}},{"key":"geid_138_1605","source":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/install().","target":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/version().","attributes":{"type":"uses","at":[24,11,24,18]}},{"key":"geid_138_1606","source":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/install().","target":"scip-python python packaging 23.1 `packaging.version`/Version#","attributes":{"type":"uses","at":[24,33,24,40]}},{"key":"geid_138_1607","source":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/install().","target":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/run_pip().","attributes":{"type":"uses","at":[25,12,25,19]}},{"key":"geid_138_1608","source":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/install().","target":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/is_installed().","attributes":{"type":"uses","at":[29,11,29,23]}},{"key":"geid_138_1609","source":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/install().","target":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/run_pip().","attributes":{"type":"uses","at":[30,8,30,15]}},{"key":"geid_138_1610","source":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/install().","target":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/run_pip().","attributes":{"type":"uses","at":[31,8,31,15]}},{"key":"geid_138_1611","source":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/install().","target":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/is_installed().","attributes":{"type":"uses","at":[35,11,35,23]}},{"key":"geid_138_1612","source":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/install().","target":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/run_pip().","attributes":{"type":"uses","at":[36,8,36,15]}},{"key":"geid_138_1613","source":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/install().","target":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/is_installed().","attributes":{"type":"uses","at":[39,11,39,23]}},{"key":"geid_138_1614","source":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/install().","target":"scip-python python temp indexer `src.streamdiffusion.pip_utils`/run_pip().","attributes":{"type":"uses","at":[40,8,40,15]}},{"key":"geid_138_1615","source":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/","attributes":{"type":"uses","at":[47,3,47,11]}},{"key":"geid_138_1616","source":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/__init__:","target":"scip-python python temp indexer `src.streamdiffusion.tools.install-tensorrt`/install().","attributes":{"type":"uses","at":[48,14,48,21]}},{"key":"geid_138_1617","source":"scip-python python temp indexer `utils.viewer`/receive_images().","target":"scip-python python temp indexer `utils.viewer`/receive_images().on_closing().","attributes":{"type":"defines","at":[82,4,85,14]}},{"key":"geid_138_1618","source":"scip-python python temp indexer `utils.viewer`/__init__:","target":"scip-python python temp indexer `utils.viewer`/receive_images().","attributes":{"type":"defines","at":[64,0,96,14]}},{"key":"geid_138_1619","source":"scip-python python temp indexer `utils.viewer`/__init__:","target":"scip-python python temp indexer `utils.viewer`/_receive_images().","attributes":{"type":"defines","at":[30,0,61,18]}},{"key":"geid_138_1620","source":"scip-python python temp indexer `utils.viewer`/__init__:","target":"scip-python python temp indexer `utils.viewer`/update_image().","attributes":{"type":"defines","at":[13,0,28,26]}},{"key":"geid_138_1621","source":"scip-python python temp indexer `utils.viewer`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[0,7,0,9]}},{"key":"geid_138_1622","source":"scip-python python temp indexer `utils.viewer`/__init__:","target":"scip-python python python-stdlib 3.11 sys/__init__:","attributes":{"type":"uses","at":[1,7,1,10]}},{"key":"geid_138_1623","source":"scip-python python temp indexer `utils.viewer`/__init__:","target":"scip-python python python-stdlib 3.11 threading/__init__:","attributes":{"type":"uses","at":[2,7,2,16]}},{"key":"geid_138_1624","source":"scip-python python temp indexer `utils.viewer`/__init__:","target":"scip-python python python-stdlib 3.11 time/__init__:","attributes":{"type":"uses","at":[3,7,3,11]}},{"key":"geid_138_1625","source":"scip-python python temp indexer `utils.viewer`/__init__:","target":"scip-python python python-stdlib 3.11 tkinter/__init__:","attributes":{"type":"uses","at":[4,7,4,14]}},{"key":"geid_138_1626","source":"scip-python python temp indexer `utils.viewer`/__init__:","target":"scip-python python python-stdlib 3.11 multiprocessing/__init__:","attributes":{"type":"uses","at":[5,5,5,20]}},{"key":"geid_138_1627","source":"scip-python python temp indexer `utils.viewer`/__init__:","target":"scip-python python python-stdlib 3.11 typing/__init__:","attributes":{"type":"uses","at":[6,5,6,11]}},{"key":"geid_138_1628","source":"scip-python python temp indexer `utils.viewer`/__init__:","target":"scip-python python python-stdlib 3.11 typing/List.","attributes":{"type":"uses","at":[6,19,6,23]}},{"key":"geid_138_1629","source":"scip-python python temp indexer `utils.viewer`/__init__:","target":"scip-python python Pillow 10.0.0 PIL/__init__:","attributes":{"type":"uses","at":[7,5,7,8]}},{"key":"geid_138_1630","source":"scip-python python temp indexer `utils.viewer`/__init__:","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[7,16,7,21]}},{"key":"geid_138_1631","source":"scip-python python temp indexer `utils.viewer`/__init__:","target":"scip-python python Pillow 10.0.0 `PIL.ImageTk`/__init__:","attributes":{"type":"uses","at":[7,23,7,30]}},{"key":"geid_138_1632","source":"scip-python python temp indexer `utils.viewer`/__init__:","target":"scip-python python temp indexer `streamdiffusion.image_utils`/__init__:","attributes":{"type":"uses","at":[8,5,8,32]}},{"key":"geid_138_1633","source":"scip-python python temp indexer `utils.viewer`/__init__:","target":"scip-python python python-stdlib 3.11 sys/__init__:","attributes":{"type":"uses","at":[10,0,10,3]}},{"key":"geid_138_1634","source":"scip-python python temp indexer `utils.viewer`/__init__:","target":"scip-python python python-stdlib 3.11 sys/path.path.","attributes":{"type":"uses","at":[10,4,10,8]}},{"key":"geid_138_1635","source":"scip-python python temp indexer `utils.viewer`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[10,16,10,18]}},{"key":"geid_138_1636","source":"scip-python python temp indexer `utils.viewer`/__init__:","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[10,19,10,23]}},{"key":"geid_138_1637","source":"scip-python python temp indexer `utils.viewer`/__init__:","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[10,24,10,28]}},{"key":"geid_138_1638","source":"scip-python python temp indexer `utils.viewer`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[10,29,10,31]}},{"key":"geid_138_1639","source":"scip-python python temp indexer `utils.viewer`/__init__:","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[10,32,10,36]}},{"key":"geid_138_1640","source":"scip-python python temp indexer `utils.viewer`/__init__:","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[10,37,10,44]}},{"key":"geid_138_1641","source":"scip-python python temp indexer `utils.viewer`/update_image().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[13,29,13,34]}},{"key":"geid_138_1642","source":"scip-python python temp indexer `utils.viewer`/update_image().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#","attributes":{"type":"uses","at":[13,35,13,40]}},{"key":"geid_138_1643","source":"scip-python python temp indexer `utils.viewer`/update_image().","target":"scip-python python python-stdlib 3.11 tkinter/Label#","attributes":{"type":"uses","at":[13,52,13,57]}},{"key":"geid_138_1644","source":"scip-python python temp indexer `utils.viewer`/update_image().","target":"scip-python python Pillow 10.0.0 `PIL.ImageTk`/__init__:","attributes":{"type":"uses","at":[26,15,26,22]}},{"key":"geid_138_1645","source":"scip-python python temp indexer `utils.viewer`/update_image().","target":"scip-python python Pillow 10.0.0 `pil.imagetk`/PhotoImage#","attributes":{"type":"uses","at":[26,23,26,33]}},{"key":"geid_138_1646","source":"scip-python python temp indexer `utils.viewer`/update_image().","target":"scip-python python python-stdlib 3.11 tkinter/Label#configure().","attributes":{"type":"uses","at":[27,10,27,19]}},{"key":"geid_138_1647","source":"scip-python python temp indexer `utils.viewer`/_receive_images().","target":"scip-python python python-stdlib 3.11 tkinter/Label#","attributes":{"type":"uses","at":[31,46,31,51]}},{"key":"geid_138_1648","source":"scip-python python temp indexer `utils.viewer`/_receive_images().","target":"scip-python python python-stdlib 3.11 tkinter/Label#","attributes":{"type":"uses","at":[31,67,31,72]}},{"key":"geid_138_1649","source":"scip-python python temp indexer `utils.viewer`/_receive_images().","target":"scip-python python python-stdlib 3.11 queue/Queue#empty().","attributes":{"type":"uses","at":[49,25,49,30]}},{"key":"geid_138_1650","source":"scip-python python temp indexer `utils.viewer`/_receive_images().","target":"scip-python python python-stdlib 3.11 tkinter/Misc#after().","attributes":{"type":"uses","at":[50,22,50,27]}},{"key":"geid_138_1651","source":"scip-python python temp indexer `utils.viewer`/_receive_images().","target":"scip-python python temp indexer `utils.viewer`/update_image().","attributes":{"type":"uses","at":[52,20,52,32]}},{"key":"geid_138_1652","source":"scip-python python temp indexer `utils.viewer`/_receive_images().","target":"scip-python python python-stdlib 3.11 queue/Queue#empty().","attributes":{"type":"uses","at":[56,29,56,34]}},{"key":"geid_138_1653","source":"scip-python python temp indexer `utils.viewer`/_receive_images().","target":"scip-python python python-stdlib 3.11 tkinter/Label#config.","attributes":{"type":"uses","at":[57,26,57,32]}},{"key":"geid_138_1654","source":"scip-python python temp indexer `utils.viewer`/_receive_images().","target":"scip-python python python-stdlib 3.11 time/__init__:","attributes":{"type":"uses","at":[59,12,59,16]}},{"key":"geid_138_1655","source":"scip-python python temp indexer `utils.viewer`/_receive_images().","target":"scip-python python python-stdlib 3.11 time/sleep().","attributes":{"type":"uses","at":[59,17,59,22]}},{"key":"geid_138_1656","source":"scip-python python temp indexer `utils.viewer`/receive_images().","target":"scip-python python python-stdlib 3.11 tkinter/Tk#","attributes":{"type":"uses","at":[75,14,75,16]}},{"key":"geid_138_1657","source":"scip-python python temp indexer `utils.viewer`/receive_images().","target":"scip-python python python-stdlib 3.11 tkinter/Wm#title.","attributes":{"type":"uses","at":[76,9,76,14]}},{"key":"geid_138_1658","source":"scip-python python temp indexer `utils.viewer`/receive_images().","target":"scip-python python python-stdlib 3.11 tkinter/Label#","attributes":{"type":"uses","at":[77,15,77,20]}},{"key":"geid_138_1659","source":"scip-python python temp indexer `utils.viewer`/receive_images().","target":"scip-python python python-stdlib 3.11 tkinter/Label#","attributes":{"type":"uses","at":[78,19,78,24]}},{"key":"geid_138_1660","source":"scip-python python temp indexer `utils.viewer`/receive_images().","target":"scip-python python python-stdlib 3.11 tkinter/Grid#grid.","attributes":{"type":"uses","at":[79,10,79,14]}},{"key":"geid_138_1661","source":"scip-python python temp indexer `utils.viewer`/receive_images().","target":"scip-python python python-stdlib 3.11 tkinter/Grid#grid.","attributes":{"type":"uses","at":[80,14,80,18]}},{"key":"geid_138_1662","source":"scip-python python temp indexer `utils.viewer`/receive_images().","target":"scip-python python python-stdlib 3.11 threading/__init__:","attributes":{"type":"uses","at":[87,13,87,22]}},{"key":"geid_138_1663","source":"scip-python python temp indexer `utils.viewer`/receive_images().","target":"scip-python python python-stdlib 3.11 threading/Thread#","attributes":{"type":"uses","at":[87,23,87,29]}},{"key":"geid_138_1664","source":"scip-python python temp indexer `utils.viewer`/receive_images().","target":"scip-python python temp indexer `utils.viewer`/_receive_images().","attributes":{"type":"uses","at":[88,15,88,30]}},{"key":"geid_138_1665","source":"scip-python python temp indexer `utils.viewer`/receive_images().","target":"scip-python python python-stdlib 3.11 threading/Thread#start().","attributes":{"type":"uses","at":[90,11,90,16]}},{"key":"geid_138_1666","source":"scip-python python temp indexer `utils.viewer`/receive_images().","target":"scip-python python python-stdlib 3.11 tkinter/Wm#protocol.","attributes":{"type":"uses","at":[93,13,93,21]}},{"key":"geid_138_1667","source":"scip-python python temp indexer `utils.viewer`/receive_images().","target":"scip-python python python-stdlib 3.11 tkinter/Misc#mainloop().","attributes":{"type":"uses","at":[94,13,94,21]}},{"key":"geid_138_1668","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#create_prefix().","attributes":{"type":"defines","at":[481,16,490,176]}},{"key":"geid_138_1669","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","attributes":{"type":"defines","at":[344,4,656,21]}},{"key":"geid_138_1670","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#postprocess_image().","attributes":{"type":"defines","at":[323,4,342,84]}},{"key":"geid_138_1671","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#preprocess_image().","attributes":{"type":"defines","at":[300,4,321,50]}},{"key":"geid_138_1672","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#img2img().","attributes":{"type":"defines","at":[266,4,298,20]}},{"key":"geid_138_1673","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#txt2img().","attributes":{"type":"defines","at":[229,4,264,20]}},{"key":"geid_138_1674","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__call__().","attributes":{"type":"defines","at":[204,4,227,39]}},{"key":"geid_138_1675","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#prepare().","attributes":{"type":"defines","at":[173,4,202,9]}},{"key":"geid_138_1676","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__init__().","attributes":{"type":"defines","at":[21,4,171,120]}},{"key":"geid_138_1677","source":"scip-python python temp indexer `utils.wrapper`/__init__:","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#","attributes":{"type":"defines","at":[20,0,656,21]}},{"key":"geid_138_1678","source":"scip-python python temp indexer `utils.wrapper`/__init__:","target":"scip-python python python-stdlib 3.11 gc/__init__:","attributes":{"type":"uses","at":[0,7,0,9]}},{"key":"geid_138_1679","source":"scip-python python temp indexer `utils.wrapper`/__init__:","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[1,7,1,9]}},{"key":"geid_138_1680","source":"scip-python python temp indexer `utils.wrapper`/__init__:","target":"scip-python python python-stdlib 3.11 pathlib/__init__:","attributes":{"type":"uses","at":[2,5,2,12]}},{"key":"geid_138_1681","source":"scip-python python temp indexer `utils.wrapper`/__init__:","target":"scip-python python python-stdlib 3.11 pathlib/Path#","attributes":{"type":"uses","at":[2,20,2,24]}},{"key":"geid_138_1682","source":"scip-python python temp indexer `utils.wrapper`/__init__:","target":"scip-python python python-stdlib 3.11 traceback/__init__:","attributes":{"type":"uses","at":[3,7,3,16]}},{"key":"geid_138_1683","source":"scip-python python temp indexer `utils.wrapper`/__init__:","target":"scip-python python python-stdlib 3.11 typing/__init__:","attributes":{"type":"uses","at":[4,5,4,11]}},{"key":"geid_138_1684","source":"scip-python python temp indexer `utils.wrapper`/__init__:","target":"scip-python python python-stdlib 3.11 typing/List.","attributes":{"type":"uses","at":[4,19,4,23]}},{"key":"geid_138_1685","source":"scip-python python temp indexer `utils.wrapper`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[4,25,4,32]}},{"key":"geid_138_1686","source":"scip-python python temp indexer `utils.wrapper`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[4,34,4,42]}},{"key":"geid_138_1687","source":"scip-python python temp indexer `utils.wrapper`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Union.","attributes":{"type":"uses","at":[4,44,4,49]}},{"key":"geid_138_1688","source":"scip-python python temp indexer `utils.wrapper`/__init__:","target":"scip-python python python-stdlib 3.11 typing/Dict.","attributes":{"type":"uses","at":[4,51,4,55]}},{"key":"geid_138_1689","source":"scip-python python temp indexer `utils.wrapper`/__init__:","target":"scip-python python numpy 1.25.2 numpy/__init__:","attributes":{"type":"uses","at":[6,7,6,12]}},{"key":"geid_138_1690","source":"scip-python python temp indexer `utils.wrapper`/__init__:","target":"scip-python python temp indexer diffusers/__init__:","attributes":{"type":"uses","at":[8,5,8,14]}},{"key":"geid_138_1691","source":"scip-python python temp indexer `utils.wrapper`/__init__:","target":"scip-python python Pillow 10.0.0 PIL/__init__:","attributes":{"type":"uses","at":[9,5,9,8]}},{"key":"geid_138_1692","source":"scip-python python temp indexer `utils.wrapper`/__init__:","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[9,16,9,21]}},{"key":"geid_138_1693","source":"scip-python python temp indexer `utils.wrapper`/__init__:","target":"scip-python python temp indexer streamdiffusion/__init__:","attributes":{"type":"uses","at":[11,5,11,20]}},{"key":"geid_138_1694","source":"scip-python python temp indexer `utils.wrapper`/__init__:","target":"scip-python python temp indexer `streamdiffusion.image_utils`/__init__:","attributes":{"type":"uses","at":[12,5,12,32]}},{"key":"geid_138_1695","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__init__().","target":"scip-python python python-stdlib 3.11 typing/List.","attributes":{"type":"uses","at":[24,22,24,26]}},{"key":"geid_138_1696","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__init__().","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[25,19,25,27]}},{"key":"geid_138_1697","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__init__().","target":"scip-python python python-stdlib 3.11 typing/Dict.","attributes":{"type":"uses","at":[25,28,25,32]}},{"key":"geid_138_1698","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__init__().","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[26,14,26,21]}},{"key":"geid_138_1699","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__init__().","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[27,21,27,28]}},{"key":"geid_138_1700","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__init__().","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[28,21,28,29]}},{"key":"geid_138_1701","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__init__().","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[29,16,29,24]}},{"key":"geid_138_1702","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__init__().","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[30,16,30,23]}},{"key":"geid_138_1703","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__init__().","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[36,22,36,29]}},{"key":"geid_138_1704","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__init__().","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[38,20,38,28]}},{"key":"geid_138_1705","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__init__().","target":"scip-python python python-stdlib 3.11 typing/List.","attributes":{"type":"uses","at":[38,29,38,33]}},{"key":"geid_138_1706","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__init__().","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[45,18,45,25]}},{"key":"geid_138_1707","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__init__().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#sd_turbo.","attributes":{"type":"uses","at":[123,28,123,36]}},{"key":"geid_138_1708","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__init__().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","attributes":{"type":"uses","at":[150,44,150,55]}},{"key":"geid_138_1709","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__init__().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#stream.","attributes":{"type":"uses","at":[166,17,166,23]}},{"key":"geid_138_1710","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__init__().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#stream.","attributes":{"type":"uses","at":[167,21,167,27]}},{"key":"geid_138_1711","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__init__().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#stream.","attributes":{"type":"uses","at":[171,17,171,23]}},{"key":"geid_138_1712","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#prepare().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#stream.","attributes":{"type":"uses","at":[196,13,196,19]}},{"key":"geid_138_1713","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__call__().","target":"scip-python python python-stdlib 3.11 typing/Union.","attributes":{"type":"uses","at":[208,9,208,14]}},{"key":"geid_138_1714","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__call__().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[208,15,208,20]}},{"key":"geid_138_1715","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__call__().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#","attributes":{"type":"uses","at":[208,21,208,26]}},{"key":"geid_138_1716","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__call__().","target":"scip-python python python-stdlib 3.11 typing/List.","attributes":{"type":"uses","at":[208,28,208,32]}},{"key":"geid_138_1717","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__call__().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[208,33,208,38]}},{"key":"geid_138_1718","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__call__().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#","attributes":{"type":"uses","at":[208,39,208,44]}},{"key":"geid_138_1719","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__call__().","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[206,15,206,23]}},{"key":"geid_138_1720","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__call__().","target":"scip-python python python-stdlib 3.11 typing/Union.","attributes":{"type":"uses","at":[206,24,206,29]}},{"key":"geid_138_1721","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__call__().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[206,35,206,40]}},{"key":"geid_138_1722","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__call__().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#","attributes":{"type":"uses","at":[206,41,206,46]}},{"key":"geid_138_1723","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__call__().","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[207,16,207,24]}},{"key":"geid_138_1724","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__call__().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#mode.","attributes":{"type":"uses","at":[224,16,224,20]}},{"key":"geid_138_1725","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__call__().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#img2img().","attributes":{"type":"uses","at":[225,24,225,31]}},{"key":"geid_138_1726","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#__call__().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#txt2img().","attributes":{"type":"uses","at":[227,24,227,31]}},{"key":"geid_138_1727","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#txt2img().","target":"scip-python python python-stdlib 3.11 typing/Union.","attributes":{"type":"uses","at":[231,9,231,14]}},{"key":"geid_138_1728","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#txt2img().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[231,15,231,20]}},{"key":"geid_138_1729","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#txt2img().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#","attributes":{"type":"uses","at":[231,21,231,26]}},{"key":"geid_138_1730","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#txt2img().","target":"scip-python python python-stdlib 3.11 typing/List.","attributes":{"type":"uses","at":[231,28,231,32]}},{"key":"geid_138_1731","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#txt2img().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[231,33,231,38]}},{"key":"geid_138_1732","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#txt2img().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#","attributes":{"type":"uses","at":[231,39,231,44]}},{"key":"geid_138_1733","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#txt2img().","target":"scip-python python numpy 1.25.2 numpy/ndarray#","attributes":{"type":"uses","at":[231,64,231,71]}},{"key":"geid_138_1734","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#txt2img().","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[230,22,230,30]}},{"key":"geid_138_1735","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#txt2img().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#stream.","attributes":{"type":"uses","at":[246,17,246,23]}},{"key":"geid_138_1736","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#txt2img().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#sd_turbo.","attributes":{"type":"uses","at":[248,16,248,24]}},{"key":"geid_138_1737","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#txt2img().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#stream.","attributes":{"type":"uses","at":[249,32,249,38]}},{"key":"geid_138_1738","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#txt2img().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#batch_size.","attributes":{"type":"uses","at":[249,61,249,71]}},{"key":"geid_138_1739","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#txt2img().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#stream.","attributes":{"type":"uses","at":[251,32,251,38]}},{"key":"geid_138_1740","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#txt2img().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#frame_buffer_size.","attributes":{"type":"uses","at":[251,52,251,69]}},{"key":"geid_138_1741","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#txt2img().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#postprocess_image().","attributes":{"type":"uses","at":[252,21,252,38]}},{"key":"geid_138_1742","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#txt2img().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#output_type.","attributes":{"type":"uses","at":[252,70,252,81]}},{"key":"geid_138_1743","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#txt2img().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#use_safety_checker.","attributes":{"type":"uses","at":[254,16,254,34]}},{"key":"geid_138_1744","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#txt2img().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#feature_extractor.","attributes":{"type":"uses","at":[255,40,255,57]}},{"key":"geid_138_1745","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#txt2img().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#device.","attributes":{"type":"uses","at":[257,22,257,28]}},{"key":"geid_138_1746","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#txt2img().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#safety_checker.","attributes":{"type":"uses","at":[258,39,258,53]}},{"key":"geid_138_1747","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#txt2img().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#dtype.","attributes":{"type":"uses","at":[259,44,259,49]}},{"key":"geid_138_1748","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#txt2img().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#dtype.","attributes":{"type":"uses","at":[260,69,260,74]}},{"key":"geid_138_1749","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#txt2img().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#nsfw_fallback_img.","attributes":{"type":"uses","at":[262,25,262,42]}},{"key":"geid_138_1750","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#img2img().","target":"scip-python python python-stdlib 3.11 typing/Union.","attributes":{"type":"uses","at":[268,9,268,14]}},{"key":"geid_138_1751","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#img2img().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[268,15,268,20]}},{"key":"geid_138_1752","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#img2img().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#","attributes":{"type":"uses","at":[268,21,268,26]}},{"key":"geid_138_1753","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#img2img().","target":"scip-python python python-stdlib 3.11 typing/List.","attributes":{"type":"uses","at":[268,28,268,32]}},{"key":"geid_138_1754","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#img2img().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[268,33,268,38]}},{"key":"geid_138_1755","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#img2img().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#","attributes":{"type":"uses","at":[268,39,268,44]}},{"key":"geid_138_1756","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#img2img().","target":"scip-python python numpy 1.25.2 numpy/ndarray#","attributes":{"type":"uses","at":[268,64,268,71]}},{"key":"geid_138_1757","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#img2img().","target":"scip-python python python-stdlib 3.11 typing/Union.","attributes":{"type":"uses","at":[267,21,267,26]}},{"key":"geid_138_1758","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#img2img().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[267,32,267,37]}},{"key":"geid_138_1759","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#img2img().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#","attributes":{"type":"uses","at":[267,38,267,43]}},{"key":"geid_138_1760","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#img2img().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[282,55,282,60]}},{"key":"geid_138_1761","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#img2img().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#","attributes":{"type":"uses","at":[282,61,282,66]}},{"key":"geid_138_1762","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#img2img().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#preprocess_image().","attributes":{"type":"uses","at":[283,25,283,41]}},{"key":"geid_138_1763","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#img2img().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#stream.","attributes":{"type":"uses","at":[285,28,285,34]}},{"key":"geid_138_1764","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#img2img().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#postprocess_image().","attributes":{"type":"uses","at":[286,21,286,38]}},{"key":"geid_138_1765","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#img2img().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#output_type.","attributes":{"type":"uses","at":[286,70,286,81]}},{"key":"geid_138_1766","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#img2img().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#use_safety_checker.","attributes":{"type":"uses","at":[288,16,288,34]}},{"key":"geid_138_1767","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#img2img().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#feature_extractor.","attributes":{"type":"uses","at":[289,40,289,57]}},{"key":"geid_138_1768","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#img2img().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#device.","attributes":{"type":"uses","at":[291,22,291,28]}},{"key":"geid_138_1769","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#img2img().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#safety_checker.","attributes":{"type":"uses","at":[292,39,292,53]}},{"key":"geid_138_1770","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#img2img().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#dtype.","attributes":{"type":"uses","at":[293,44,293,49]}},{"key":"geid_138_1771","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#img2img().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#dtype.","attributes":{"type":"uses","at":[294,69,294,74]}},{"key":"geid_138_1772","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#img2img().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#nsfw_fallback_img.","attributes":{"type":"uses","at":[296,25,296,42]}},{"key":"geid_138_1773","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#preprocess_image().","target":"scip-python python python-stdlib 3.11 typing/Union.","attributes":{"type":"uses","at":[300,38,300,43]}},{"key":"geid_138_1774","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#preprocess_image().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[300,49,300,54]}},{"key":"geid_138_1775","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#preprocess_image().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#","attributes":{"type":"uses","at":[300,55,300,60]}},{"key":"geid_138_1776","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#preprocess_image().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[315,20,315,25]}},{"key":"geid_138_1777","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#preprocess_image().","target":"scip-python python Pillow 10.0.0 `pil.image`/open().","attributes":{"type":"uses","at":[315,26,315,30]}},{"key":"geid_138_1778","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#preprocess_image().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#convert().","attributes":{"type":"uses","at":[315,38,315,45]}},{"key":"geid_138_1779","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#preprocess_image().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#resize().","attributes":{"type":"uses","at":[315,53,315,59]}},{"key":"geid_138_1780","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#preprocess_image().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#width.","attributes":{"type":"uses","at":[315,66,315,71]}},{"key":"geid_138_1781","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#preprocess_image().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#height.","attributes":{"type":"uses","at":[315,78,315,84]}},{"key":"geid_138_1782","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#preprocess_image().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[316,29,316,34]}},{"key":"geid_138_1783","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#preprocess_image().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#","attributes":{"type":"uses","at":[316,35,316,40]}},{"key":"geid_138_1784","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#preprocess_image().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#convert().","attributes":{"type":"uses","at":[317,26,317,33]}},{"key":"geid_138_1785","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#preprocess_image().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#resize().","attributes":{"type":"uses","at":[317,41,317,47]}},{"key":"geid_138_1786","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#preprocess_image().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#width.","attributes":{"type":"uses","at":[317,54,317,59]}},{"key":"geid_138_1787","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#preprocess_image().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#height.","attributes":{"type":"uses","at":[317,66,317,72]}},{"key":"geid_138_1788","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#preprocess_image().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#stream.","attributes":{"type":"uses","at":[319,20,319,26]}},{"key":"geid_138_1789","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#preprocess_image().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#height.","attributes":{"type":"uses","at":[320,24,320,30]}},{"key":"geid_138_1790","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#preprocess_image().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#width.","attributes":{"type":"uses","at":[320,37,320,42]}},{"key":"geid_138_1791","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#preprocess_image().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#device.","attributes":{"type":"uses","at":[321,25,321,31]}},{"key":"geid_138_1792","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#preprocess_image().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#dtype.","attributes":{"type":"uses","at":[321,44,321,49]}},{"key":"geid_138_1793","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#postprocess_image().","target":"scip-python python python-stdlib 3.11 typing/Union.","attributes":{"type":"uses","at":[325,9,325,14]}},{"key":"geid_138_1794","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#postprocess_image().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[325,15,325,20]}},{"key":"geid_138_1795","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#postprocess_image().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#","attributes":{"type":"uses","at":[325,21,325,26]}},{"key":"geid_138_1796","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#postprocess_image().","target":"scip-python python python-stdlib 3.11 typing/List.","attributes":{"type":"uses","at":[325,28,325,32]}},{"key":"geid_138_1797","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#postprocess_image().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[325,33,325,38]}},{"key":"geid_138_1798","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#postprocess_image().","target":"scip-python python Pillow 10.0.0 `pil.image`/Image#","attributes":{"type":"uses","at":[325,39,325,44]}},{"key":"geid_138_1799","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#postprocess_image().","target":"scip-python python numpy 1.25.2 numpy/ndarray#","attributes":{"type":"uses","at":[325,64,325,71]}},{"key":"geid_138_1800","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#postprocess_image().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#frame_buffer_size.","attributes":{"type":"uses","at":[339,16,339,33]}},{"key":"geid_138_1801","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 typing/List.","attributes":{"type":"uses","at":[347,22,347,26]}},{"key":"geid_138_1802","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[348,19,348,27]}},{"key":"geid_138_1803","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 typing/Dict.","attributes":{"type":"uses","at":[348,28,348,32]}},{"key":"geid_138_1804","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[349,21,349,29]}},{"key":"geid_138_1805","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 typing/Optional.","attributes":{"type":"uses","at":[350,16,350,24]}},{"key":"geid_138_1806","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[351,22,351,29]}},{"key":"geid_138_1807","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 typing/Literal.","attributes":{"type":"uses","at":[356,18,356,25]}},{"key":"geid_138_1808","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#device.","attributes":{"type":"uses","at":[412,29,412,35]}},{"key":"geid_138_1809","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#dtype.","attributes":{"type":"uses","at":[412,48,412,53]}},{"key":"geid_138_1810","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#device.","attributes":{"type":"uses","at":[417,29,417,35]}},{"key":"geid_138_1811","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#dtype.","attributes":{"type":"uses","at":[417,48,417,53]}},{"key":"geid_138_1812","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 traceback/__init__:","attributes":{"type":"uses","at":[419,12,419,21]}},{"key":"geid_138_1813","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 traceback/print_exc().","attributes":{"type":"uses","at":[419,22,419,31]}},{"key":"geid_138_1814","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#dtype.","attributes":{"type":"uses","at":[426,29,426,34]}},{"key":"geid_138_1815","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#width.","attributes":{"type":"uses","at":[427,23,427,28]}},{"key":"geid_138_1816","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#height.","attributes":{"type":"uses","at":[428,24,428,30]}},{"key":"geid_138_1817","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#frame_buffer_size.","attributes":{"type":"uses","at":[430,35,430,52]}},{"key":"geid_138_1818","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#use_denoising_batch.","attributes":{"type":"uses","at":[431,37,431,56]}},{"key":"geid_138_1819","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#sd_turbo.","attributes":{"type":"uses","at":[434,20,434,28]}},{"key":"geid_138_1820","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer polygraphy/__init__:","attributes":{"type":"uses","at":[464,21,464,31]}},{"key":"geid_138_1821","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `streamdiffusion.acceleration.tensorrt`/__init__:","attributes":{"type":"uses","at":[465,21,465,58]}},{"key":"geid_138_1822","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `streamdiffusion.acceleration.tensorrt.engine`/__init__:","attributes":{"type":"uses","at":[471,21,471,65]}},{"key":"geid_138_1823","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `streamdiffusion.acceleration.tensorrt.models`/__init__:","attributes":{"type":"uses","at":[475,21,475,65]}},{"key":"geid_138_1824","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#create_prefix().","target":"scip-python python python-stdlib 3.11 pathlib/Path#","attributes":{"type":"uses","at":[486,33,486,37]}},{"key":"geid_138_1825","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#create_prefix().","target":"scip-python python python-stdlib 3.11 pathlib/Path#exists().","attributes":{"type":"uses","at":[487,34,487,40]}},{"key":"geid_138_1826","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#create_prefix().","target":"scip-python python python-stdlib 3.11 pathlib/PurePath#stem().","attributes":{"type":"uses","at":[488,45,488,49]}},{"key":"geid_138_1827","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#create_prefix().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#mode.","attributes":{"type":"uses","at":[488,169,488,173]}},{"key":"geid_138_1828","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#create_prefix().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#mode.","attributes":{"type":"uses","at":[490,170,490,174]}},{"key":"geid_138_1829","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[492,29,492,31]}},{"key":"geid_138_1830","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[492,32,492,36]}},{"key":"geid_138_1831","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[492,37,492,41]}},{"key":"geid_138_1832","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[493,28,493,30]}},{"key":"geid_138_1833","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[493,31,493,35]}},{"key":"geid_138_1834","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[493,36,493,40]}},{"key":"geid_138_1835","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#create_prefix().","attributes":{"type":"uses","at":[495,20,495,33]}},{"key":"geid_138_1836","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[502,35,502,37]}},{"key":"geid_138_1837","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[502,38,502,42]}},{"key":"geid_138_1838","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[502,43,502,47]}},{"key":"geid_138_1839","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#create_prefix().","attributes":{"type":"uses","at":[504,20,504,33]}},{"key":"geid_138_1840","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#batch_size.","attributes":{"type":"uses","at":[506,44,506,54]}},{"key":"geid_138_1841","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#mode.","attributes":{"type":"uses","at":[507,32,507,36]}},{"key":"geid_138_1842","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#batch_size.","attributes":{"type":"uses","at":[509,44,509,54]}},{"key":"geid_138_1843","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#mode.","attributes":{"type":"uses","at":[510,32,510,36]}},{"key":"geid_138_1844","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[515,35,515,37]}},{"key":"geid_138_1845","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[515,38,515,42]}},{"key":"geid_138_1846","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[515,43,515,47]}},{"key":"geid_138_1847","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#create_prefix().","attributes":{"type":"uses","at":[517,20,517,33]}},{"key":"geid_138_1848","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#batch_size.","attributes":{"type":"uses","at":[519,44,519,54]}},{"key":"geid_138_1849","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#mode.","attributes":{"type":"uses","at":[520,32,520,36]}},{"key":"geid_138_1850","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#batch_size.","attributes":{"type":"uses","at":[522,44,522,54]}},{"key":"geid_138_1851","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#mode.","attributes":{"type":"uses","at":[523,32,523,36]}},{"key":"geid_138_1852","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[529,23,529,25]}},{"key":"geid_138_1853","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[529,26,529,30]}},{"key":"geid_138_1854","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[529,31,529,37]}},{"key":"geid_138_1855","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[530,20,530,22]}},{"key":"geid_138_1856","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 os/makedirs().","attributes":{"type":"uses","at":[530,23,530,31]}},{"key":"geid_138_1857","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[530,32,530,34]}},{"key":"geid_138_1858","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[530,35,530,39]}},{"key":"geid_138_1859","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[530,40,530,47]}},{"key":"geid_138_1860","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[548,23,548,25]}},{"key":"geid_138_1861","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[548,26,548,30]}},{"key":"geid_138_1862","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[548,31,548,37]}},{"key":"geid_138_1863","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[549,20,549,22]}},{"key":"geid_138_1864","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 os/makedirs().","attributes":{"type":"uses","at":[549,23,549,31]}},{"key":"geid_138_1865","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[549,32,549,34]}},{"key":"geid_138_1866","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[549,35,549,39]}},{"key":"geid_138_1867","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[549,40,549,47]}},{"key":"geid_138_1868","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#batch_size.","attributes":{"type":"uses","at":[553,44,553,54]}},{"key":"geid_138_1869","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#mode.","attributes":{"type":"uses","at":[554,32,554,36]}},{"key":"geid_138_1870","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#batch_size.","attributes":{"type":"uses","at":[556,44,556,54]}},{"key":"geid_138_1871","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#mode.","attributes":{"type":"uses","at":[557,32,557,36]}},{"key":"geid_138_1872","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#batch_size.","attributes":{"type":"uses","at":[566,44,566,54]}},{"key":"geid_138_1873","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#mode.","attributes":{"type":"uses","at":[567,32,567,36]}},{"key":"geid_138_1874","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[572,23,572,25]}},{"key":"geid_138_1875","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[572,26,572,30]}},{"key":"geid_138_1876","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[572,31,572,37]}},{"key":"geid_138_1877","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[573,20,573,22]}},{"key":"geid_138_1878","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 os/makedirs().","attributes":{"type":"uses","at":[573,23,573,31]}},{"key":"geid_138_1879","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 os/__init__:","attributes":{"type":"uses","at":[573,32,573,34]}},{"key":"geid_138_1880","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 os/path.","attributes":{"type":"uses","at":[573,35,573,39]}},{"key":"geid_138_1881","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 ntpath/join().","attributes":{"type":"uses","at":[573,40,573,47]}},{"key":"geid_138_1882","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#batch_size.","attributes":{"type":"uses","at":[577,44,577,54]}},{"key":"geid_138_1883","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#mode.","attributes":{"type":"uses","at":[578,32,578,36]}},{"key":"geid_138_1884","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#batch_size.","attributes":{"type":"uses","at":[580,44,580,54]}},{"key":"geid_138_1885","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#mode.","attributes":{"type":"uses","at":[581,32,581,36]}},{"key":"geid_138_1886","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#batch_size.","attributes":{"type":"uses","at":[590,44,590,54]}},{"key":"geid_138_1887","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#mode.","attributes":{"type":"uses","at":[591,32,591,36]}},{"key":"geid_138_1888","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 gc/__init__:","attributes":{"type":"uses","at":[613,16,613,18]}},{"key":"geid_138_1889","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 gc/collect().","attributes":{"type":"uses","at":[613,19,613,26]}},{"key":"geid_138_1890","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `streamdiffusion.acceleration.sfast`/__init__:","attributes":{"type":"uses","at":[618,21,618,55]}},{"key":"geid_138_1891","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 traceback/__init__:","attributes":{"type":"uses","at":[625,12,625,21]}},{"key":"geid_138_1892","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python python-stdlib 3.11 traceback/print_exc().","attributes":{"type":"uses","at":[625,22,625,31]}},{"key":"geid_138_1893","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python numpy 1.25.2 `numpy.random`/__init__:","attributes":{"type":"uses","at":[629,22,629,28]}},{"key":"geid_138_1894","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python numpy 1.25.2 `numpy.random.mtrand`/randint.","attributes":{"type":"uses","at":[629,29,629,36]}},{"key":"geid_138_1895","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#use_safety_checker.","attributes":{"type":"uses","at":[642,16,642,34]}},{"key":"geid_138_1896","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer transformers/__init__:","attributes":{"type":"uses","at":[643,17,643,29]}},{"key":"geid_138_1897","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python temp indexer `diffusers.pipelines.stable_diffusion.safety_checker`/__init__:","attributes":{"type":"uses","at":[644,17,644,68]}},{"key":"geid_138_1898","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python Pillow 10.0.0 `PIL.Image`/__init__:","attributes":{"type":"uses","at":[654,37,654,42]}},{"key":"geid_138_1899","source":"scip-python python temp indexer `utils.wrapper`/StreamDiffusionWrapper#_load_model().","target":"scip-python python Pillow 10.0.0 `pil.image`/new().","attributes":{"type":"uses","at":[654,43,654,46]}}]}